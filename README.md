# ğŸµ DetecciÃ³n de Parkinson mediante AnÃ¡lisis de Voz

Sistema de clasificaciÃ³n binaria (Healthy vs Parkinson) usando redes neuronales convolucionales 2D sobre espectrogramas Mel de seÃ±ales de voz.

---

## ğŸ“‹ Ãndice

1. [Resumen del Proyecto](#resumen-del-proyecto)
2. [Estructura del Proyecto](#estructura-del-proyecto)
3. [Flujo de Trabajo](#flujo-de-trabajo)
4. [Notebooks Disponibles](#notebooks-disponibles)
5. [Pipelines Automatizados](#pipelines-automatizados)
6. [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
7. [Resultados](#resultados)

---

## ğŸ¯ Resumen del Proyecto

### Objetivo
Clasificar automÃ¡ticamente seÃ±ales de voz para detectar Parkinson usando tÃ©cnicas de Deep Learning.

### MetodologÃ­a
- **Preprocesamiento**: Resampling, segmentaciÃ³n, Mel spectrograms
- **Data Augmentation**: Pitch shift, time stretch, noise, SpecAugment
- **Modelos**:
  - **CNN2D**: Modelo baseline sin Domain Adaptation
  - **CNN2D_DA**: Modelo con Domain Adaptation y Gradient Reversal Layer (GRL)

### ImplementaciÃ³n
Basado en el paper: **Ibarra et al. (2023)** - "Towards a Corpus (and Language)-Independent Screening of Parkinson's Disease from Voice and Speech through Domain Adaptation"

---

## ğŸ“ Estructura del Proyecto

```
parkinson-voice-uncertainty/
â”‚
â”œâ”€â”€ ğŸ““ NOTEBOOKS (Ejecutar en este orden)
â”‚   â”œâ”€â”€ 1. data_preprocessing.ipynb    â† Preprocesar datos (UNA VEZ)
â”‚   â”œâ”€â”€ 2. cnn_training.ipynb          â† CNN2D baseline
â”‚   â””â”€â”€ 3. cnn_da_training.ipynb       â† CNN2D_DA con GRL
â”‚
â”œâ”€â”€ ğŸš€ pipelines/                      â† Scripts automatizados
â”‚   â”œâ”€â”€ README.md                      â† DocumentaciÃ³n
â”‚   â”œâ”€â”€ train_cnn.py                   â† Pipeline CNN2D + MC Dropout
â”‚   â””â”€â”€ train_cnn_da_kfold.py         â† Pipeline CNN2D_DA + K-fold
â”‚
â”œâ”€â”€ ğŸ“¦ modules/                        â† CÃ³digo compartido
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ augmentation.py                â† Data augmentation
â”‚   â”œâ”€â”€ cache_utils.py                 â† GestiÃ³n de cache
â”‚   â”œâ”€â”€ cnn_inference.py               â† Inferencia con MC Dropout
â”‚   â”œâ”€â”€ cnn_model.py                   â† CNN2D y CNN2D_DA
â”‚   â”œâ”€â”€ cnn_training.py                â† Funciones de entrenamiento
â”‚   â”œâ”€â”€ cnn_utils.py                   â† Utilidades
â”‚   â”œâ”€â”€ cnn_visualization.py           â† Visualizaciones
â”‚   â”œâ”€â”€ dataset.py                     â† GestiÃ³n de datasets
â”‚   â”œâ”€â”€ preprocessing.py               â† Preprocesamiento
â”‚   â”œâ”€â”€ utils.py                       â† Utilidades generales
â”‚   â””â”€â”€ visualization.py               â† Visualizaciones generales
â”‚
â”œâ”€â”€ ğŸ’¾ cache/                          â† Datos preprocesados
â”‚   â”œâ”€â”€ healthy/
â”‚   â””â”€â”€ parkinson/
â”‚
â”œâ”€â”€ ğŸ“Š data/                           â† Datos raw
â”‚   â”œâ”€â”€ vowels_healthy/
â”‚   â””â”€â”€ vowels_pk/
â”‚
â”œâ”€â”€ ğŸ¯ results/                        â† Resultados de entrenamientos
â”‚   â”œâ”€â”€ cnn_no_da/                     â† Desde cnn_training.ipynb
â”‚   â”œâ”€â”€ cnn_da/                        â† Desde cnn_da_training.ipynb
â”‚   â””â”€â”€ cnn_da_kfold/                  â† Desde pipelines/
â”‚
â””â”€â”€ ğŸ“ DocumentaciÃ³n adicional
    â””â”€â”€ data_preparation/              â† GuÃ­as de preparaciÃ³n de datos
```

---

## ğŸ”„ Flujo de Trabajo

### ğŸ“Š Diagrama de Flujo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 1: data_preprocessing.ipynb   â”‚
â”‚ Ejecutar UNA VEZ                    â”‚
â”‚ â±ï¸  7-10 minutos                     â”‚
â”‚ â†“                                   â”‚
â”‚ Genera cache/ con datos augmentados â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paso 2A:    â”‚   â”‚ Paso 2B:         â”‚
â”‚ CNN2D       â”‚   â”‚ CNN2D_DA         â”‚
â”‚ (baseline)  â”‚   â”‚ (domain adapt)   â”‚
â”‚             â”‚   â”‚                  â”‚
â”‚ cnn_        â”‚   â”‚ cnn_da_          â”‚
â”‚ training    â”‚   â”‚ training         â”‚
â”‚ .ipynb      â”‚   â”‚ .ipynb           â”‚
â”‚             â”‚   â”‚                  â”‚
â”‚ â±ï¸ 10-15 min â”‚   â”‚ â±ï¸ 15-20 min     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                   â†“
results/          results/
cnn_no_da/        cnn_da/
    â†“                   â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        3. Comparar
           Resultados
```

### âš¡ Quick Start

```bash
# Primera vez (setup completo):
jupyter notebook data_preprocessing.ipynb  # 1. Generar cache (~7-10 min)
jupyter notebook cnn_training.ipynb        # 2A. Baseline (~10-15 min)
jupyter notebook cnn_da_training.ipynb     # 2B. Domain Adapt (~15-20 min)

# ExperimentaciÃ³n (cache ya existe):
jupyter notebook cnn_training.ipynb        # Modificar hiperparÃ¡metros y ejecutar

# ProducciÃ³n (automatizado):
python pipelines/train_cnn.py --lr 0.001
python pipelines/train_cnn_da_kfold.py --n_folds 10
```

---

## ğŸ““ Notebooks Disponibles

### 1ï¸âƒ£ `data_preprocessing.ipynb`

**PropÃ³sito**: Generar cache de datos preprocesados y augmentados

**Ejecutar**: UNA VEZ (o cuando cambies parÃ¡metros de preprocesamiento)

**Contenido**:
- ğŸ”Š VisualizaciÃ³n de audio raw
- ğŸµ Preprocesamiento (resampling, segmentaciÃ³n, Mel spectrograms)
- ğŸ¨ Data augmentation (pitch shift, time stretch, noise, SpecAugment)
- ğŸ’¾ GeneraciÃ³n de cache

**Output**:
```
cache/
â”œâ”€â”€ healthy/augmented_dataset_*.pkl (~1553 muestras)
â””â”€â”€ parkinson/augmented_dataset_*.pkl (~1219 muestras)
```

**Tiempo**: ~7-10 minutos

---

### 2ï¸âƒ£ `cnn_training.ipynb`

**PropÃ³sito**: Entrenar modelo CNN2D baseline sin Domain Adaptation

**Prerequisito**: âš ï¸ Cache generado (ejecutar `data_preprocessing.ipynb` primero)

**Contenido**:
- ğŸ“ Carga cache (~5 segundos)
- ğŸ“Š Split train/val/test (70/15/15)
- ğŸ—ï¸ Modelo CNN2D con backbone Ibarra (sin DA)
- ğŸš€ Entrenamiento con Adam + early stopping
- ğŸ“ˆ EvaluaciÃ³n y visualizaciÃ³n

**Output**:
```
results/cnn_no_da/
â”œâ”€â”€ best_model.pth
â”œâ”€â”€ test_metrics.json
â”œâ”€â”€ training_progress.png
â””â”€â”€ confusion_matrix_test.png
```

**Tiempo**: ~10-15 minutos

---

### 3ï¸âƒ£ `cnn_da_training.ipynb`

**PropÃ³sito**: Entrenar modelo CNN2D_DA con Domain Adaptation

**Prerequisito**: âš ï¸ Cache generado (ejecutar `data_preprocessing.ipynb` primero)

**Contenido**:
- ğŸ“ Carga cache (~5 segundos)
- ğŸ“Š Split train/val/test (70/15/15)
- ğŸ—ï¸ Modelo CNN2D_DA (dual-head con GRL)
- ğŸš€ Entrenamiento multi-task con SGD
- ğŸ“ˆ EvaluaciÃ³n PD + Domain

**Output**:
```
results/cnn_da/
â”œâ”€â”€ best_model_da.pth
â”œâ”€â”€ test_metrics_da.json
â”œâ”€â”€ training_progress_da.png
â””â”€â”€ confusion_matrix_test_da.png
```

**Tiempo**: ~15-20 minutos

---

## ğŸš€ Pipelines Automatizados

### `pipelines/train_cnn.py`

Pipeline completo para entrenar CNN2D con MC Dropout:

```bash
python pipelines/train_cnn.py --epochs 100 --lr 0.001
```

**CaracterÃ­sticas**:
- Entrenamiento automatizado CNN2D
- Implementa MC Dropout para incertidumbre
- ConfiguraciÃ³n vÃ­a argumentos de lÃ­nea de comandos

---

### `pipelines/train_cnn_da_kfold.py`

Pipeline completo para entrenar CNN2D_DA con validaciÃ³n cruzada:

```bash
python pipelines/train_cnn_da_kfold.py --n_folds 10
```

**CaracterÃ­sticas**:
- Entrenamiento automatizado CNN2D_DA
- K-fold cross-validation (10-fold por defecto)
- ImplementaciÃ³n segÃºn Ibarra (2023)

---

## ğŸ’» InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos del Sistema

- Python 3.8+
- PyTorch 1.8+
- CUDA (opcional, para GPU)

### InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone <repo-url>
cd parkinson-voice-uncertainty
```

2. **Crear entorno virtual**:
```bash
python -m venv parkinson_env
source parkinson_env/bin/activate  # Linux/Mac
# o
parkinson_env\Scripts\activate  # Windows
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

### Estructura de Datos

Colocar archivos de audio en:
```
data/
â”œâ”€â”€ vowels_healthy/  â† Archivos .egg de sujetos sanos
â””â”€â”€ vowels_pk/       â† Archivos .egg de pacientes Parkinson
```

---

## ğŸ“Š Resultados

### ComparaciÃ³n de Modelos

| Modelo | Accuracy | F1-Score | ParÃ¡metros |
|--------|----------|----------|------------|
| CNN2D (baseline) | ~98.8% | ~98.8% | 674,562 |
| CNN2D_DA (con GRL) | TBD | TBD | ~800,000+ |

### âš ï¸ Diferencias ArquitectÃ³nicas Importantes

**Los modelos NO son idÃ©nticos**. Diferencias clave:

#### 1ï¸âƒ£ MaxPooling
- **CNN2D**: MaxPool **2Ã—2** (configuraciÃ³n estÃ¡ndar)
- **CNN2D_DA**: MaxPool **3Ã—3** (segÃºn paper Ibarra 2023)

#### 2ï¸âƒ£ Dimensiones de Features
```python
# DespuÃ©s de feature extraction:
CNN2D:    (B, 64, 16, 10) â†’ Flatten â†’ (B, 10,240)
CNN2D_DA: (B, 64, 17, 11) â†’ Flatten â†’ (B, 11,968)
```

#### 3ï¸âƒ£ Estructura
- **CNN2D**: Single-head (solo clasificaciÃ³n PD)
- **CNN2D_DA**: Dual-head (clasificaciÃ³n PD + Domain con GRL)

#### 4ï¸âƒ£ Entrenamiento
- **CNN2D**: Adam (LR=0.001), Loss simple
- **CNN2D_DA**: SGD (LR=0.1), Loss multi-task

### Tabla Comparativa Detallada

| CaracterÃ­stica | CNN2D | CNN2D_DA |
|----------------|-------|----------|
| **MaxPool** | 2Ã—2 | 3Ã—3 |
| **Feature Dim** | 10,240 | 11,968 |
| **Heads** | 1 (PD) | 2 (PD + Domain) |
| **GRL** | âŒ No | âœ… SÃ­ |
| **ParÃ¡metros** | 674,562 | ~800,000+ |
| **Loss** | CrossEntropy | Multi-task |
| **Optimizer** | Adam | SGD |
| **LR** | 0.001 | 0.1 |
| **Uso** | Baseline | Domain Adapt |

### CaracterÃ­sticas de los Modelos

**âš¡ Ambos modelos comparten el MISMO Feature Extractor para comparaciÃ³n justa:**
- 2 bloques Conv2D â†’ BatchNorm â†’ ReLU â†’ MaxPool(3Ã—3) â†’ Dropout

#### CNN2D (Baseline)
- **Arquitectura**: Single-head CNN (solo cabeza PD)
- **Feature Extractor**: IdÃ©ntico a CNN2D_DA (arquitectura Ibarra 2023)
- **Entrenamiento**: Adam optimizer
- **Output**: ClasificaciÃ³n Healthy/Parkinson
- **Ventaja**: Simplicidad, sin Domain Adaptation

#### CNN2D_DA (Domain Adaptation)
- **Arquitectura**: Dual-head CNN con GRL
- **Feature Extractor**: Compartido con CNN2D (arquitectura Ibarra 2023)
- **Entrenamiento**: SGD optimizer (segÃºn paper)
- **Output**: ClasificaciÃ³n PD + Domain
- **Ventaja**: Robustez ante diferentes dominios
- **Paper**: ImplementaciÃ³n fiel a Ibarra et al. (2023)

**ğŸ”„ Ventaja del diseÃ±o modular:**
- Domain Adaptation es un mÃ³dulo que se puede agregar/quitar
- ComparaciÃ³n justa: mismo backbone, diferente cabeza
- Sin duplicaciÃ³n de cÃ³digo (FeatureExtractor compartido)

---

## ğŸ“ Conceptos Clave

| TÃ©rmino | Significado |
|---------|-------------|
| **Pipeline** | Flujo completo automatizado end-to-end |
| **Module** | CÃ³digo reutilizable (librerÃ­a) |
| **Notebook** | Experimento interactivo Jupyter |
| **Cache** | Datos preprocesados guardados en disco |
| **DA** | Domain Adaptation (adaptaciÃ³n de dominio) |
| **GRL** | Gradient Reversal Layer |
| **MC Dropout** | Monte Carlo Dropout (cuantificaciÃ³n de incertidumbre) |
| **K-fold** | ValidaciÃ³n cruzada en K particiones |

---

## ğŸ“ GuÃ­a de Uso Paso a Paso

### Primera EjecuciÃ³n (Setup Completo)

**DÃ­a 1: PreparaciÃ³n y Entrenamiento (Total: ~35-45 min)**

1. **Generar Cache** (~7-10 min)
```bash
jupyter notebook data_preprocessing.ipynb
# Ejecutar todas las celdas (Cell â†’ Run All)
```
âœ… Resultado: Cache en `cache/healthy/` y `cache/parkinson/`

2. **Entrenar Baseline** (~10-15 min)
```bash
jupyter notebook cnn_training.ipynb
# Ejecutar todas las celdas
```
âœ… Resultado: Modelo en `results/cnn_no_da/`

3. **Entrenar con DA** (~15-20 min)
```bash
jupyter notebook cnn_da_training.ipynb
# Ejecutar todas las celdas
```
âœ… Resultado: Modelo en `results/cnn_da/`

---

### ExperimentaciÃ³n (Con Cache Existente)

**Solo Modificar y Entrenar (~10-15 min por experimento)**

```bash
# Abrir notebook
jupyter notebook cnn_training.ipynb

# Modificar hiperparÃ¡metros en la celda correspondiente:
N_EPOCHS = 150
LEARNING_RATE = 5e-4

# Ejecutar todas las celdas
```

---

## ğŸ”§ ConfiguraciÃ³n de ParÃ¡metros

### Preprocesamiento (en `modules/preprocessing.py`)

```python
SAMPLE_RATE = 16000      # Hz
WINDOW_MS = 100          # ms
OVERLAP = 0.5            # 50%
N_MELS = 65              # Bandas Mel
TARGET_FRAMES = 41       # Frames por espectrograma
```

### Data Augmentation

```python
AUGMENTATION_TYPES = [
    "original",
    "pitch_shift",
    "time_stretch", 
    "noise"
]
NUM_SPEC_AUGMENT_VERSIONS = 2
```

### Entrenamiento CNN2D

```python
N_EPOCHS = 100
LEARNING_RATE = 1e-3
BATCH_SIZE = 32
EARLY_STOPPING_PATIENCE = 15
```

### Entrenamiento CNN2D_DA

```python
N_EPOCHS = 100
LEARNING_RATE = 0.1      # SGD segÃºn Ibarra
ALPHA = 1.0              # Peso de loss_domain
LAMBDA_CONSTANT = 1.0    # Lambda para GRL
```

---

## ğŸ” Detalles TÃ©cnicos

### Arquitectura CNN2D (sin DA)

```
Input: (B, 1, 65, 41)
â†“
[Feature Extractor - Ibarra 2023]
Block1: Conv2D(32, 3Ã—3) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ Dropout
Block2: Conv2D(64, 3Ã—3) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ Dropout
â†“
[PD Head]
Flatten â†’ FC(64) â†’ ReLU â†’ Dropout â†’ FC(2)
â†“
Output: Softmax (Healthy/Parkinson)
```

**Nota**: Usa el mismo FeatureExtractor que CNN2D_DA para comparaciÃ³n justa.

---

### Arquitectura CNN2D_DA (con DA)

```
Input: (B, 1, 65, 41)
â†“
[Feature Extractor - Ibarra 2023] (COMPARTIDO)
Block1: Conv2D(32, 3Ã—3) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ Dropout
Block2: Conv2D(64, 3Ã—3) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ Dropout
â†“
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [PD Head]               â”‚ [Domain Head]           â”‚
â”‚ Flatten â†’ FC(64) â†’ FC(2)â”‚ GRL â†’ FC(64) â†’ FC(n_dom)â”‚
â”‚ â†“                       â”‚ â†“                       â”‚
â”‚ Healthy/Parkinson       â”‚ Domain ID               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DiseÃ±o Modular**: El DA es un mÃ³dulo que se puede agregar/quitar sin duplicar cÃ³digo.

---

## ğŸ’¡ Ventajas de la OrganizaciÃ³n Actual

### âœ… Antes (Problemas)
- âŒ Todo mezclado en un notebook gigante
- âŒ Reprocesar datos en cada experimento (~6 min cada vez)
- âŒ Scripts duplicando cÃ³digo de notebooks
- âŒ DifÃ­cil saber quÃ© ejecutar primero

### âœ… Ahora (Soluciones)
- âœ… Notebooks modulares (una responsabilidad por notebook)
- âœ… Cache reutilizable (ahorro de ~6 min/experimento)
- âœ… Scripts organizados en `pipelines/` (sin duplicaciÃ³n)
- âœ… Flujo claro y documentado

---

## ğŸ†˜ Troubleshooting

### Error: "Cache not found"
**SoluciÃ³n**: Ejecutar `data_preprocessing.ipynb` primero

### Error: "ImportError"
**SoluciÃ³n**: Verificar que estÃ¡s en la raÃ­z del proyecto

### Error: "Out of memory"
**SoluciÃ³n**: Reducir `BATCH_SIZE` en notebook de entrenamiento

### Cache desactualizado
**SoluciÃ³n**:
```python
# En data_preprocessing.ipynb, modificar:
FORCE_REGENERATE = True  # â† Regenera cache
```

---

## ğŸ“ˆ ComparaciÃ³n: Notebooks vs Pipelines

| CaracterÃ­stica | Notebooks | Pipelines |
|---------------|-----------|-----------|
| **Interfaz** | Jupyter (interactivo) | CLI (automatizado) |
| **Uso** | ExploraciÃ³n, debugging | ProducciÃ³n, batch |
| **SupervisiÃ³n** | Paso a paso | Desatendido |
| **VisualizaciÃ³n** | Inline | Archivos PNG |
| **ConfiguraciÃ³n** | En celdas | Argumentos CLI |

---

## ğŸ¯ Casos de Uso

### ğŸ“Š ExploraciÃ³n y Desarrollo
**â†’ Usar Notebooks**
- Ver resultados paso a paso
- Modificar hiperparÃ¡metros fÃ¡cilmente
- Visualizaciones inline
- Ideal para entender el proceso

### ğŸš€ ProducciÃ³n y Batch
**â†’ Usar Pipelines**
- Ejecutar mÃºltiples experimentos
- Automatizar entrenamiento
- ConfiguraciÃ³n vÃ­a CLI
- Ideal para validaciÃ³n cruzada

---

## ğŸ“š InformaciÃ³n TÃ©cnica Adicional

### Cache de Datos

**UbicaciÃ³n**: `cache/healthy/` y `cache/parkinson/`

**Contenido**:
- Espectrogramas Mel augmentados
- ~1553 muestras Healthy
- ~1219 muestras Parkinson
- Total: ~2772 espectrogramas

**Ventaja**: 
- Carga instantÃ¡nea (~5 segundos)
- Ahorro de ~6 minutos por experimento
- Mismos datos para todos los modelos

### Resultados Guardados

Cada entrenamiento guarda:
- âœ“ Modelo entrenado (`.pth`)
- âœ“ MÃ©tricas de test (`.json`)
- âœ“ GrÃ¡ficas de progreso (`.png`)
- âœ“ Matriz de confusiÃ³n (`.png`)

---

## ğŸ”„ Variables Importantes

### DespuÃ©s de `data_preprocessing.ipynb`:
```python
X_healthy     # Tensor (1553, 1, 65, 41)
X_parkinson   # Tensor (1219, 1, 65, 41)
cache/        # Archivos .pkl para reutilizar
```

### DespuÃ©s de `cnn_training.ipynb`:
```python
model         # CNN2D entrenado
history       # Historial de entrenamiento
test_metrics  # Accuracy, F1, Precision, Recall
```

### DespuÃ©s de `cnn_da_training.ipynb`:
```python
model_da         # CNN2D_DA entrenado
history_da       # Historial multi-task
test_metrics_da  # MÃ©tricas PD + Domain
```

---

## âœ… Checklist de Inicio

### Primera Vez
- [ ] Instalar dependencias (`pip install -r requirements.txt`)
- [ ] Colocar datos en `data/vowels_healthy/` y `data/vowels_pk/`
- [ ] Ejecutar `data_preprocessing.ipynb`
- [ ] Verificar que `cache/` existe
- [ ] Ejecutar `cnn_training.ipynb`
- [ ] Ejecutar `cnn_da_training.ipynb`
- [ ] Comparar resultados en `results/`

### ExperimentaciÃ³n
- [ ] Cache ya existe
- [ ] Modificar hiperparÃ¡metros segÃºn necesidad
- [ ] Ejecutar notebook de entrenamiento
- [ ] Comparar con runs previos

---

## ğŸ“– Referencias

**Paper Principal**:
- Ibarra et al. (2023): "Towards a Corpus (and Language)-Independent Screening of Parkinson's Disease from Voice and Speech through Domain Adaptation"

**TÃ©cnicas Implementadas**:
- Domain Adaptation con Gradient Reversal Layer (GRL)
- Monte Carlo Dropout para cuantificaciÃ³n de incertidumbre
- Data Augmentation (SpecAugment)
- K-fold Cross-Validation

---

## ğŸ¯ PrÃ³ximos Pasos (Futuro)

1. **MC Dropout**: Implementar inferencia con MC Dropout en notebooks
2. **AnÃ¡lisis de Incertidumbre**: Cuantificar incertidumbre en predicciones
3. **ComparaciÃ³n Completa**: Notebook dedicado a comparar CNN2D vs CNN2D_DA
4. **Limpieza**: Eliminar notebooks legacy si es necesario

---

## ğŸ’¬ Soporte

Para preguntas o problemas:
1. Revisar la documentaciÃ³n en `pipelines/README.md`
2. Verificar troubleshooting en esta guÃ­a
3. Revisar logs de ejecuciÃ³n

---

## ğŸ“„ Licencia

[Especificar licencia del proyecto]

---

**Ãšltima actualizaciÃ³n**: 2025-10-17

**Autor**: [Tu nombre/equipo]

**VersiÃ³n**: 2.0 (ReorganizaciÃ³n modular)
