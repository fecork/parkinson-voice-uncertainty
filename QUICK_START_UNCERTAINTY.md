# ðŸš€ Quick Start: Sistema de Incertidumbre

## Â¿QuÃ© hemos construido?

Un sistema completo de **estimaciÃ³n de incertidumbre epistÃ©mica + aleatoria** para clasificaciÃ³n de Parkinson basado en CNN.

---

## ðŸ“¦ Componentes Creados

### 1. MÃ³dulos Python (`modules/`)

#### `uncertainty_model.py`
- **UncertaintyCNN**: Modelo con 2 cabezas
  - Cabeza A: logits (predicciÃ³n)
  - Cabeza B: s_logit (log-varianza)
- **MCDropout/MCDropout2d**: Dropout activo en inferencia
- FunciÃ³n de resumen del modelo

#### `uncertainty_loss.py`
- **heteroscedastic_classification_loss**: PÃ©rdida con ruido gaussiano
- **compute_nll**: Negative Log-Likelihood
- **compute_brier_score**: Error cuadrÃ¡tico de probabilidades
- **compute_ece**: Expected Calibration Error

#### `uncertainty_training.py`
- **train_uncertainty_model**: Loop de entrenamiento completo
- **evaluate_with_uncertainty**: EvaluaciÃ³n con MC Dropout (T_test pases)
- **print_uncertainty_results**: Imprime mÃ©tricas bonitas

#### `uncertainty_visualization.py`
- **plot_uncertainty_histograms**: 3 histogramas (total, epistÃ©mica, aleatoria)
- **plot_reliability_diagram**: Diagrama de calibraciÃ³n
- **plot_uncertainty_scatter**: Scatter epistÃ©mica vs aleatoria
- **plot_training_history_uncertainty**: Progreso de entrenamiento

### 2. Notebook Principal

#### `cnn_uncertainty_training.ipynb`
Pipeline completo:
1. Setup
2. Carga de datos (cache)
3. Split (70/15/15)
4. CreaciÃ³n del modelo
5. Entrenamiento
6. EvaluaciÃ³n con MC Dropout
7. Visualizaciones

### 3. DocumentaciÃ³n

- **UNCERTAINTY_README.md**: DocumentaciÃ³n tÃ©cnica completa
- **QUICK_START_UNCERTAINTY.md**: Este archivo

---

## âš¡ Uso RÃ¡pido

### Paso 1: Preprocesar datos (si no lo has hecho)
```bash
jupyter notebook data_preprocessing.ipynb
# Ejecutar todas las celdas
```

### Paso 2: Entrenar modelo con incertidumbre
```bash
jupyter notebook cnn_uncertainty_training.ipynb
# Ejecutar todas las celdas
```

### Paso 3: Ver resultados
Los archivos se guardan en `results/cnn_uncertainty/`:
- `best_model_uncertainty.pth`: Modelo entrenado
- `test_metrics_uncertainty.json`: MÃ©tricas JSON
- Varias grÃ¡ficas PNG

---

## ðŸ”¬ Detalles TÃ©cnicos Clave

### Entrenamiento
```python
# T_noise = 5 (rÃ¡pido, estable)
for batch in train_loader:
    logits, s_logit = model(x)
    loss = heteroscedastic_loss(logits, s_logit, y, T_noise=5)
    loss.backward()
```

### Inferencia
```python
# T_test = 30 (mÃ¡s muestras = mejor estimaciÃ³n)
results = model.predict_with_uncertainty(x, n_samples=30)

# Outputs:
# - pred: PredicciÃ³n final
# - probs_mean: Probabilidades promedio
# - entropy_total: H(pÌ„)
# - epistemic: BALD
# - aleatoric: ÏƒÂ²
# - confidence: max(pÌ„)
```

---

## ðŸ“Š Interpretando Resultados

### Buena SeÃ±al âœ…
```
Entropy(correctos):   0.08  â† Bajo
Entropy(incorrectos): 0.45  â† Alto
ECE: 0.03  â† Bien calibrado
```
âžœ El modelo **sabe cuÃ¡ndo estÃ¡ inseguro**

### Mala SeÃ±al âŒ
```
Entropy(correctos):   0.25  â† Similar
Entropy(incorrectos): 0.28  â† Similar
ECE: 0.15  â† Mal calibrado
```
âžœ Modelo no distingue certeza

### DiagnÃ³stico

| Alta EpistÃ©mica | âžœ Necesita mÃ¡s datos |
| Alta Aleatoria | âžœ Datos ruidosos (lÃ­mite intrÃ­nseco) |

---

## ðŸŽ¯ PrÃ³ximos Pasos

### ExperimentaciÃ³n
1. Ajustar `T_noise` (probar 3, 5, 10)
2. Ajustar `T_test` (probar 30, 50, 100)
3. Probar `dropout_p` (0.2, 0.25, 0.3)
4. Ajustar clamp de `s_logit`

### ComparaciÃ³n
- Comparar con modelo base (`cnn_training.ipynb`)
- Comparar con modelo DA (`cnn_da_training.ipynb`)
- AÃ±adir incertidumbre al modelo DA

### AplicaciÃ³n
- Usar incertidumbre para detectar casos difÃ­ciles
- Filtrar predicciones por confianza
- Active learning (seleccionar samples con alta epistÃ©mica)

---

## âš™ï¸ HiperparÃ¡metros Recomendados

| ParÃ¡metro | Valor Inicial | Rango | Notas |
|-----------|---------------|-------|-------|
| `dropout_p` | 0.25 | 0.2-0.3 | Mayor = mÃ¡s epistÃ©mica |
| `T_noise` | 5 | 3-10 | MÃ¡s = estable pero lento |
| `T_test` | 30 | 30-100 | MÃ¡s = mejor estimaciÃ³n |
| `s_min` | -10.0 | -15 a -5 | exp(-10/2) â‰ˆ 0.0067 |
| `s_max` | 3.0 | 2 a 5 | exp(3/2) â‰ˆ 4.48 |
| `lr` | 1e-3 | 1e-4 a 1e-2 | AdamW |
| `weight_decay` | 1e-4 | 1e-5 a 1e-3 | RegularizaciÃ³n |

---

## ðŸ› Troubleshooting

### PÃ©rdida diverge (NaN)
- Reducir `T_noise` a 3
- Ajustar `s_min/s_max` a [-15, 2]
- Reducir `lr` a 5e-4
- Verificar que no haya NaN en datos

### EpistÃ©mica muy baja
- Aumentar `dropout_p` a 0.3
- Verificar que MCDropout estÃ© activo en eval

### Aleatoria muy alta
- Revisar calidad de datos
- Puede ser normal si datos tienen ruido intrÃ­nseco
- Considerar limpieza de datos

### ECE alto (> 0.10)
- Aumentar `T_noise`
- Ajustar `weight_decay`
- Considerar temperature scaling post-training

---

## ðŸ“ž Contacto

Para preguntas o issues sobre el sistema de incertidumbre, consultar:
- `UNCERTAINTY_README.md` para detalles tÃ©cnicos
- CÃ³digo fuente en `modules/uncertainty_*.py`
- Notebook de ejemplo: `cnn_uncertainty_training.ipynb`

---

**Happy Uncertainty Estimation! ðŸŽ²**

