{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† CNN 1D con Domain Adaptation para Detecci√≥n de Parkinson\n",
        "## Modelo con Atenci√≥n Temporal y GRL - Speaker-Independent Split\n",
        "\n",
        "Este notebook entrena un modelo **CNN1D_DA** (con Domain Adaptation y atenci√≥n temporal) para clasificaci√≥n binaria Parkinson vs Healthy.\n",
        "\n",
        "### üìã Pipeline:\n",
        "1. **Setup**: Configuraci√≥n del entorno\n",
        "2. **Data Loading**: Carga de datos desde cache y transformaci√≥n a [B, F, T]\n",
        "3. **Split**: Train/Val/Test speaker-independent (70/15/15)\n",
        "4. **Model**: CNN1D_DA con 3 bloques Conv1D, atenci√≥n temporal, y dual-head\n",
        "5. **Training**: Entrenamiento multi-task con GRL y early stopping\n",
        "6. **Evaluation**: M√©tricas segment-level y patient-level\n",
        "7. **Visualization**: Curvas de training, confusion matrix, y t-SNE\n",
        "\n",
        "### ‚ö†Ô∏è PREREQUISITO:\n",
        "**Ejecutar primero `data_preprocessing.ipynb`** para generar el cache de datos preprocesados.\n",
        "\n",
        "### üìö Referencia:\n",
        "Implementaci√≥n seg√∫n Ibarra et al. (2023): \"Towards a Corpus (and Language)-Independent Screening of Parkinson's Disease from Voice and Speech through Domain Adaptation\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup y Configuraci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# IMPORTS Y CONFIGURACI√ìN\n",
        "# ============================================================\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Agregar m√≥dulos propios al path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Importar m√≥dulos propios\n",
        "from modules.augmentation import create_augmented_dataset\n",
        "from modules.dataset import to_pytorch_tensors, speaker_independent_split, group_by_patient\n",
        "from modules.cnn1d_model import CNN1D_DA\n",
        "from modules.cnn1d_training import (\n",
        "    train_model_da, \n",
        "    evaluate_da,\n",
        "    evaluate_patient_level,\n",
        "    save_metrics\n",
        ")\n",
        "from modules.cnn1d_visualization import (\n",
        "    plot_1d_training_progress,\n",
        "    plot_tsne_embeddings,\n",
        "    plot_simple_confusion_matrix\n",
        ")\n",
        "from modules.cnn_utils import compute_class_weights_auto\n",
        "\n",
        "# Configuraci√≥n de matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Configuraci√≥n de PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Reporte de configuraci√≥n\n",
        "print(\"=\"*70)\n",
        "print(\"üß† CNN 1D-DA TRAINING (con Atenci√≥n Temporal y Domain Adaptation)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ Librer√≠as cargadas correctamente\")\n",
        "print(f\"üîß Dispositivo: {device}\")\n",
        "print(f\"üì¶ PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga de Datos desde Cache\n",
        "\n",
        "‚ö†Ô∏è **CRITICAL**: Transformamos spectrogramas de [B, 1, F, T] a [B, F, T] para CNN1D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURACI√ìN DE RUTAS Y CARGA DE CACHE\n",
        "# ============================================================\n",
        "\n",
        "DATA_PATH_HEALTHY = \"./data/vowels_healthy\"\n",
        "DATA_PATH_PARKINSON = \"./data/vowels_pk\"\n",
        "CACHE_DIR_HEALTHY = \"./cache/healthy\"\n",
        "CACHE_DIR_PARKINSON = \"./cache/parkinson\"\n",
        "\n",
        "# Configuraci√≥n de augmentation (debe coincidir con data_preprocessing.ipynb)\n",
        "AUGMENTATION_TYPES = [\"original\", \"pitch_shift\", \"time_stretch\", \"noise\"]\n",
        "NUM_SPEC_AUGMENT_VERSIONS = 2\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìÅ CARGANDO DATOS DESDE CACHE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Cargar Healthy\n",
        "audio_files_healthy = list(Path(DATA_PATH_HEALTHY).glob(\"*.egg\"))\n",
        "augmented_dataset_healthy = create_augmented_dataset(\n",
        "    audio_files_healthy,\n",
        "    augmentation_types=AUGMENTATION_TYPES,\n",
        "    apply_spec_augment=True,\n",
        "    num_spec_augment_versions=NUM_SPEC_AUGMENT_VERSIONS,\n",
        "    use_cache=True,\n",
        "    cache_dir=CACHE_DIR_HEALTHY,\n",
        "    force_regenerate=False,\n",
        "    progress_every=5\n",
        ")\n",
        "X_healthy, y_task_healthy, y_domain_healthy, meta_healthy = to_pytorch_tensors(augmented_dataset_healthy)\n",
        "\n",
        "print(f\"üü¢ Healthy: {X_healthy.shape[0]} muestras\")\n",
        "\n",
        "# Cargar Parkinson\n",
        "audio_files_parkinson = list(Path(DATA_PATH_PARKINSON).glob(\"*.egg\"))\n",
        "augmented_dataset_parkinson = create_augmented_dataset(\n",
        "    audio_files_parkinson,\n",
        "    augmentation_types=AUGMENTATION_TYPES,\n",
        "    apply_spec_augment=True,\n",
        "    num_spec_augment_versions=NUM_SPEC_AUGMENT_VERSIONS,\n",
        "    use_cache=True,\n",
        "    cache_dir=CACHE_DIR_PARKINSON,\n",
        "    force_regenerate=False,\n",
        "    progress_every=5\n",
        ")\n",
        "X_parkinson, y_task_parkinson, y_domain_parkinson, meta_parkinson = to_pytorch_tensors(augmented_dataset_parkinson)\n",
        "\n",
        "print(f\"üî¥ Parkinson: {X_parkinson.shape[0]} muestras\")\n",
        "\n",
        "# CRITICAL: Transformar para CNN1D\n",
        "print(\"\\nüîÑ Transformando datos para CNN1D...\")\n",
        "print(f\"   Shape original: {X_healthy.shape} (B, 1, F, T)\")\n",
        "\n",
        "# Remover dimensi√≥n de canal: [B, 1, 65, 41] ‚Üí [B, 65, 41]\n",
        "X_healthy = X_healthy.squeeze(1)\n",
        "X_parkinson = X_parkinson.squeeze(1)\n",
        "\n",
        "print(f\"   Shape para CNN1D: {X_healthy.shape} (B, F, T)\")\n",
        "\n",
        "# Combinar datasets\n",
        "X_combined = torch.cat([X_healthy, X_parkinson], dim=0)\n",
        "y_task_combined = torch.cat([\n",
        "    torch.zeros(len(X_healthy), dtype=torch.long),\n",
        "    torch.ones(len(X_parkinson), dtype=torch.long)\n",
        "], dim=0)\n",
        "\n",
        "# Ajustar dominios para evitar colisiones\n",
        "max_domain_hc = y_domain_healthy.max().item()\n",
        "y_domain_combined = torch.cat([\n",
        "    y_domain_healthy,\n",
        "    y_domain_parkinson + max_domain_hc + 1\n",
        "], dim=0)\n",
        "\n",
        "# Metadata combinada (para patient grouping)\n",
        "all_metadata = meta_healthy + meta_parkinson\n",
        "patient_ids = [m.subject_id for m in all_metadata]\n",
        "\n",
        "print(f\"\\nüìä Dataset combinado: {len(X_combined)} muestras\")\n",
        "print(f\"   ‚Ä¢ Dominios √∫nicos: {len(torch.unique(y_domain_combined))}\")\n",
        "print(f\"   ‚Ä¢ Pacientes √∫nicos: {len(set(patient_ids))}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split Speaker-Independent Train/Val/Test\n",
        "\n",
        "**Cr√≠tico**: Ning√∫n speaker se repite entre splits para evitar data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SPEAKER-INDEPENDENT SPLIT\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä SPLIT SPEAKER-INDEPENDENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Obtener √≠ndices por speaker\n",
        "train_idx, val_idx, test_idx = speaker_independent_split(\n",
        "    all_metadata,\n",
        "    test_size=0.15,\n",
        "    val_size=0.176,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Extraer datos\n",
        "X_train = X_combined[train_idx]\n",
        "X_val = X_combined[val_idx]\n",
        "X_test = X_combined[test_idx]\n",
        "\n",
        "y_task_train = y_task_combined[train_idx]\n",
        "y_task_val = y_task_combined[val_idx]\n",
        "y_task_test = y_task_combined[test_idx]\n",
        "\n",
        "y_domain_train = y_domain_combined[train_idx]\n",
        "y_domain_val = y_domain_combined[val_idx]\n",
        "y_domain_test = y_domain_combined[test_idx]\n",
        "\n",
        "# Patient IDs para evaluaci√≥n\n",
        "patient_ids_train = [patient_ids[i] for i in train_idx]\n",
        "patient_ids_val = [patient_ids[i] for i in val_idx]\n",
        "patient_ids_test = [patient_ids[i] for i in test_idx]\n",
        "\n",
        "print(f\"\\nüìä Datos divididos:\")\n",
        "print(f\"   ‚Ä¢ Train: {len(X_train)} samples de {len(set(patient_ids_train))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Val:   {len(X_val)} samples de {len(set(patient_ids_val))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Test:  {len(X_test)} samples de {len(set(patient_ids_test))} pacientes\")\n",
        "\n",
        "# Crear DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_task_train, y_domain_train)\n",
        "val_dataset = TensorDataset(X_val, y_task_val, y_domain_val)\n",
        "test_dataset = TensorDataset(X_test, y_task_test, y_domain_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders creados (batch_size={BATCH_SIZE})\")\n",
        "print(f\"   ‚Ä¢ Train batches: {len(train_loader)}\")\n",
        "print(f\"   ‚Ä¢ Val batches: {len(val_loader)}\")\n",
        "print(f\"   ‚Ä¢ Test batches: {len(test_loader)}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Crear Modelo CNN1D_DA\n",
        "\n",
        "Arquitectura seg√∫n paper:\n",
        "- 3 bloques Conv1D (kernels: 5, 11, 21)\n",
        "- Atenci√≥n temporal (split + softmax)\n",
        "- Dual-head: PD detector + Domain detector con GRL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREAR MODELO CNN1D_DA\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üèóÔ∏è  CREANDO MODELO CNN1D_DA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# N√∫mero de dominios √∫nicos\n",
        "n_domains = len(torch.unique(y_domain_combined))\n",
        "\n",
        "print(f\"\\nüìä Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ Dominios √∫nicos: {n_domains}\")\n",
        "print(f\"   ‚Ä¢ Clases PD: 2 (Healthy/Parkinson)\")\n",
        "print(f\"   ‚Ä¢ Input shape: (B, F=65, T=41)\")\n",
        "print(f\"   ‚Ä¢ Arquitectura: 3 bloques Conv1D + Atenci√≥n Temporal\")\n",
        "\n",
        "# Crear modelo\n",
        "model = CNN1D_DA(\n",
        "    in_ch=65,      # Freq bins\n",
        "    c1=64,         # Canales bloque 1\n",
        "    c2=128,        # Canales bloque 2\n",
        "    c3=128,        # Canales bloque 3\n",
        "    p_drop=0.3,    # Dropout\n",
        "    num_pd=2,      # HC/PD\n",
        "    num_domains=n_domains\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo creado en device: {device}\")\n",
        "\n",
        "# Mostrar arquitectura\n",
        "from modules.cnn1d_model import print_model_summary\n",
        "print_model_summary(model)\n",
        "\n",
        "# Test con batch dummy\n",
        "print(\"üß™ Test Forward Pass:\")\n",
        "x_dummy = torch.randn(2, 65, 41).to(device)\n",
        "logits_pd, logits_domain, embeddings = model(x_dummy, return_embeddings=True)\n",
        "print(f\"   ‚Ä¢ Input: {x_dummy.shape}\")\n",
        "print(f\"   ‚Ä¢ Output PD: {logits_pd.shape}\")\n",
        "print(f\"   ‚Ä¢ Output Domain: {logits_domain.shape}\")\n",
        "print(f\"   ‚Ä¢ Embeddings: {embeddings.shape}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuraci√≥n de Entrenamiento\n",
        "\n",
        "Seg√∫n paper Ibarra et al.:\n",
        "- Optimizer: SGD (lr=0.1, momentum=0.9)\n",
        "- LR Scheduler: StepLR\n",
        "- Lambda GRL: scheduler lineal 0‚Üí1\n",
        "- Weighted CE si desbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURAR ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚öôÔ∏è  CONFIGURACI√ìN DE ENTRENAMIENTO (IBARRA 2023)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hiperpar√°metros seg√∫n paper\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.1  # SGD seg√∫n Ibarra\n",
        "ALPHA = 1.0  # Peso de loss_domain\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "# Calcular class weights autom√°ticos\n",
        "print(\"\\nüìä Calculando class weights:\")\n",
        "pd_weights = compute_class_weights_auto(y_task_train, threshold=0.4)\n",
        "domain_weights = compute_class_weights_auto(y_domain_train, threshold=0.4)\n",
        "\n",
        "# Crear criterios\n",
        "if pd_weights is not None:\n",
        "    criterion_pd = nn.CrossEntropyLoss(weight=pd_weights.to(device))\n",
        "else:\n",
        "    criterion_pd = nn.CrossEntropyLoss()\n",
        "\n",
        "if domain_weights is not None:\n",
        "    criterion_domain = nn.CrossEntropyLoss(weight=domain_weights.to(device))\n",
        "else:\n",
        "    criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer SGD con LR 0.1 (Ibarra 2023)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# LR Scheduler: StepLR\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=30,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# Lambda scheduler lineal 0‚Üí1\n",
        "lambda_scheduler = lambda epoch: epoch / N_EPOCHS\n",
        "\n",
        "# Directorio para guardar\n",
        "save_dir = Path(\"./results/cnn1d_da\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìã Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ √âpocas m√°ximas: {N_EPOCHS}\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE} (SGD)\")\n",
        "print(f\"   ‚Ä¢ LR Scheduler: StepLR (step=30, gamma=0.1)\")\n",
        "print(f\"   ‚Ä¢ Lambda GRL: scheduler lineal 0‚Üí1\")\n",
        "print(f\"   ‚Ä¢ Alpha (peso dominio): {ALPHA}\")\n",
        "print(f\"   ‚Ä¢ Early stopping: {EARLY_STOPPING_PATIENCE} √©pocas\")\n",
        "print(f\"   ‚Ä¢ Save dir: {save_dir}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Entrenamiento Multi-task\n",
        "\n",
        "Entrenar con Domain Adaptation (PD + Domain classification con GRL).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ENTRENAR MODELO CON DOMAIN ADAPTATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ INICIANDO ENTRENAMIENTO CON DOMAIN ADAPTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_results = train_model_da(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion_pd=criterion_pd,\n",
        "    criterion_domain=criterion_domain,\n",
        "    device=device,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    alpha=ALPHA,\n",
        "    lambda_scheduler=lambda_scheduler,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    save_dir=str(save_dir),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Extraer resultados\n",
        "model = training_results[\"model\"]\n",
        "history = training_results[\"history\"]\n",
        "best_val_loss_pd = training_results[\"best_val_loss_pd\"]\n",
        "total_time = training_results[\"total_time\"]\n",
        "\n",
        "# Calcular mejor √©poca\n",
        "best_epoch = history[\"val_loss_pd\"].index(min(history[\"val_loss_pd\"])) + 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Resultados:\")\n",
        "print(f\"   ‚Ä¢ Mejor √©poca: {best_epoch}\")\n",
        "print(f\"   ‚Ä¢ Mejor val loss PD: {best_val_loss_pd:.4f}\")\n",
        "print(f\"   ‚Ä¢ Tiempo total: {total_time/60:.1f} minutos\")\n",
        "print(f\"   ‚Ä¢ Modelo guardado en: {save_dir / 'best_model_1d_da.pth'}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluaci√≥n Dual: Segment-Level y Patient-Level\n",
        "\n",
        "Evaluar el modelo en test set:\n",
        "1. Segment-level: m√©tricas directas por segmento\n",
        "2. Patient-level: agregaci√≥n de segmentos por paciente (seg√∫n paper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN SEGMENT-LEVEL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ EVALUACI√ìN EN TEST SET (SEGMENT-LEVEL)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluar modelo DA con embeddings\n",
        "test_metrics = evaluate_da(\n",
        "    model=model,\n",
        "    loader=test_loader,\n",
        "    criterion_pd=criterion_pd,\n",
        "    criterion_domain=criterion_domain,\n",
        "    device=device,\n",
        "    alpha=ALPHA,\n",
        "    return_embeddings=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS SEGMENT-LEVEL:\")\n",
        "print(f\"   ‚Ä¢ Loss PD: {test_metrics['loss_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss Domain: {test_metrics['loss_domain']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy PD: {test_metrics['acc_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1 PD: {test_metrics['f1_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Precision PD: {test_metrics['precision_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Recall PD: {test_metrics['recall_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy Domain: {test_metrics['acc_domain']:.4f}\")\n",
        "\n",
        "# Extraer datos para patient-level\n",
        "all_probs = test_metrics['probs_pd']\n",
        "all_labels = test_metrics['labels_pd']\n",
        "all_embeddings = test_metrics['embeddings']\n",
        "all_domains = test_metrics['labels_domain']\n",
        "\n",
        "print(f\"\\nüì¶ Datos extra√≠dos:\")\n",
        "print(f\"   ‚Ä¢ Probabilidades: {all_probs.shape}\")\n",
        "print(f\"   ‚Ä¢ Embeddings: {all_embeddings.shape}\")\n",
        "print(f\"   ‚Ä¢ Labels: {len(all_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN PATIENT-LEVEL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üë• EVALUACI√ìN PATIENT-LEVEL (AGREGACI√ìN DE SEGMENTOS)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Agregar predicciones por paciente\n",
        "patient_metrics = evaluate_patient_level(\n",
        "    all_probs=all_probs,\n",
        "    all_labels=all_labels,\n",
        "    patient_ids=patient_ids_test,\n",
        "    method='mean'  # Promedio de probabilidades\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS PATIENT-LEVEL:\")\n",
        "print(f\"   ‚Ä¢ N¬∞ Pacientes: {patient_metrics['n_patients']}\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {patient_metrics['acc']:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {patient_metrics['f1']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Precision: {patient_metrics['precision']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Recall: {patient_metrics['recall']:.4f}\")\n",
        "\n",
        "# Matriz de confusi√≥n patient-level\n",
        "cm_patient = patient_metrics['confusion_matrix']\n",
        "\n",
        "print(f\"\\nüìä MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL):\")\n",
        "print(f\"              Pred HC  Pred PD\")\n",
        "print(f\"Real HC       {cm_patient[0, 0]:7d}  {cm_patient[0, 1]:7d}\")\n",
        "print(f\"Real PD       {cm_patient[1, 0]:7d}  {cm_patient[1, 1]:7d}\")\n",
        "\n",
        "# Guardar m√©tricas\n",
        "save_metrics(\n",
        "    {\n",
        "        'segment_level': {\n",
        "            'acc': float(test_metrics['acc_pd']),\n",
        "            'f1': float(test_metrics['f1_pd']),\n",
        "            'precision': float(test_metrics['precision_pd']),\n",
        "            'recall': float(test_metrics['recall_pd']),\n",
        "        },\n",
        "        'patient_level': {\n",
        "            'acc': float(patient_metrics['acc']),\n",
        "            'f1': float(patient_metrics['f1']),\n",
        "            'precision': float(patient_metrics['precision']),\n",
        "            'recall': float(patient_metrics['recall']),\n",
        "            'confusion_matrix': cm_patient.tolist(),\n",
        "            'n_patients': patient_metrics['n_patients']\n",
        "        }\n",
        "    },\n",
        "    save_dir / \"test_metrics_1d_da.json\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüíæ M√©tricas guardadas en: {save_dir / 'test_metrics_1d_da.json'}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizaci√≥n\n",
        "\n",
        "1. Curvas de entrenamiento (dual loss)\n",
        "2. Matriz de confusi√≥n (patient-level)\n",
        "3. t-SNE de embeddings (verificaci√≥n DA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 1: PROGRESO DE ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä VISUALIZACI√ìN: PROGRESO DE ENTRENAMIENTO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_1d_training_progress(\n",
        "    history,\n",
        "    save_path=save_dir / \"training_progress_1d_da.png\",\n",
        "    show=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Gr√°fica de progreso guardada en: {save_dir / 'training_progress_1d_da.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 2: CONFUSION MATRIX (PATIENT-LEVEL)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä VISUALIZACI√ìN: MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_simple_confusion_matrix(\n",
        "    cm_patient,\n",
        "    class_names=[\"Healthy\", \"Parkinson\"],\n",
        "    title=\"Matriz de Confusi√≥n - Patient-Level (CNN1D_DA)\",\n",
        "    save_path=save_dir / \"confusion_matrix_patient_1d_da.png\",\n",
        "    show=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Matriz de confusi√≥n guardada en: {save_dir / 'confusion_matrix_patient_1d_da.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 3: t-SNE DE EMBEDDINGS (VERIFICACI√ìN DA)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üî¨ VISUALIZACI√ìN: t-SNE DE EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_tsne_embeddings(\n",
        "    embeddings=all_embeddings,\n",
        "    labels=all_labels,\n",
        "    domains=all_domains,\n",
        "    save_path=save_dir / \"tsne_embeddings_1d_da.png\",\n",
        "    show=True,\n",
        "    perplexity=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ t-SNE guardado en: {save_dir / 'tsne_embeddings_1d_da.png'}\")\n",
        "print(\"\\nüí° Interpretaci√≥n:\")\n",
        "print(\"   ‚Ä¢ Clusters por CLASE (HC/PD) = DA funciona ‚úì\")\n",
        "print(\"   ‚Ä¢ Clusters por DOMINIO = DA falla ‚úó\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Resumen Final\n",
        "\n",
        "Comparaci√≥n de m√©tricas segment-level vs patient-level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RESUMEN FINAL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìã RESUMEN FINAL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüéØ MODELO: CNN1D_DA (con Atenci√≥n Temporal y Domain Adaptation)\")\n",
        "print(f\"\\nüìä DATASET:\")\n",
        "print(f\"   ‚Ä¢ Total muestras: {len(X_combined)}\")\n",
        "print(f\"   ‚Ä¢ Train: {len(X_train)} samples | {len(set(patient_ids_train))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Val:   {len(X_val)} samples | {len(set(patient_ids_val))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Test:  {len(X_test)} samples | {len(set(patient_ids_test))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Dominios: {n_domains}\")\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO (√âpoca {best_epoch}):\")\n",
        "print(f\"   ‚Ä¢ Val Loss PD: {best_val_loss_pd:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà COMPARACI√ìN SEGMENT-LEVEL vs PATIENT-LEVEL:\")\n",
        "print(f\"   {'M√©trica':<15} {'Segment':<10} {'Patient':<10} {'Mejora':<10}\")\n",
        "print(f\"   {'-'*45}\")\n",
        "\n",
        "seg_acc = test_metrics['acc_pd']\n",
        "pat_acc = patient_metrics['acc']\n",
        "print(f\"   {'Accuracy':<15} {seg_acc:<10.4f} {pat_acc:<10.4f} {(pat_acc-seg_acc)*100:+.2f}%\")\n",
        "\n",
        "seg_f1 = test_metrics['f1_pd']\n",
        "pat_f1 = patient_metrics['f1']\n",
        "print(f\"   {'F1-Score':<15} {seg_f1:<10.4f} {pat_f1:<10.4f} {(pat_f1-seg_f1)*100:+.2f}%\")\n",
        "\n",
        "seg_prec = test_metrics['precision_pd']\n",
        "pat_prec = patient_metrics['precision']\n",
        "print(f\"   {'Precision':<15} {seg_prec:<10.4f} {pat_prec:<10.4f} {(pat_prec-seg_prec)*100:+.2f}%\")\n",
        "\n",
        "seg_rec = test_metrics['recall_pd']\n",
        "pat_rec = patient_metrics['recall']\n",
        "print(f\"   {'Recall':<15} {seg_rec:<10.4f} {pat_rec:<10.4f} {(pat_rec-seg_rec)*100:+.2f}%\")\n",
        "\n",
        "print(f\"\\nüìÅ ARCHIVOS GUARDADOS EN: {save_dir}\")\n",
        "print(f\"   ‚úì best_model_1d_da.pth\")\n",
        "print(f\"   ‚úì test_metrics_1d_da.json\")\n",
        "print(f\"   ‚úì training_progress_1d_da.png\")\n",
        "print(f\"   ‚úì confusion_matrix_patient_1d_da.png\")\n",
        "print(f\"   ‚úì tsne_embeddings_1d_da.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PIPELINE COMPLETADO EXITOSAMENTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüí° NOTAS:\")\n",
        "print(\"   ‚Ä¢ Patient-level aggregation reduce ruido de segmentos\")\n",
        "print(\"   ‚Ä¢ t-SNE verifica que DA mezcla dominios correctamente\")\n",
        "print(\"   ‚Ä¢ Comparar con CNN2D_DA para an√°lisis completo\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
