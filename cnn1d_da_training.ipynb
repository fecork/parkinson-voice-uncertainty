{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† CNN 1D con Domain Adaptation para Detecci√≥n de Parkinson\n",
        "## Modelo con Atenci√≥n Temporal y GRL - Speaker-Independent Split\n",
        "\n",
        "Este notebook entrena un modelo **CNN1D_DA** (con Domain Adaptation y atenci√≥n temporal) para clasificaci√≥n binaria Parkinson vs Healthy.\n",
        "\n",
        "### üìã Pipeline:\n",
        "1. **Setup**: Configuraci√≥n del entorno\n",
        "2. **Data Loading**: Carga de datos desde cache y transformaci√≥n a [B, F, T]\n",
        "3. **Split**: Train/Val/Test speaker-independent (70/15/15)\n",
        "4. **Model**: CNN1D_DA con 3 bloques Conv1D, atenci√≥n temporal, y dual-head\n",
        "5. **Training**: Entrenamiento multi-task con GRL y early stopping\n",
        "6. **Evaluation**: M√©tricas segment-level y patient-level\n",
        "7. **Visualization**: Curvas de training, confusion matrix, y t-SNE\n",
        "\n",
        "### ‚ö†Ô∏è PREREQUISITO:\n",
        "**Ejecutar primero `data_preprocessing.ipynb`** para generar el cache de datos preprocesados.\n",
        "\n",
        "### üìö Referencia:\n",
        "Implementaci√≥n seg√∫n Ibarra et al. (2023): \"Towards a Corpus (and Language)-Independent Screening of Parkinson's Disease from Voice and Speech through Domain Adaptation\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup y Configuraci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üß† CNN 1D-DA TRAINING (con Atenci√≥n Temporal y Domain Adaptation)\n",
            "======================================================================\n",
            "‚úÖ Librer√≠as cargadas correctamente\n",
            "üîß Dispositivo: cpu\n",
            "üì¶ PyTorch: 2.8.0+cpu\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORTS Y CONFIGURACI√ìN\n",
        "# ============================================================\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Agregar m√≥dulos propios al path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Importar m√≥dulos propios\n",
        "from modules.augmentation import create_augmented_dataset\n",
        "from modules.dataset import to_pytorch_tensors, speaker_independent_split, group_by_patient\n",
        "from modules.cnn1d_model import CNN1D_DA\n",
        "from modules.cnn1d_training import (\n",
        "    train_model_da, \n",
        "    evaluate_da,\n",
        "    evaluate_patient_level,\n",
        "    save_metrics\n",
        ")\n",
        "from modules.cnn1d_visualization import (\n",
        "    plot_1d_training_progress,\n",
        "    plot_tsne_embeddings,\n",
        "    plot_simple_confusion_matrix\n",
        ")\n",
        "from modules.cnn_utils import compute_class_weights_auto\n",
        "\n",
        "# Configuraci√≥n de matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Configuraci√≥n de PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Reporte de configuraci√≥n\n",
        "print(\"=\"*70)\n",
        "print(\"üß† CNN 1D-DA TRAINING (con Atenci√≥n Temporal y Domain Adaptation)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ Librer√≠as cargadas correctamente\")\n",
        "print(f\"üîß Dispositivo: {device}\")\n",
        "print(f\"üì¶ PyTorch: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga de Datos desde Cache\n",
        "\n",
        "‚ö†Ô∏è **CRITICAL**: Transformamos spectrogramas de [B, 1, F, T] a [B, F, T] para CNN1D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üìÅ CARGANDO DATOS DESDE CACHE\n",
            "======================================================================\n",
            "üíæ Cargando dataset desde cache...\n",
            "   üìÅ ./cache/healthy\\augmented_dataset_c6631e32.pkl\n",
            "‚úÖ Cache cargado exitosamente: 1553 muestras\n",
            "‚ö° Tiempo ahorrado: ~6.5 min\n",
            "üìä PyTorch tensors listos:\n",
            "  - X: (1553, 1, 65, 41)\n",
            "  - y_task: (1553,)  (dist={0: 1553})\n",
            "  - y_domain: (1553,)  (K dominios=13)\n",
            "üü¢ Healthy: 1553 muestras\n",
            "üíæ Cargando dataset desde cache...\n",
            "   üìÅ ./cache/parkinson\\augmented_dataset_c6631e32.pkl\n",
            "‚úÖ Cache cargado exitosamente: 1219 muestras\n",
            "‚ö° Tiempo ahorrado: ~6.5 min\n",
            "üìä PyTorch tensors listos:\n",
            "  - X: (1219, 1, 65, 41)\n",
            "  - y_task: (1219,)  (dist={0: 1219})\n",
            "  - y_domain: (1219,)  (K dominios=13)\n",
            "üî¥ Parkinson: 1219 muestras\n",
            "\n",
            "üîÑ Transformando datos para CNN1D...\n",
            "   Shape original: torch.Size([1553, 1, 65, 41]) (B, 1, F, T)\n",
            "   Shape para CNN1D: torch.Size([1553, 65, 41]) (B, F, T)\n",
            "\n",
            "üìä Dataset combinado: 2772 muestras\n",
            "   ‚Ä¢ Dominios √∫nicos: 26\n",
            "   ‚Ä¢ Pacientes √∫nicos: 14\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURACI√ìN DE RUTAS Y CARGA DE CACHE\n",
        "# ============================================================\n",
        "\n",
        "DATA_PATH_HEALTHY = \"./data/vowels_healthy\"\n",
        "DATA_PATH_PARKINSON = \"./data/vowels_pk\"\n",
        "CACHE_DIR_HEALTHY = \"./cache/healthy\"\n",
        "CACHE_DIR_PARKINSON = \"./cache/parkinson\"\n",
        "\n",
        "# Configuraci√≥n de augmentation (debe coincidir con data_preprocessing.ipynb)\n",
        "AUGMENTATION_TYPES = [\"original\", \"pitch_shift\", \"time_stretch\", \"noise\"]\n",
        "NUM_SPEC_AUGMENT_VERSIONS = 2\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìÅ CARGANDO DATOS DESDE CACHE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Cargar Healthy\n",
        "audio_files_healthy = list(Path(DATA_PATH_HEALTHY).glob(\"*.egg\"))\n",
        "augmented_dataset_healthy = create_augmented_dataset(\n",
        "    audio_files_healthy,\n",
        "    augmentation_types=AUGMENTATION_TYPES,\n",
        "    apply_spec_augment=True,\n",
        "    num_spec_augment_versions=NUM_SPEC_AUGMENT_VERSIONS,\n",
        "    use_cache=True,\n",
        "    cache_dir=CACHE_DIR_HEALTHY,\n",
        "    force_regenerate=False,\n",
        "    progress_every=5\n",
        ")\n",
        "X_healthy, y_task_healthy, y_domain_healthy, meta_healthy = to_pytorch_tensors(augmented_dataset_healthy)\n",
        "\n",
        "print(f\"üü¢ Healthy: {X_healthy.shape[0]} muestras\")\n",
        "\n",
        "# Cargar Parkinson\n",
        "audio_files_parkinson = list(Path(DATA_PATH_PARKINSON).glob(\"*.egg\"))\n",
        "augmented_dataset_parkinson = create_augmented_dataset(\n",
        "    audio_files_parkinson,\n",
        "    augmentation_types=AUGMENTATION_TYPES,\n",
        "    apply_spec_augment=True,\n",
        "    num_spec_augment_versions=NUM_SPEC_AUGMENT_VERSIONS,\n",
        "    use_cache=True,\n",
        "    cache_dir=CACHE_DIR_PARKINSON,\n",
        "    force_regenerate=False,\n",
        "    progress_every=5\n",
        ")\n",
        "X_parkinson, y_task_parkinson, y_domain_parkinson, meta_parkinson = to_pytorch_tensors(augmented_dataset_parkinson)\n",
        "\n",
        "print(f\"üî¥ Parkinson: {X_parkinson.shape[0]} muestras\")\n",
        "\n",
        "# CRITICAL: Transformar para CNN1D\n",
        "print(\"\\nüîÑ Transformando datos para CNN1D...\")\n",
        "print(f\"   Shape original: {X_healthy.shape} (B, 1, F, T)\")\n",
        "\n",
        "# Remover dimensi√≥n de canal: [B, 1, 65, 41] ‚Üí [B, 65, 41]\n",
        "X_healthy = X_healthy.squeeze(1)\n",
        "X_parkinson = X_parkinson.squeeze(1)\n",
        "\n",
        "print(f\"   Shape para CNN1D: {X_healthy.shape} (B, F, T)\")\n",
        "\n",
        "# Combinar datasets\n",
        "X_combined = torch.cat([X_healthy, X_parkinson], dim=0)\n",
        "y_task_combined = torch.cat([\n",
        "    torch.zeros(len(X_healthy), dtype=torch.long),\n",
        "    torch.ones(len(X_parkinson), dtype=torch.long)\n",
        "], dim=0)\n",
        "\n",
        "# Ajustar dominios para evitar colisiones\n",
        "max_domain_hc = y_domain_healthy.max().item()\n",
        "y_domain_combined = torch.cat([\n",
        "    y_domain_healthy,\n",
        "    y_domain_parkinson + max_domain_hc + 1\n",
        "], dim=0)\n",
        "\n",
        "# Metadata combinada (para patient grouping)\n",
        "all_metadata = meta_healthy + meta_parkinson\n",
        "patient_ids = [m.subject_id for m in all_metadata]\n",
        "\n",
        "print(f\"\\nüìä Dataset combinado: {len(X_combined)} muestras\")\n",
        "print(f\"   ‚Ä¢ Dominios √∫nicos: {len(torch.unique(y_domain_combined))}\")\n",
        "print(f\"   ‚Ä¢ Pacientes √∫nicos: {len(set(patient_ids))}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split Speaker-Independent Train/Val/Test\n",
        "\n",
        "**Cr√≠tico**: Ning√∫n speaker se repite entre splits para evitar data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üìä SPLIT SPEAKER-INDEPENDENT\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "SPEAKER-INDEPENDENT SPLIT\n",
            "======================================================================\n",
            "Pacientes √∫nicos: 14\n",
            "  - Train: 9 pacientes ‚Üí 1600 samples\n",
            "  - Val:   2 pacientes ‚Üí 120 samples\n",
            "  - Test:  3 pacientes ‚Üí 1052 samples\n",
            "======================================================================\n",
            "\n",
            "\n",
            "üìä Datos divididos:\n",
            "   ‚Ä¢ Train: 1600 samples de 9 pacientes\n",
            "   ‚Ä¢ Val:   120 samples de 2 pacientes\n",
            "   ‚Ä¢ Test:  1052 samples de 3 pacientes\n",
            "\n",
            "‚úÖ DataLoaders creados (batch_size=32)\n",
            "   ‚Ä¢ Train batches: 50\n",
            "   ‚Ä¢ Val batches: 2\n",
            "   ‚Ä¢ Test batches: 17\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SPEAKER-INDEPENDENT SPLIT\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä SPLIT SPEAKER-INDEPENDENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Obtener √≠ndices por speaker\n",
        "train_idx, val_idx, test_idx = speaker_independent_split(\n",
        "    all_metadata,\n",
        "    test_size=0.15,\n",
        "    val_size=0.176,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Extraer datos\n",
        "X_train = X_combined[train_idx]\n",
        "X_val = X_combined[val_idx]\n",
        "X_test = X_combined[test_idx]\n",
        "\n",
        "y_task_train = y_task_combined[train_idx]\n",
        "y_task_val = y_task_combined[val_idx]\n",
        "y_task_test = y_task_combined[test_idx]\n",
        "\n",
        "y_domain_train = y_domain_combined[train_idx]\n",
        "y_domain_val = y_domain_combined[val_idx]\n",
        "y_domain_test = y_domain_combined[test_idx]\n",
        "\n",
        "# Patient IDs para evaluaci√≥n\n",
        "patient_ids_train = [patient_ids[i] for i in train_idx]\n",
        "patient_ids_val = [patient_ids[i] for i in val_idx]\n",
        "patient_ids_test = [patient_ids[i] for i in test_idx]\n",
        "\n",
        "n_patients_train = len(set(patient_ids_train))\n",
        "n_patients_val = len(set(patient_ids_val))\n",
        "n_patients_test = len(set(patient_ids_test))\n",
        "\n",
        "print(f\"\\nüìä Datos divididos:\")\n",
        "print(f\"   ‚Ä¢ Train: {len(X_train)} samples de {n_patients_train} pacientes\")\n",
        "print(f\"   ‚Ä¢ Val:   {len(X_val)} samples de {n_patients_val} pacientes\")\n",
        "print(f\"   ‚Ä¢ Test:  {len(X_test)} samples de {n_patients_test} pacientes\")\n",
        "\n",
        "# Warning si dataset muy peque√±o\n",
        "total_patients = len(set(patient_ids))\n",
        "if total_patients < 20:\n",
        "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Dataset peque√±o ({total_patients} pacientes √∫nicos)\")\n",
        "    print(f\"   ‚Ä¢ Speaker-independent split puede generar splits desbalanceados\")\n",
        "    print(f\"   ‚Ä¢ Para datasets peque√±os, considerar:\")\n",
        "    print(f\"     - Usar split normal (no speaker-independent)\")\n",
        "    print(f\"     - Usar K-fold CV para resultados robustos\")\n",
        "    print(f\"     - A√±adir m√°s datos de m√°s pacientes\")\n",
        "\n",
        "# Crear DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_task_train, y_domain_train)\n",
        "val_dataset = TensorDataset(X_val, y_task_val, y_domain_val)\n",
        "test_dataset = TensorDataset(X_test, y_task_test, y_domain_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders creados (batch_size={BATCH_SIZE})\")\n",
        "print(f\"   ‚Ä¢ Train batches: {len(train_loader)}\")\n",
        "print(f\"   ‚Ä¢ Val batches: {len(val_loader)}\")\n",
        "print(f\"   ‚Ä¢ Test batches: {len(test_loader)}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Crear Modelo CNN1D_DA\n",
        "\n",
        "Arquitectura seg√∫n paper:\n",
        "- 3 bloques Conv1D (kernels: 5, 11, 21)\n",
        "- Atenci√≥n temporal (split + softmax)\n",
        "- Dual-head: PD detector + Domain detector con GRL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üèóÔ∏è  CREANDO MODELO CNN1D_DA\n",
            "======================================================================\n",
            "\n",
            "üìä Configuraci√≥n:\n",
            "   ‚Ä¢ Dominios √∫nicos: 26\n",
            "   ‚Ä¢ Clases PD: 2 (Healthy/Parkinson)\n",
            "   ‚Ä¢ Input shape: (B, F=65, T=41)\n",
            "   ‚Ä¢ Arquitectura: 3 bloques Conv1D + Atenci√≥n Temporal\n",
            "\n",
            "‚úÖ Modelo creado en device: cpu\n",
            "\n",
            "======================================================================\n",
            "ARQUITECTURA DEL MODELO CNN1D_DA\n",
            "======================================================================\n",
            "CNN1D_DA(\n",
            "  (block1): Sequential(\n",
            "    (0): Conv1d(65, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): MaxPool1d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,), bias=False)\n",
            "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): MaxPool1d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Conv1d(128, 128, kernel_size=(21,), stride=(1,), padding=(10,), bias=False)\n",
            "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (pd_head): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
            "  )\n",
            "  (dom_head): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=26, bias=True)\n",
            "  )\n",
            "  (grl): GradientReversalLayer()\n",
            ")\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Par√°metros totales: 465,756\n",
            "Par√°metros entrenables: 465,756\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üß™ Test Forward Pass:\n",
            "   ‚Ä¢ Input: torch.Size([2, 65, 41])\n",
            "   ‚Ä¢ Output PD: torch.Size([2, 2])\n",
            "   ‚Ä¢ Output Domain: torch.Size([2, 26])\n",
            "   ‚Ä¢ Embeddings: torch.Size([2, 64])\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CREAR MODELO CNN1D_DA\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üèóÔ∏è  CREANDO MODELO CNN1D_DA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# N√∫mero de dominios √∫nicos\n",
        "n_domains = len(torch.unique(y_domain_combined))\n",
        "\n",
        "print(f\"\\nüìä Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ Dominios √∫nicos: {n_domains}\")\n",
        "print(f\"   ‚Ä¢ Clases PD: 2 (Healthy/Parkinson)\")\n",
        "print(f\"   ‚Ä¢ Input shape: (B, F=65, T=41)\")\n",
        "print(f\"   ‚Ä¢ Arquitectura: 3 bloques Conv1D + Atenci√≥n Temporal\")\n",
        "\n",
        "# Crear modelo\n",
        "model = CNN1D_DA(\n",
        "    in_ch=65,      # Freq bins\n",
        "    c1=64,         # Canales bloque 1\n",
        "    c2=128,        # Canales bloque 2\n",
        "    c3=128,        # Canales bloque 3\n",
        "    p_drop=0.3,    # Dropout\n",
        "    num_pd=2,      # HC/PD\n",
        "    num_domains=n_domains\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo creado en device: {device}\")\n",
        "\n",
        "# Mostrar arquitectura\n",
        "from modules.cnn1d_model import print_model_summary\n",
        "print_model_summary(model)\n",
        "\n",
        "# Test con batch dummy\n",
        "print(\"üß™ Test Forward Pass:\")\n",
        "x_dummy = torch.randn(2, 65, 41).to(device)\n",
        "logits_pd, logits_domain, embeddings = model(x_dummy, return_embeddings=True)\n",
        "print(f\"   ‚Ä¢ Input: {x_dummy.shape}\")\n",
        "print(f\"   ‚Ä¢ Output PD: {logits_pd.shape}\")\n",
        "print(f\"   ‚Ä¢ Output Domain: {logits_domain.shape}\")\n",
        "print(f\"   ‚Ä¢ Embeddings: {embeddings.shape}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuraci√≥n de Entrenamiento\n",
        "\n",
        "Seg√∫n paper Ibarra et al.:\n",
        "- Optimizer: SGD (lr=0.1, momentum=0.9)\n",
        "- LR Scheduler: StepLR\n",
        "- Lambda GRL: scheduler lineal 0‚Üí1\n",
        "- Weighted CE si desbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "‚öôÔ∏è  CONFIGURACI√ìN DE ENTRENAMIENTO (IBARRA 2023)\n",
            "======================================================================\n",
            "\n",
            "üìä Calculando class weights:\n",
            "   ‚ö†Ô∏è  Desbalance detectado (min class: 23.8%). Aplicando pesos.\n",
            "   ‚ö†Ô∏è  Desbalance detectado (min class: 0.0%). Aplicando pesos.\n",
            "\n",
            "üìã Configuraci√≥n:\n",
            "   ‚Ä¢ √âpocas m√°ximas: 100\n",
            "   ‚Ä¢ Learning rate: 0.1 (SGD)\n",
            "   ‚Ä¢ LR Scheduler: StepLR (step=30, gamma=0.1)\n",
            "   ‚Ä¢ Lambda GRL: scheduler lineal 0‚Üí1\n",
            "   ‚Ä¢ Alpha (peso dominio): 1.0\n",
            "   ‚Ä¢ Early stopping: 15 √©pocas\n",
            "   ‚Ä¢ Save dir: results\\cnn1d_da\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURAR ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚öôÔ∏è  CONFIGURACI√ìN DE ENTRENAMIENTO (IBARRA 2023)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hiperpar√°metros seg√∫n paper\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.1  # SGD seg√∫n Ibarra\n",
        "ALPHA = 1.0  # Peso de loss_domain\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "# Calcular class weights autom√°ticos\n",
        "print(\"\\nüìä Calculando class weights:\")\n",
        "pd_weights = compute_class_weights_auto(y_task_train, threshold=0.4)\n",
        "\n",
        "# DEBUG: Verificar distribuci√≥n de dominios\n",
        "unique_domains_train = torch.unique(y_domain_train)\n",
        "print(f\"\\nüîç DEBUG Dominios en train:\")\n",
        "print(f\"   ‚Ä¢ Dominios √∫nicos: {len(unique_domains_train)}\")\n",
        "print(f\"   ‚Ä¢ Rango: {unique_domains_train.min().item()} - {unique_domains_train.max().item()}\")\n",
        "\n",
        "# Para domain weights, NO usar auto (puede dar NaN si hay dominios sin muestras)\n",
        "# Usar weights manuales solo si hay desbalance extremo\n",
        "domain_counts = torch.bincount(y_domain_train, minlength=n_domains)\n",
        "print(f\"   ‚Ä¢ Samples por dominio: min={domain_counts[domain_counts>0].min().item()}, max={domain_counts.max().item()}\")\n",
        "\n",
        "# Criterio: solo aplicar weights si hay dominios con <5 samples\n",
        "if (domain_counts > 0).sum() < n_domains or domain_counts[domain_counts>0].min() < 5:\n",
        "    print(f\"   ‚ö†Ô∏è  Algunos dominios con pocas muestras, usando weights\")\n",
        "    # Calcular weights solo para dominios presentes\n",
        "    weights = torch.zeros(n_domains)\n",
        "    for i in range(n_domains):\n",
        "        if domain_counts[i] > 0:\n",
        "            weights[i] = 1.0 / domain_counts[i]\n",
        "    weights = weights / weights.sum() * len(unique_domains_train)\n",
        "    domain_weights = weights\n",
        "else:\n",
        "    print(f\"   ‚úì Dominios balanceados, sin weights\")\n",
        "    domain_weights = None\n",
        "\n",
        "# Crear criterios\n",
        "if pd_weights is not None:\n",
        "    criterion_pd = nn.CrossEntropyLoss(weight=pd_weights.to(device))\n",
        "else:\n",
        "    criterion_pd = nn.CrossEntropyLoss()\n",
        "\n",
        "if domain_weights is not None:\n",
        "    # Filtrar weights de dominios vac√≠os para evitar NaN\n",
        "    criterion_domain = nn.CrossEntropyLoss(weight=domain_weights.to(device))\n",
        "else:\n",
        "    criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "# Verificar que no hay NaN en weights\n",
        "if domain_weights is not None:\n",
        "    if torch.isnan(domain_weights).any():\n",
        "        print(f\"   ‚ö†Ô∏è  NaN detectado en domain weights, usando CE sin pesos\")\n",
        "        criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer SGD con LR 0.1 (Ibarra 2023)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# LR Scheduler: StepLR\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=30,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# Lambda scheduler lineal 0‚Üí1\n",
        "lambda_scheduler = lambda epoch: epoch / N_EPOCHS\n",
        "\n",
        "# Directorio para guardar\n",
        "save_dir = Path(\"./results/cnn1d_da\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìã Configuraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ √âpocas m√°ximas: {N_EPOCHS}\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE} (SGD)\")\n",
        "print(f\"   ‚Ä¢ LR Scheduler: StepLR (step=30, gamma=0.1)\")\n",
        "print(f\"   ‚Ä¢ Lambda GRL: scheduler lineal 0‚Üí1\")\n",
        "print(f\"   ‚Ä¢ Alpha (peso dominio): {ALPHA}\")\n",
        "print(f\"   ‚Ä¢ Early stopping: {EARLY_STOPPING_PATIENCE} √©pocas\")\n",
        "print(f\"   ‚Ä¢ Save dir: {save_dir}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Entrenamiento Multi-task\n",
        "\n",
        "Entrenar con Domain Adaptation (PD + Domain classification con GRL).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ INICIANDO ENTRENAMIENTO CON DOMAIN ADAPTATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "INICIO DE ENTRENAMIENTO CNN1D_DA\n",
            "======================================================================\n",
            "√âpocas m√°ximas: 100\n",
            "Early stopping patience: 15\n",
            "Alpha (peso dominio): 1.0\n",
            "Device: cpu\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "√âpoca   1/100 | Œª=0.000 | Train: L_PD=0.2823 L_Dom=3.0800 | Val: L_PD=0.1852 F1=0.0000 | 2.6s\n",
            "√âpoca   2/100 | Œª=0.010 | Train: L_PD=0.1385 L_Dom=3.0455 | Val: L_PD=0.4894 F1=0.0000 | 1.3s\n",
            "√âpoca   3/100 | Œª=0.020 | Train: L_PD=0.1030 L_Dom=3.0041 | Val: L_PD=0.8676 F1=0.0000 | 1.1s\n",
            "√âpoca   4/100 | Œª=0.030 | Train: L_PD=0.0991 L_Dom=2.9779 | Val: L_PD=1.6144 F1=0.0000 | 1.0s\n",
            "√âpoca   5/100 | Œª=0.040 | Train: L_PD=0.1031 L_Dom=3.0890 | Val: L_PD=0.4291 F1=0.0000 | 0.9s\n",
            "√âpoca   6/100 | Œª=0.050 | Train: L_PD=0.0599 L_Dom=3.0632 | Val: L_PD=0.8189 F1=0.0000 | 0.9s\n",
            "√âpoca   7/100 | Œª=0.060 | Train: L_PD=0.0457 L_Dom=3.0753 | Val: L_PD=1.0804 F1=0.0000 | 1.1s\n",
            "√âpoca   8/100 | Œª=0.070 | Train: L_PD=0.0818 L_Dom=3.0967 | Val: L_PD=1.4577 F1=0.0000 | 1.1s\n",
            "√âpoca   9/100 | Œª=0.080 | Train: L_PD=0.0356 L_Dom=3.0657 | Val: L_PD=1.0197 F1=0.0000 | 1.0s\n",
            "√âpoca  10/100 | Œª=0.090 | Train: L_PD=0.0312 L_Dom=3.0894 | Val: L_PD=0.7530 F1=0.0000 | 1.0s\n",
            "√âpoca  11/100 | Œª=0.100 | Train: L_PD=0.0306 L_Dom=3.0658 | Val: L_PD=2.6826 F1=0.0000 | 1.2s\n",
            "√âpoca  12/100 | Œª=0.110 | Train: L_PD=0.0608 L_Dom=3.0737 | Val: L_PD=1.0172 F1=0.0000 | 1.0s\n",
            "√âpoca  13/100 | Œª=0.120 | Train: L_PD=0.0488 L_Dom=3.0867 | Val: L_PD=0.8031 F1=0.0000 | 0.9s\n",
            "√âpoca  14/100 | Œª=0.130 | Train: L_PD=0.0126 L_Dom=3.0754 | Val: L_PD=1.2522 F1=0.0000 | 0.9s\n",
            "√âpoca  15/100 | Œª=0.140 | Train: L_PD=0.0066 L_Dom=3.0739 | Val: L_PD=1.6243 F1=0.0000 | 0.8s\n",
            "√âpoca  16/100 | Œª=0.150 | Train: L_PD=0.0401 L_Dom=3.0830 | Val: L_PD=1.3281 F1=0.0000 | 0.9s\n",
            "\n",
            "‚ö†Ô∏è  Early stopping en √©poca 16\n",
            "    Mejor √©poca: 1\n",
            "    Mejor val_loss_pd: 0.1852\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ENTRENAMIENTO COMPLETADO\n",
            "======================================================================\n",
            "Tiempo total: 0.3 minutos\n",
            "Mejor val_loss_pd: 0.1852\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ENTRENAMIENTO COMPLETADO\n",
            "======================================================================\n",
            "\n",
            "üìä Resultados:\n",
            "   ‚Ä¢ Mejor √©poca: 1\n",
            "   ‚Ä¢ Mejor val loss PD: 0.1852\n",
            "   ‚Ä¢ Tiempo total: 0.3 minutos\n",
            "   ‚Ä¢ Modelo guardado en: results\\cnn1d_da\\best_model_1d_da.pth\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ENTRENAR MODELO CON DOMAIN ADAPTATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ INICIANDO ENTRENAMIENTO CON DOMAIN ADAPTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_results = train_model_da(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion_pd=criterion_pd,\n",
        "    criterion_domain=criterion_domain,\n",
        "    device=device,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    alpha=ALPHA,\n",
        "    lambda_scheduler=lambda_scheduler,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    save_dir=str(save_dir),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Extraer resultados\n",
        "model = training_results[\"model\"]\n",
        "history = training_results[\"history\"]\n",
        "best_val_loss_pd = training_results[\"best_val_loss_pd\"]\n",
        "total_time = training_results[\"total_time\"]\n",
        "\n",
        "# Calcular mejor √©poca\n",
        "best_epoch = history[\"val_loss_pd\"].index(min(history[\"val_loss_pd\"])) + 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Resultados:\")\n",
        "print(f\"   ‚Ä¢ Mejor √©poca: {best_epoch}\")\n",
        "print(f\"   ‚Ä¢ Mejor val loss PD: {best_val_loss_pd:.4f}\")\n",
        "print(f\"   ‚Ä¢ Tiempo total: {total_time/60:.1f} minutos\")\n",
        "print(f\"   ‚Ä¢ Modelo guardado en: {save_dir / 'best_model_1d_da.pth'}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluaci√≥n Dual: Segment-Level y Patient-Level\n",
        "\n",
        "Evaluar el modelo en test set:\n",
        "1. Segment-level: m√©tricas directas por segmento\n",
        "2. Patient-level: agregaci√≥n de segmentos por paciente (seg√∫n paper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üéØ EVALUACI√ìN EN TEST SET (SEGMENT-LEVEL)\n",
            "======================================================================\n",
            "\n",
            "üìä M√âTRICAS SEGMENT-LEVEL:\n",
            "   ‚Ä¢ Loss PD: 0.8062\n",
            "   ‚Ä¢ Loss Domain: nan\n",
            "   ‚Ä¢ Accuracy PD: 0.7357\n",
            "   ‚Ä¢ F1 PD: 0.0000\n",
            "   ‚Ä¢ Precision PD: 0.0000\n",
            "   ‚Ä¢ Recall PD: 0.0000\n",
            "   ‚Ä¢ Accuracy Domain: 0.0000\n",
            "\n",
            "üì¶ Datos extra√≠dos:\n",
            "   ‚Ä¢ Probabilidades: (1052, 2)\n",
            "   ‚Ä¢ Embeddings: (1052, 64)\n",
            "   ‚Ä¢ Labels: 1052\n",
            "   ‚Ä¢ Domains: 1052\n",
            "   ‚Ä¢ Patient IDs test: 1052\n",
            "\n",
            "‚úÖ Validaci√≥n: todas las longitudes coinciden (1052 samples)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN SEGMENT-LEVEL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ EVALUACI√ìN EN TEST SET (SEGMENT-LEVEL)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluar modelo DA con embeddings\n",
        "test_metrics = evaluate_da(\n",
        "    model=model,\n",
        "    loader=test_loader,\n",
        "    criterion_pd=criterion_pd,\n",
        "    criterion_domain=criterion_domain,\n",
        "    device=device,\n",
        "    alpha=ALPHA,\n",
        "    return_embeddings=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS SEGMENT-LEVEL:\")\n",
        "print(f\"   ‚Ä¢ Loss PD: {test_metrics['loss_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Loss Domain: {test_metrics['loss_domain']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy PD: {test_metrics['acc_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1 PD: {test_metrics['f1_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Precision PD: {test_metrics['precision_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Recall PD: {test_metrics['recall_pd']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy Domain: {test_metrics['acc_domain']:.4f}\")\n",
        "\n",
        "# Extraer datos para patient-level\n",
        "all_probs = test_metrics['probs_pd']\n",
        "all_labels = test_metrics['labels_pd']\n",
        "all_embeddings = test_metrics['embeddings']\n",
        "all_domains = test_metrics['labels_domain']\n",
        "\n",
        "print(f\"\\nüì¶ Datos extra√≠dos:\")\n",
        "print(f\"   ‚Ä¢ Probabilidades: {all_probs.shape}\")\n",
        "print(f\"   ‚Ä¢ Embeddings: {all_embeddings.shape}\")\n",
        "print(f\"   ‚Ä¢ Labels: {len(all_labels)}\")\n",
        "print(f\"   ‚Ä¢ Domains: {len(all_domains)}\")\n",
        "print(f\"   ‚Ä¢ Patient IDs test: {len(patient_ids_test)}\")\n",
        "\n",
        "# Validaci√≥n de consistencia\n",
        "assert len(all_probs) == len(all_labels) == len(patient_ids_test), \\\n",
        "    f\"Longitudes inconsistentes: probs={len(all_probs)}, labels={len(all_labels)}, patients={len(patient_ids_test)}\"\n",
        "print(f\"\\n‚úÖ Validaci√≥n: todas las longitudes coinciden ({len(all_probs)} samples)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üë• EVALUACI√ìN PATIENT-LEVEL (AGREGACI√ìN DE SEGMENTOS)\n",
            "======================================================================\n",
            "\n",
            "üìä M√âTRICAS PATIENT-LEVEL:\n",
            "   ‚Ä¢ N¬∞ Pacientes: 3\n",
            "   ‚Ä¢ Accuracy: 1.0000\n",
            "   ‚Ä¢ F1-Score: 0.0000\n",
            "   ‚Ä¢ Precision: 0.0000\n",
            "   ‚Ä¢ Recall: 0.0000\n",
            "\n",
            "üìä MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL):\n",
            "              Pred HC  Pred PD\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for axis 1 with size 1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m              Pred HC  Pred PD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal HC       \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm_patient[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm_patient[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal PD       \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm_patient[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcm_patient[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Guardar m√©tricas\u001b[39;00m\n",
            "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN PATIENT-LEVEL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üë• EVALUACI√ìN PATIENT-LEVEL (AGREGACI√ìN DE SEGMENTOS)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Agregar predicciones por paciente\n",
        "patient_metrics = evaluate_patient_level(\n",
        "    all_probs=all_probs,\n",
        "    all_labels=all_labels,\n",
        "    patient_ids=patient_ids_test,\n",
        "    method='mean'  # Promedio de probabilidades\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS PATIENT-LEVEL:\")\n",
        "print(f\"   ‚Ä¢ N¬∞ Pacientes: {patient_metrics['n_patients']}\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {patient_metrics['acc']:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {patient_metrics['f1']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Precision: {patient_metrics['precision']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Recall: {patient_metrics['recall']:.4f}\")\n",
        "\n",
        "# Matriz de confusi√≥n patient-level\n",
        "cm_patient = patient_metrics['confusion_matrix']\n",
        "\n",
        "print(f\"\\nüìä MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL):\")\n",
        "print(f\"              Pred HC  Pred PD\")\n",
        "print(f\"Real HC       {cm_patient[0, 0]:7d}  {cm_patient[0, 1]:7d}\")\n",
        "print(f\"Real PD       {cm_patient[1, 0]:7d}  {cm_patient[1, 1]:7d}\")\n",
        "\n",
        "# Guardar m√©tricas\n",
        "save_metrics(\n",
        "    {\n",
        "        'segment_level': {\n",
        "            'acc': float(test_metrics['acc_pd']),\n",
        "            'f1': float(test_metrics['f1_pd']),\n",
        "            'precision': float(test_metrics['precision_pd']),\n",
        "            'recall': float(test_metrics['recall_pd']),\n",
        "        },\n",
        "        'patient_level': {\n",
        "            'acc': float(patient_metrics['acc']),\n",
        "            'f1': float(patient_metrics['f1']),\n",
        "            'precision': float(patient_metrics['precision']),\n",
        "            'recall': float(patient_metrics['recall']),\n",
        "            'confusion_matrix': cm_patient.tolist(),\n",
        "            'n_patients': patient_metrics['n_patients']\n",
        "        }\n",
        "    },\n",
        "    save_dir / \"test_metrics_1d_da.json\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüíæ M√©tricas guardadas en: {save_dir / 'test_metrics_1d_da.json'}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizaci√≥n\n",
        "\n",
        "1. Curvas de entrenamiento (dual loss)\n",
        "2. Matriz de confusi√≥n (patient-level)\n",
        "3. t-SNE de embeddings (verificaci√≥n DA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 1: PROGRESO DE ENTRENAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä VISUALIZACI√ìN: PROGRESO DE ENTRENAMIENTO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_1d_training_progress(\n",
        "    history,\n",
        "    save_path=save_dir / \"training_progress_1d_da.png\",\n",
        "    show=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Gr√°fica de progreso guardada en: {save_dir / 'training_progress_1d_da.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 2: CONFUSION MATRIX (PATIENT-LEVEL)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä VISUALIZACI√ìN: MATRIZ DE CONFUSI√ìN (PATIENT-LEVEL)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_simple_confusion_matrix(\n",
        "    cm_patient,\n",
        "    class_names=[\"Healthy\", \"Parkinson\"],\n",
        "    title=\"Matriz de Confusi√≥n - Patient-Level (CNN1D_DA)\",\n",
        "    save_path=save_dir / \"confusion_matrix_patient_1d_da.png\",\n",
        "    show=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Matriz de confusi√≥n guardada en: {save_dir / 'confusion_matrix_patient_1d_da.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZACI√ìN 3: t-SNE DE EMBEDDINGS (VERIFICACI√ìN DA)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üî¨ VISUALIZACI√ìN: t-SNE DE EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plot_tsne_embeddings(\n",
        "    embeddings=all_embeddings,\n",
        "    labels=all_labels,\n",
        "    domains=all_domains,\n",
        "    save_path=save_dir / \"tsne_embeddings_1d_da.png\",\n",
        "    show=True,\n",
        "    perplexity=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ t-SNE guardado en: {save_dir / 'tsne_embeddings_1d_da.png'}\")\n",
        "print(\"\\nüí° Interpretaci√≥n:\")\n",
        "print(\"   ‚Ä¢ Clusters por CLASE (HC/PD) = DA funciona ‚úì\")\n",
        "print(\"   ‚Ä¢ Clusters por DOMINIO = DA falla ‚úó\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Resumen Final\n",
        "\n",
        "Comparaci√≥n de m√©tricas segment-level vs patient-level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RESUMEN FINAL\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìã RESUMEN FINAL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüéØ MODELO: CNN1D_DA (con Atenci√≥n Temporal y Domain Adaptation)\")\n",
        "print(f\"\\nüìä DATASET:\")\n",
        "print(f\"   ‚Ä¢ Total muestras: {len(X_combined)}\")\n",
        "print(f\"   ‚Ä¢ Train: {len(X_train)} samples | {len(set(patient_ids_train))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Val:   {len(X_val)} samples | {len(set(patient_ids_val))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Test:  {len(X_test)} samples | {len(set(patient_ids_test))} pacientes\")\n",
        "print(f\"   ‚Ä¢ Dominios: {n_domains}\")\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO (√âpoca {best_epoch}):\")\n",
        "print(f\"   ‚Ä¢ Val Loss PD: {best_val_loss_pd:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà COMPARACI√ìN SEGMENT-LEVEL vs PATIENT-LEVEL:\")\n",
        "print(f\"   {'M√©trica':<15} {'Segment':<10} {'Patient':<10} {'Mejora':<10}\")\n",
        "print(f\"   {'-'*45}\")\n",
        "\n",
        "seg_acc = test_metrics['acc_pd']\n",
        "pat_acc = patient_metrics['acc']\n",
        "print(f\"   {'Accuracy':<15} {seg_acc:<10.4f} {pat_acc:<10.4f} {(pat_acc-seg_acc)*100:+.2f}%\")\n",
        "\n",
        "seg_f1 = test_metrics['f1_pd']\n",
        "pat_f1 = patient_metrics['f1']\n",
        "print(f\"   {'F1-Score':<15} {seg_f1:<10.4f} {pat_f1:<10.4f} {(pat_f1-seg_f1)*100:+.2f}%\")\n",
        "\n",
        "seg_prec = test_metrics['precision_pd']\n",
        "pat_prec = patient_metrics['precision']\n",
        "print(f\"   {'Precision':<15} {seg_prec:<10.4f} {pat_prec:<10.4f} {(pat_prec-seg_prec)*100:+.2f}%\")\n",
        "\n",
        "seg_rec = test_metrics['recall_pd']\n",
        "pat_rec = patient_metrics['recall']\n",
        "print(f\"   {'Recall':<15} {seg_rec:<10.4f} {pat_rec:<10.4f} {(pat_rec-seg_rec)*100:+.2f}%\")\n",
        "\n",
        "print(f\"\\nüìÅ ARCHIVOS GUARDADOS EN: {save_dir}\")\n",
        "print(f\"   ‚úì best_model_1d_da.pth\")\n",
        "print(f\"   ‚úì test_metrics_1d_da.json\")\n",
        "print(f\"   ‚úì training_progress_1d_da.png\")\n",
        "print(f\"   ‚úì confusion_matrix_patient_1d_da.png\")\n",
        "print(f\"   ‚úì tsne_embeddings_1d_da.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PIPELINE COMPLETADO EXITOSAMENTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüí° NOTAS:\")\n",
        "print(\"   ‚Ä¢ Patient-level aggregation reduce ruido de segmentos\")\n",
        "print(\"   ‚Ä¢ t-SNE verifica que DA mezcla dominios correctamente\")\n",
        "print(\"   ‚Ä¢ Comparar con CNN2D_DA para an√°lisis completo\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "parkinson_env",
      "language": "python",
      "name": "parkinson_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
