"""
Funciones de visualizaciÃ³n para modelos con incertidumbre.
"""

import numpy as np
import matplotlib.pyplot as plt


def plot_uncertainty_histograms(metrics, save_path=None, show=True):
    """
    Histogramas de incertidumbres separando aciertos vs errores.

    Args:
        metrics: Dict con resultados de evaluate_with_uncertainty
        save_path: Path para guardar la figura
        show: Si mostrar la figura
    """
    preds = metrics["predictions"]
    targets = metrics["targets"]
    entropy = metrics["entropy"]
    epistemic = metrics["epistemic"]
    aleatoric = metrics["aleatoric"]

    correct_mask = preds == targets

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # EntropÃ­a
    axes[0].hist(
        entropy[correct_mask],
        bins=30,
        alpha=0.6,
        label="Correcto",
        color="green",
        density=True,
    )
    axes[0].hist(
        entropy[~correct_mask],
        bins=30,
        alpha=0.6,
        label="Incorrecto",
        color="red",
        density=True,
    )
    axes[0].set_xlabel("EntropÃ­a Predictiva")
    axes[0].set_ylabel("Densidad")
    axes[0].set_title("EntropÃ­a Total (H(pÌ„))")
    axes[0].legend()
    axes[0].grid(alpha=0.3)

    # EpistÃ©mica
    axes[1].hist(
        epistemic[correct_mask],
        bins=30,
        alpha=0.6,
        label="Correcto",
        color="green",
        density=True,
    )
    axes[1].hist(
        epistemic[~correct_mask],
        bins=30,
        alpha=0.6,
        label="Incorrecto",
        color="red",
        density=True,
    )
    axes[1].set_xlabel("Incertidumbre EpistÃ©mica (BALD)")
    axes[1].set_ylabel("Densidad")
    axes[1].set_title("EpistÃ©mica (Modelo)")
    axes[1].legend()
    axes[1].grid(alpha=0.3)

    # Aleatoria
    axes[2].hist(
        aleatoric[correct_mask],
        bins=30,
        alpha=0.6,
        label="Correcto",
        color="green",
        density=True,
    )
    axes[2].hist(
        aleatoric[~correct_mask],
        bins=30,
        alpha=0.6,
        label="Incorrecto",
        color="red",
        density=True,
    )
    axes[2].set_xlabel("Incertidumbre Aleatoria (ÏƒÂ²)")
    axes[2].set_ylabel("Densidad")
    axes[2].set_title("Aleatoria (Datos)")
    axes[2].legend()
    axes[2].grid(alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"ðŸ’¾ Histogramas guardados en: {save_path}")

    if show:
        plt.show()
    else:
        plt.close()

    return fig


def plot_reliability_diagram(metrics, n_bins=10, save_path=None, show=True):
    """
    Diagrama de calibraciÃ³n (reliability plot).

    Args:
        metrics: Dict con resultados de evaluate_with_uncertainty
        n_bins: NÃºmero de bins para el diagrama
        save_path: Path para guardar la figura
        show: Si mostrar la figura
    """
    preds = metrics["predictions"]
    targets = metrics["targets"]
    confidence = metrics["confidence"]

    # Crear bins
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    accuracies = []
    confidences_bin = []
    counts = []

    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (confidence > bin_lower) & (confidence <= bin_upper)
        prop_in_bin = in_bin.sum() / len(confidence)

        if prop_in_bin > 0:
            accuracy_in_bin = (preds[in_bin] == targets[in_bin]).mean()
            avg_confidence_in_bin = confidence[in_bin].mean()

            accuracies.append(accuracy_in_bin)
            confidences_bin.append(avg_confidence_in_bin)
            counts.append(in_bin.sum())
        else:
            accuracies.append(0)
            confidences_bin.append(0)
            counts.append(0)

    fig, ax = plt.subplots(figsize=(8, 8))

    # Diagonal perfecta
    ax.plot([0, 1], [0, 1], "k--", label="CalibraciÃ³n perfecta")

    # Barras
    width = 1.0 / n_bins
    ax.bar(
        confidences_bin,
        accuracies,
        width=width,
        alpha=0.6,
        edgecolor="black",
        label="CalibraciÃ³n real",
    )

    ax.set_xlabel("Confianza", fontsize=12)
    ax.set_ylabel("Accuracy", fontsize=12)
    ax.set_title(f"Reliability Diagram (ECE: {metrics['ece']:.4f})", fontsize=14)
    ax.legend()
    ax.grid(alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"ðŸ’¾ Diagrama de calibraciÃ³n guardado en: {save_path}")

    if show:
        plt.show()
    else:
        plt.close()

    return fig


def plot_uncertainty_scatter(metrics, save_path=None, show=True):
    """
    Scatter plot de incertidumbre epistÃ©mica vs aleatoria.

    Args:
        metrics: Dict con resultados de evaluate_with_uncertainty
        save_path: Path para guardar la figura
        show: Si mostrar la figura
    """
    preds = metrics["predictions"]
    targets = metrics["targets"]
    epistemic = metrics["epistemic"]
    aleatoric = metrics["aleatoric"]

    correct_mask = preds == targets

    fig, ax = plt.subplots(figsize=(8, 6))

    ax.scatter(
        epistemic[correct_mask],
        aleatoric[correct_mask],
        alpha=0.5,
        c="green",
        label="Correcto",
        s=20,
    )
    ax.scatter(
        epistemic[~correct_mask],
        aleatoric[~correct_mask],
        alpha=0.7,
        c="red",
        label="Incorrecto",
        s=30,
        marker="x",
    )

    ax.set_xlabel("Incertidumbre EpistÃ©mica (BALD)", fontsize=12)
    ax.set_ylabel("Incertidumbre Aleatoria (ÏƒÂ²)", fontsize=12)
    ax.set_title("Incertidumbre EpistÃ©mica vs Aleatoria", fontsize=14)
    ax.legend()
    ax.grid(alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"ðŸ’¾ Scatter plot guardado en: {save_path}")

    if show:
        plt.show()
    else:
        plt.close()

    return fig


def plot_training_history_uncertainty(history, save_path=None, show=True):
    """
    Plot del historial de entrenamiento.

    Args:
        history: Dict con listas de train_loss, val_loss, train_acc, val_acc
        save_path: Path para guardar
        show: Si mostrar
    """
    epochs = range(1, len(history["train_loss"]) + 1)

    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Loss
    axes[0].plot(epochs, history["train_loss"], "b-", label="Train Loss", linewidth=2)
    axes[0].plot(epochs, history["val_loss"], "r-", label="Val Loss", linewidth=2)
    axes[0].set_xlabel("Ã‰poca")
    axes[0].set_ylabel("Loss")
    axes[0].set_title("PÃ©rdida HeteroscedÃ¡stica")
    axes[0].legend()
    axes[0].grid(alpha=0.3)

    # Accuracy
    axes[1].plot(epochs, history["train_acc"], "b-", label="Train Acc", linewidth=2)
    axes[1].plot(epochs, history["val_acc"], "r-", label="Val Acc", linewidth=2)
    axes[1].set_xlabel("Ã‰poca")
    axes[1].set_ylabel("Accuracy")
    axes[1].set_title("Accuracy")
    axes[1].legend()
    axes[1].grid(alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"ðŸ’¾ Historial guardado en: {save_path}")

    if show:
        plt.show()
    else:
        plt.close()

    return fig
