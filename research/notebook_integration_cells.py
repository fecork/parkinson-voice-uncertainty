# ============================================================
# CELDAS PARA INTEGRAR EN EL NOTEBOOK
# ============================================================

"""
Estas son las celdas que puedes copiar y pegar en tu notebook
para reemplazar las celdas existentes y usar el nuevo sistema.
"""

# ============================================================
# CELDA 1: SELECTOR DE HIPERPAR√ÅMETROS (REEMPLAZA LA CELDA DE CONFIGURACI√ìN)
# ============================================================

"""
# ============================================================
# SELECTOR DE HIPERPAR√ÅMETROS: IBARRA vs OPTUNA
# ============================================================

# üîß CONFIGURACI√ìN PRINCIPAL - CAMBIA ESTE VALOR
USE_IBARRA_HYPERPARAMETERS = True  # True = Ibarra, False = Optuna

# ============================================================
# IMPORTAR SISTEMA DE CONFIGURACI√ìN
# ============================================================

import sys
from pathlib import Path

# Agregar m√≥dulos al path
project_root = Path.cwd().parent
sys.path.insert(0, str(project_root))

from modules.core.hyperparameter_config import HyperparameterManager, compare_hyperparameters

# ============================================================
# CARGAR HIPERPAR√ÅMETROS SEG√öN CONFIGURACI√ìN
# ============================================================

print("=" * 80)
print("üîß SELECTOR DE HIPERPAR√ÅMETROS")
print("=" * 80)

# Crear manager
manager = HyperparameterManager()

# Obtener hiperpar√°metros seg√∫n configuraci√≥n
if USE_IBARRA_HYPERPARAMETERS:
    print("üìö Usando hiperpar√°metros del PAPER DE IBARRA 2023")
    hyperparameters = manager.get_ibarra_hyperparameters()
    source = "Paper Ibarra 2023"
else:
    print("üîç Usando mejores hiperpar√°metros de OPTUNA")
    hyperparameters = manager.get_optuna_hyperparameters()
    source = "Optuna Optimizado"

print(f"‚úÖ Fuente: {source}")

# ============================================================
# MOSTRAR PAR√ÅMETROS SELECCIONADOS
# ============================================================

print(f"\nüìä PAR√ÅMETROS SELECCIONADOS:")
print("-" * 50)

# Par√°metros de arquitectura
print("üèóÔ∏è  ARQUITECTURA:")
print(f"   ‚Ä¢ kernel_size_1: {hyperparameters['kernel_size_1']}")
print(f"   ‚Ä¢ kernel_size_2: {hyperparameters['kernel_size_2']}")
print(f"   ‚Ä¢ filters_1: {hyperparameters['filters_1']}")
print(f"   ‚Ä¢ filters_2: {hyperparameters['filters_2']}")
print(f"   ‚Ä¢ dense_units: {hyperparameters['dense_units']}")
print(f"   ‚Ä¢ p_drop_conv: {hyperparameters['p_drop_conv']}")
print(f"   ‚Ä¢ p_drop_fc: {hyperparameters['p_drop_fc']}")

# Par√°metros de entrenamiento
print("\nüöÄ ENTRENAMIENTO:")
print(f"   ‚Ä¢ batch_size: {hyperparameters['batch_size']}")
print(f"   ‚Ä¢ learning_rate: {hyperparameters['learning_rate']}")
print(f"   ‚Ä¢ momentum: {hyperparameters['momentum']}")
print(f"   ‚Ä¢ weight_decay: {hyperparameters['weight_decay']}")
print(f"   ‚Ä¢ n_epochs: {hyperparameters['n_epochs']}")
print(f"   ‚Ä¢ early_stopping_patience: {hyperparameters['early_stopping_patience']}")

# Par√°metros del scheduler
print("\nüìà SCHEDULER:")
print(f"   ‚Ä¢ step_size: {hyperparameters['step_size']}")
print(f"   ‚Ä¢ gamma: {hyperparameters['gamma']}")
print(f"   ‚Ä¢ optimizer: {hyperparameters['optimizer']}")

# ============================================================
# COMPARACI√ìN (OPCIONAL)
# ============================================================

if not USE_IBARRA_HYPERPARAMETERS:
    print(f"\nüìä COMPARACI√ìN CON IBARRA:")
    print("-" * 50)
    
    ibarra_params = manager.get_ibarra_hyperparameters()
    
    # Comparar par√°metros clave
    key_params = ["kernel_size_1", "kernel_size_2", "filters_2", "dense_units", "batch_size", "learning_rate"]
    
    for param in key_params:
        ibarra_val = ibarra_params[param]
        optuna_val = hyperparameters[param]
        
        if ibarra_val != optuna_val:
            if isinstance(ibarra_val, (int, float)) and isinstance(optuna_val, (int, float)):
                diff = optuna_val - ibarra_val
                diff_str = f"{diff:+.3f}" if isinstance(diff, float) else f"{diff:+d}"
                print(f"   ‚Ä¢ {param}: Ibarra={ibarra_val} ‚Üí Optuna={optuna_val} ({diff_str})")
            else:
                print(f"   ‚Ä¢ {param}: Ibarra={ibarra_val} ‚Üí Optuna={optuna_val}")
        else:
            print(f"   ‚Ä¢ {param}: {ibarra_val} (igual)")

# ============================================================
# GUARDAR CONFIGURACI√ìN
# ============================================================

# Guardar configuraci√≥n actual
manager.save_config(use_ibarra=USE_IBARRA_HYPERPARAMETERS)

print(f"\nüíæ Configuraci√≥n guardada:")
print(f"   ‚Ä¢ Usar Ibarra: {USE_IBARRA_HYPERPARAMETERS}")
print(f"   ‚Ä¢ Archivo: config/hyperparameter_config.json")

# ============================================================
# PREPARAR VARIABLES PARA EL RESTO DEL NOTEBOOK
# ============================================================

# Crear variables globales que el resto del notebook puede usar
BEST_PARAMS = hyperparameters
USE_IBARRA = USE_IBARRA_HYPERPARAMETERS
HYPERPARAMETER_SOURCE = source

print(f"\n‚úÖ Variables preparadas:")
print(f"   ‚Ä¢ BEST_PARAMS: Diccionario con hiperpar√°metros")
print(f"   ‚Ä¢ USE_IBARRA: {USE_IBARRA}")
print(f"   ‚Ä¢ HYPERPARAMETER_SOURCE: {HYPERPARAMETER_SOURCE}")

print("=" * 80)
print("üéØ LISTO PARA ENTRENAR CON LOS PAR√ÅMETROS SELECCIONADOS")
print("=" * 80)
"""

# ============================================================
# CELDA 2: CREAR MODELO (REEMPLAZA LA CELDA DE CREACI√ìN DE MODELO)
# ============================================================

"""
# ============================================================
# CREAR MODELO CON HIPERPAR√ÅMETROS SELECCIONADOS
# ============================================================

print("=" * 70)
print("CREANDO MODELO CON HIPERPAR√ÅMETROS SELECCIONADOS")
print("=" * 70)

# Verificar que las variables est√©n definidas
if 'BEST_PARAMS' not in globals():
    print("‚ùå Error: BEST_PARAMS no est√° definido.")
    print("   Ejecuta primero la celda del selector de hiperpar√°metros.")
    print("=" * 70)
else:
    print(f"‚úÖ Usando hiperpar√°metros de: {HYPERPARAMETER_SOURCE}")
    
    # Crear modelo con los par√°metros seleccionados
    best_model = CNN2D(
        n_classes=2,
        p_drop_conv=BEST_PARAMS["p_drop_conv"],
        p_drop_fc=BEST_PARAMS["p_drop_fc"],
        input_shape=(65, 41),
        filters_1=BEST_PARAMS["filters_1"],
        filters_2=BEST_PARAMS["filters_2"],
        kernel_size_1=BEST_PARAMS["kernel_size_1"],
        kernel_size_2=BEST_PARAMS["kernel_size_2"],
        dense_units=BEST_PARAMS["dense_units"],
    ).to(device)
    
    print(f"‚úÖ Modelo creado con hiperpar√°metros de {HYPERPARAMETER_SOURCE}:")
    print(f"   - Filters 1: {BEST_PARAMS['filters_1']}")
    print(f"   - Filters 2: {BEST_PARAMS['filters_2']}")
    print(f"   - Kernel 1: {BEST_PARAMS['kernel_size_1']}")
    print(f"   - Kernel 2: {BEST_PARAMS['kernel_size_2']}")
    print(f"   - Dense units: {BEST_PARAMS['dense_units']}")
    print(f"   - Dropout conv: {BEST_PARAMS['p_drop_conv']}")
    print(f"   - Dropout fc: {BEST_PARAMS['p_drop_fc']}")
    
    # Mostrar arquitectura
    print_model_summary(best_model)
    
    print("=" * 70)
"""

# ============================================================
# CELDA 3: CONFIGURAR ENTRENAMIENTO (REEMPLAZA LA CELDA DE CONFIGURACI√ìN DE ENTRENAMIENTO)
# ============================================================

"""
# ============================================================
# CONFIGURAR ENTRENAMIENTO CON HIPERPAR√ÅMETROS SELECCIONADOS
# ============================================================

print("=" * 70)
print("CONFIGURANDO ENTRENAMIENTO CON HIPERPAR√ÅMETROS SELECCIONADOS")
print("=" * 70)

# Verificar que las variables est√©n definidas
if 'BEST_PARAMS' not in globals():
    print("‚ùå Error: BEST_PARAMS no est√° definido.")
    print("   Ejecuta primero la celda del selector de hiperpar√°metros.")
    print("=" * 70)
else:
    print(f"‚úÖ Configurando entrenamiento con par√°metros de: {HYPERPARAMETER_SOURCE}")
    
    # Configuraci√≥n de entrenamiento usando los par√°metros seleccionados
    FINAL_TRAINING_CONFIG = {
        "n_epochs": BEST_PARAMS["n_epochs"],
        "early_stopping_patience": BEST_PARAMS["early_stopping_patience"],
        "learning_rate": BEST_PARAMS["learning_rate"],
        "batch_size": BEST_PARAMS["batch_size"]
    }
    
    # Crear DataLoaders con el batch size seleccionado
    train_loader_final = DataLoader(
        train_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=True,
        num_workers=0
    )
    val_loader_final = DataLoader(
        val_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=False,
        num_workers=0
    )
    test_loader_final = DataLoader(
        test_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=False,
        num_workers=0
    )
    
    # Optimizador SGD con momentum usando los par√°metros seleccionados
    optimizer_final = optim.SGD(
        best_model.parameters(),
        lr=FINAL_TRAINING_CONFIG['learning_rate'],
        momentum=BEST_PARAMS["momentum"],
        weight_decay=BEST_PARAMS["weight_decay"],
        nesterov=True  # Mejora sobre Ibarra
    )
    
    # Calcular class weights para balancear las clases
    if CLASS_WEIGHTS_CONFIG["enabled"]:
        class_counts = torch.bincount(y_train)
        class_weights = 1.0 / class_counts.float()
        class_weights = class_weights / class_weights.sum()
        criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))
        print(f"‚úÖ Class weights habilitados: {class_weights.tolist()}")
    else:
        criterion_final = nn.CrossEntropyLoss()
        print("‚ö†Ô∏è  Class weights deshabilitados")
    
    # Scheduler StepLR usando los par√°metros seleccionados
    scheduler_final = torch.optim.lr_scheduler.StepLR(
        optimizer_final,
        step_size=BEST_PARAMS["step_size"],
        gamma=BEST_PARAMS["gamma"]
    )
    
    print(f"\n‚öôÔ∏è  Configuraci√≥n final ({HYPERPARAMETER_SOURCE}):")
    print(f"   - Learning rate inicial: {FINAL_TRAINING_CONFIG['learning_rate']}")
    print(f"   - Momentum: {BEST_PARAMS['momentum']} (Nesterov: True)")
    print(f"   - Weight decay: {BEST_PARAMS['weight_decay']}")
    print(f"   - Scheduler: StepLR (step={BEST_PARAMS['step_size']}, gamma={BEST_PARAMS['gamma']})")
    print(f"   - Batch size: {FINAL_TRAINING_CONFIG['batch_size']}")
    print(f"   - √âpocas m√°ximas: {FINAL_TRAINING_CONFIG['n_epochs']}")
    print(f"   - Early stopping patience: {FINAL_TRAINING_CONFIG['early_stopping_patience']}")
    
    # Mostrar diferencias con Ibarra si estamos usando Optuna
    if not USE_IBARRA:
        print(f"\nüìä DIFERENCIAS CON IBARRA:")
        print(f"   - Ibarra batch_size: 64 ‚Üí Optuna: {BEST_PARAMS['batch_size']}")
        print(f"   - Ibarra kernel_1: 6 ‚Üí Optuna: {BEST_PARAMS['kernel_size_1']}")
        print(f"   - Ibarra filters_2: 64 ‚Üí Optuna: {BEST_PARAMS['filters_2']}")
        print(f"   - Ibarra dense_units: 32 ‚Üí Optuna: {BEST_PARAMS['dense_units']}")
    
    print("=" * 70)
"""

# ============================================================
# CELDA 4: COMPARACI√ìN R√ÅPIDA (NUEVA CELDA OPCIONAL)
# ============================================================

"""
# ============================================================
# COMPARACI√ìN R√ÅPIDA: IBARRA vs OPTUNA
# ============================================================

print("=" * 80)
print("üìä COMPARACI√ìN R√ÅPIDA: IBARRA vs OPTUNA")
print("=" * 80)

try:
    compare_hyperparameters()
except Exception as e:
    print(f"Error en comparaci√≥n: {e}")
    print("Aseg√∫rate de que el sistema de hiperpar√°metros est√© correctamente instalado.")

print("=" * 80)
"""
