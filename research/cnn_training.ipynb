{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH7omjwwttPo"
   },
   "source": [
    "# CNN 2D para Detección de Parkinson (Baseline con Augmentation + Optuna)\n",
    "## Baseline Model - Train/Val/Test Split + Hyperparameter Optimization\n",
    "\n",
    "Este notebook entrena un modelo **CNN2D simple** (sin Domain Adaptation) para clasificación binaria Parkinson vs Healthy **usando data augmentation** y **optimización automática de hiperparámetros con Optuna**.\n",
    "\n",
    "### Pipeline:\n",
    "1. **Setup**: Configuración del entorno\n",
    "2. **Data Loading**: Carga de datos CON augmentation\n",
    "3. **Split**: Train/Val/Test estratificado (70/15/15)\n",
    "4. **Optuna Optimization**: Optimización automática de hiperparámetros (20 configuraciones)\n",
    "5. **Final Training**: Re-entrenamiento con mejores hiperparámetros + early stopping\n",
    "6. **Evaluation**: Métricas completas en test set\n",
    "7. **Visualization**: Gráficas de progreso y resultados\n",
    "\n",
    "### Arquitectura:\n",
    "Este modelo usa el **mismo Feature Extractor** que CNN2D_DA (arquitectura Ibarra 2023) pero **sin Domain Adaptation**:\n",
    "- 2 bloques Conv2D → BN → ReLU → MaxPool(3×3) → Dropout\n",
    "- Solo cabeza de clasificación PD (sin GRL ni cabeza de dominio)\n",
    "\n",
    "### Data Augmentation:\n",
    "- Pitch shifting\n",
    "- Time stretching\n",
    "- Noise injection\n",
    "- SpecAugment (máscaras de frecuencia/tiempo)\n",
    "- Factor: ~5x más datos\n",
    "\n",
    "### Comparación:\n",
    "- **Este notebook**: Modelo CNN2D con augmentation (mejora generalización)\n",
    "- **cnn_da_training.ipynb**: Modelo CNN2D_DA sin augmentation (paper exacto)\n",
    "- El augmentation permite entrenar con más datos y mejorar robustez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYIbsc8VBC2x",
    "outputId": "de2df2b5-ac83-4a01-ee3c-795799b3b535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "$ git config --global --add safe.directory /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty fetch --all --prune\n",
      "Fetching origin\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty branch --show-current\n",
      "feature/feature/firstTraining\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty checkout feature/feature/firstTraining\n",
      "M\t.gitignore\n",
      "M\t.ipynb_checkpoints/parkinson_voice_analysis-checkpoint.ipynb\n",
      "M\tREADME.md\n",
      "M\tbash.exe.stackdump\n",
      "M\tcheckpoints/cnn2d_optuna_best_params.json\n",
      "M\tcheckpoints/cnn2d_optuna_trials.csv\n",
      "M\tdata/README.md\n",
      "M\tdata/vowels_healthy/healthy_diverse_metadata.json\n",
      "M\tdata_preparation/sample_healthy_data.py\n",
      "M\tdata_preparation/verify_sampling.py\n",
      "M\tdocs/CNN2D_TRAINING_SUMMARY.md\n",
      "M\tdocs/CONFIGURACION_VALIDATION.md\n",
      "M\tdocs/CORRECCIONES_APLICADAS.md\n",
      "M\tdocs/MIGRATION_TALOS_TO_OPTUNA.md\n",
      "M\tdocs/OPTUNA_OPTIMIZATION_SUMMARY.md\n",
      "M\tdocs/TEST_RESULTS_SUMMARY.md\n",
      "M\texamples/environment_example.py\n",
      "M\tmodules/README.md\n",
      "M\tmodules/__init__.py\n",
      "M\tmodules/core/README.md\n",
      "M\tmodules/core/__init__.py\n",
      "M\tmodules/core/cnn2d_optuna_wrapper.py\n",
      "M\tmodules/core/data_augmentation.ipynb\n",
      "M\tmodules/core/dataset.py\n",
      "M\tmodules/core/dependency_manager.py\n",
      "M\tmodules/core/deprecated/README.md\n",
      "M\tmodules/core/deprecated/cnn2d_talos_wrapper.py\n",
      "M\tmodules/core/deprecated/talos_analysis.py\n",
      "M\tmodules/core/deprecated/talos_evaluator.py\n",
      "M\tmodules/core/deprecated/talos_optimization.py\n",
      "M\tmodules/core/deprecated/talos_optimization_models.py\n",
      "M\tmodules/core/deprecated/talos_visualization.py\n",
      "M\tmodules/core/environment.py\n",
      "M\tmodules/core/experiment_config.py\n",
      "M\tmodules/core/model_evaluation.py\n",
      "M\tmodules/core/notebook_setup.py\n",
      "M\tmodules/core/optuna_checkpoint.py\n",
      "M\tmodules/core/optuna_optimization.py\n",
      "M\tmodules/core/preprocessing.py\n",
      "M\tmodules/core/sequence_dataset.py\n",
      "M\tmodules/core/training_monitor.py\n",
      "M\tmodules/core/utils.py\n",
      "M\tmodules/core/visualization.py\n",
      "M\tmodules/core/wandb_training.py\n",
      "M\tmodules/data/__init__.py\n",
      "M\tmodules/data/augmentation.py\n",
      "M\tmodules/data/cache_utils.py\n",
      "M\tmodules/models/__init__.py\n",
      "M\tmodules/models/cnn1d/__init__.py\n",
      "M\tmodules/models/cnn1d/model.py\n",
      "M\tmodules/models/cnn1d/training.py\n",
      "M\tmodules/models/cnn1d/visualization.py\n",
      "M\tmodules/models/cnn2d/__init__.py\n",
      "M\tmodules/models/cnn2d/inference.py\n",
      "M\tmodules/models/cnn2d/model.py\n",
      "M\tmodules/models/cnn2d/training.py\n",
      "M\tmodules/models/cnn2d/utils.py\n",
      "M\tmodules/models/cnn2d/visualization.py\n",
      "M\tmodules/models/common/__init__.py\n",
      "M\tmodules/models/common/layers.py\n",
      "M\tmodules/models/common/training_utils.py\n",
      "M\tmodules/models/common/visualization_utils.py\n",
      "M\tmodules/models/lstm_da/__init__.py\n",
      "M\tmodules/models/lstm_da/model.py\n",
      "M\tmodules/models/lstm_da/training.py\n",
      "M\tmodules/models/lstm_da/visualization.py\n",
      "M\tmodules/models/uncertainty/__init__.py\n",
      "M\tmodules/models/uncertainty/loss.py\n",
      "M\tmodules/models/uncertainty/model.py\n",
      "M\tmodules/models/uncertainty/training.py\n",
      "M\tmodules/models/uncertainty/visualization.py\n",
      "M\tnotebooks/cnn_uncertainty_training.ipynb\n",
      "M\tnotebooks/colab_setup_example.py\n",
      "M\tnotebooks/data_augmentation.ipynb\n",
      "M\tnotebooks/gradcam_inference.ipynb\n",
      "M\tnotebooks/svdd_data_preparation.ipynb\n",
      "M\tpipelines/README.md\n",
      "M\tpipelines/generate_lstm_sequences.py\n",
      "M\tpipelines/train_cnn.py\n",
      "M\tpipelines/train_cnn_da_kfold.py\n",
      "M\tpipelines/train_cnn_uncertainty.py\n",
      "M\tpipelines/train_lstm_da_kfold.py\n",
      "M\trequirements.txt\n",
      "M\tresearch/README.md\n",
      "M\tresearch/checkpoints/cnn2d_optuna_best_params.json\n",
      "M\tresearch/checkpoints/cnn2d_optuna_progress.json\n",
      "M\tresearch/checkpoints/cnn2d_optuna_trials.json\n",
      "M\tresearch/cnn1d_da_training.ipynb\n",
      "M\tresearch/cnn_da_training.ipynb\n",
      "M\tresearch/cnn_training.ipynb\n",
      "M\tresearch/colab_reload_fix.py\n",
      "M\tresearch/create_clean_checkpoint.py\n",
      "M\tresearch/create_corrected_checkpoint.py\n",
      "M\tresearch/create_final_checkpoint.py\n",
      "M\tresearch/create_initial_checkpoint.py\n",
      "M\tresearch/create_simple_checkpoint.py\n",
      "M\tresearch/extract_trials_from_log.py\n",
      "M\tresearch/fix_checkpoint_compatibility.py\n",
      "M\tresearch/load_optuna_results.py\n",
      "M\tresearch/lstm_da_training.ipynb\n",
      "M\tresearch/setup_optuna_results.py\n",
      "M\tresearch/use_checkpoint.py\n",
      "M\tresearch/verify_fix.py\n",
      "M\tresearch/wandb_notebook_blocks.py\n",
      "M\tresults/README.md\n",
      "M\tresults/cnn1d_da/test_metrics_1d_da.json\n",
      "M\tresults/cnn_da/test_metrics_da.json\n",
      "M\tresults/cnn_no_da/test_metrics.json\n",
      "M\tresults/cnn_optuna_optimization/optuna_trials_results.csv\n",
      "M\tresults/cnn_talos_optimization/best_params.json\n",
      "M\tresults/cnn_talos_optimization/talos_scan_results.csv\n",
      "M\tresults/cnn_uncertainty/gradcam_config.json\n",
      "M\tresults/cnn_uncertainty/test_metrics_uncertainty.Already on 'feature/feature/firstTraining'\n",
      "json\n",
      "M\ttest/Feature_Extractor_2D_CNN_Visualization.ipynb\n",
      "M\ttest/PAPER_VALIDATION_REPORT.md\n",
      "M\ttest/README.md\n",
      "M\ttest/README_TESTS.md\n",
      "M\ttest/deprecated/README.md\n",
      "M\ttest/deprecated/evaluate_talos.py\n",
      "M\ttest/deprecated/test_core_talos.py\n",
      "M\ttest/deprecated/test_talos_basic.py\n",
      "M\ttest/deprecated/test_talos_file_generation.py\n",
      "M\ttest/deprecated/test_talos_integration.py\n",
      "M\ttest/deprecated/test_talos_optimization.py\n",
      "M\ttest/deprecated/test_talos_real_files.py\n",
      "M\ttest/paper_requirements.json\n",
      "M\ttest/test_checkpoint_ibarra_compatibility.py\n",
      "M\ttest/test_checkpoint_loading.py\n",
      "M\ttest/test_cnn1d_attention.py\n",
      "M\ttest/test_cnn1d_implementation.py\n",
      "M\ttest/test_cnn2d_optuna.py\n",
      "M\ttest/test_cnn_architectures.py\n",
      "M\ttest/test_environment.py\n",
      "M\ttest/test_gradcam_math.py\n",
      "M\ttest/test_grl_completo.py\n",
      "M\ttest/test_ibarra_implementation.py\n",
      "M\ttest/test_ibarra_preprocessing.py\n",
      "M\ttest/test_ibarra_search_space_validation.py\n",
      "M\ttest/test_learning_rate_scheduler.py\n",
      "M\ttest/test_lstm_da_implementation.py\n",
      "M\ttest/test_lstm_sequences.py\n",
      "M\ttest/test_metric_consistency.py\n",
      "M\ttest/test_optuna_basic.py\n",
      "M\ttest/test_optuna_pruning_config.py\n",
      "M\ttest/test_paper_compliance.py\n",
      "M\ttest/test_training_monitor.py\n",
      "M\ttest/test_trial_saving_fix.py\n",
      "M\ttest/test_trial_value_fix.py\n",
      "M\ttest/test_uncertainty_math.py\n",
      "M\ttest/test_wandb_connection.py\n",
      "M\ttest/test_wandb_integration.py\n",
      "M\ttest/test_yarin_gal_implementation.py\n",
      "M\ttest/validate_checkpoint_values.py\n",
      "M\ttest/validate_paper_replication.py\n",
      "M\ttest/verify_labels.py\n",
      "M\ttest_checkpoints/cnn2d_optuna_trials.json\n",
      "M\ttest_checkpoints/test_value_handling_trials.json\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty pull origin feature/feature/firstTraining\n",
      "From https://github.com/fecork/parkinson-voice-uncertainty\n",
      " * branch            feature/feature/firstTraining -> FETCH_HEAD\n",
      "Already up to date.\n",
      "\n",
      "Instalando dependencias...\n",
      "$ python -m pip install -q optuna>=3.0.0\n",
      "\n",
      "$ python -m pip install -q torch>=1.9.0\n",
      "\n",
      "$ python -m pip install -q torchvision>=0.10.0\n",
      "\n",
      "$ python -m pip install -q scikit-learn>=1.0.0\n",
      "\n",
      "$ python -m pip install -q librosa>=0.8.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN PARA GOOGLE COLAB\n",
    "# ============================================================\n",
    "# DESCOMENTA TODO EL BLOQUE SI EJECUTAS EN COLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Configuración - AJUSTA ESTOS VALORES SI ES NECESARIO\n",
    "COMPUTER_NAME = \"ZenBook\"\n",
    "PROJECT_DIR = \"parkinson-voice-uncertainty\"\n",
    "BRANCH = \"feature/feature/firstTraining\"\n",
    "\n",
    "BASE = \"/content/drive/Othercomputers\"\n",
    "PROJ = os.path.join(BASE, COMPUTER_NAME, PROJECT_DIR)\n",
    "\n",
    "# Función auxiliar\n",
    "def sh(*args, check=False):\n",
    "    print(\"$\", \" \".join(args))\n",
    "    res = subprocess.run(args, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    print(res.stdout)\n",
    "    if check and res.returncode != 0:\n",
    "        raise RuntimeError(\"Command failed\")\n",
    "    return res.returncode\n",
    "\n",
    "# Verificaciones\n",
    "assert os.path.isdir(os.path.join(BASE, COMPUTER_NAME)), f\"No encuentro {COMPUTER_NAME} en {BASE}\"\n",
    "assert os.path.isdir(PROJ), f\"No encuentro el repo en: {PROJ}\"\n",
    "\n",
    "# Agregar al path\n",
    "if PROJ not in sys.path:\n",
    "    sys.path.insert(0, PROJ)\n",
    "\n",
    "# Configurar Git\n",
    "sh(\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", PROJ)\n",
    "sh(\"git\", \"-C\", PROJ, \"fetch\", \"--all\", \"--prune\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n",
    "\n",
    "# Cambiar a rama\n",
    "rc = sh(\"git\", \"-C\", PROJ, \"checkout\", BRANCH)\n",
    "if rc != 0:\n",
    "    sh(\"git\", \"-C\", PROJ, \"checkout\", \"-b\", BRANCH, f\"origin/{BRANCH}\")\n",
    "\n",
    "# Actualizar\n",
    "sh(\"git\", \"-C\", PROJ, \"pull\", \"origin\", BRANCH)\n",
    "\n",
    "# Instalar dependencias con manejo de errores mejorado\n",
    "req = os.path.join(PROJ, \"requirements.txt\")\n",
    "if os.path.exists(req):\n",
    "    os.chdir(\"/content\")\n",
    "    print(\"Instalando dependencias...\")\n",
    "    # Instalar dependencias críticas primero\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"optuna>=3.0.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"torch>=1.9.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"torchvision>=0.10.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"scikit-learn>=1.0.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"librosa>=0.8.1\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"soundfile>=0.10.3\")\n",
    "    # Instalar el resto\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req)\n",
    "    print(\"Dependencias instaladas correctamente\")\n",
    "else:\n",
    "    print(\"⚠️  No se encontró requirements.txt, instalando dependencias básicas...\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"optuna>=3.0.0\", \"torch>=1.9.0\", \"scikit-learn>=1.0.0\")\n",
    "\n",
    "os.chdir(PROJ)\n",
    "\n",
    "# Verificar que optuna esté disponible\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"✅ Optuna {optuna.__version__} disponible\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando optuna: {e}\")\n",
    "    print(\"Reinstalando optuna...\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"optuna>=3.0.0\")\n",
    "\n",
    "# Autoreload\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
    "    print(\"Autoreload activo\")\n",
    "except Exception as e:\n",
    "    print(f\"No se activó autoreload: {e}\")\n",
    "\n",
    "print(f\"Repo listo en: {PROJ}\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jjfqJs0ttPt"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTORNO Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio raíz del proyecto al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar el gestor de dependencias centralizado\n",
    "from modules.core.dependency_manager import setup_notebook_environment\n",
    "\n",
    "# Configurar el entorno automáticamente\n",
    "# Esto verifica e instala todas las dependencias necesarias\n",
    "success = setup_notebook_environment(auto_install=True, verbose=True)\n",
    "\n",
    "if not success:\n",
    "    print(\"Error configurando el entorno\")\n",
    "    print(\"Intenta instalar manualmente: pip install -r requirements.txt\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPHApIZxttPy"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN COMPLETA DEL EXPERIMENTO (PAPER IBARRA 2023)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACIÓN DEL EXPERIMENTO - PAPER IBARRA 2023\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL OPTIMIZADOR (SGD como en el paper)\n",
    "# ============================================================\n",
    "OPTIMIZER_CONFIG = {\n",
    "    \"type\": \"SGD\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,  # Cambiado de 0.0 a 1e-4 para regularización\n",
    "    \"nesterov\": True  # Agregado Nesterov momentum para mejor convergencia\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL SCHEDULER (StepLR como en el paper)\n",
    "# ============================================================\n",
    "SCHEDULER_CONFIG = {\n",
    "    \"type\": \"StepLR\",\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.1\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL K-FOLD CROSS-VALIDATION\n",
    "# ============================================================\n",
    "KFOLD_CONFIG = {\n",
    "    \"n_splits\": 10,\n",
    "    \"shuffle\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify_by_speaker\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE CLASS WEIGHTS (para balancear clases)\n",
    "# ============================================================\n",
    "CLASS_WEIGHTS_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"method\": \"inverse_frequency\"  # 1/frequency\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE FILTRADO DE VOCAL /a/\n",
    "# ============================================================\n",
    "VOCAL_FILTER_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"target_vocal\": \"a\",  # Solo vocal /a/ como en el paper\n",
    "    \"filter_healthy\": True,\n",
    "    \"filter_parkinson\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 para evitar overfitting\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"save_best_model\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE OPTUNA (OPTIMIZACIÓN DE HIPERPARÁMETROS)\n",
    "# ============================================================\n",
    "# Optuna reemplaza a Optuna - más moderno, sin problemas de instalación\n",
    "OPTUNA_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"experiment_name\": \"cnn2d_optuna_optimization\",\n",
    "    \"n_trials\": 30,  # Número de configuraciones a probar\n",
    "    \"n_epochs_per_trial\": 10,  # Épocas por configuración (reducido de 20 a 10)\n",
    "    \"metric\": \"f1\",  # Métrica a optimizar\n",
    "    \"direction\": \"maximize\",  # maximize o minimize\n",
    "    \"pruning_enabled\": True,  # Habilitar pruning agresivo\n",
    "    \"pruning_patience\": 3,  # Cortar trial si no mejora en 3 épocas\n",
    "    \"pruning_min_trials\": 2  # Mínimo 2 épocas antes de aplicar pruning\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE DATOS\n",
    "# ============================================================\n",
    "DATA_CONFIG = {\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE WEIGHTS & BIASES\n",
    "# ============================================================\n",
    "WANDB_CONFIG = {\n",
    "    \"project_name\": \"parkinson-voice-uncertainty\",\n",
    "    \"enabled\": True,\n",
    "    \"api_key\": \"b452ba0c4bbe61d8c58e966aa86a9037ae19594e\",\n",
    "    \"entity\": None,  # Usar cuenta personal por defecto\n",
    "    \"tags\": [\"cnn2d\", \"parkinson\", \"voice\", \"uncertainty\"],\n",
    "    \"notes\": \"CNN2D para detección de Parkinson con incertidumbre\",\n",
    "}\n",
    "\n",
    "# Configuración de monitoreo de entrenamiento\n",
    "TRAINING_MONITOR_CONFIG = {\n",
    "    \"use_wandb\": True,\n",
    "    \"plot_every\": 5,  # Cada 5 épocas\n",
    "    \"save_plots\": True,\n",
    "    \"plot_metrics\": [\"loss\", \"f1\", \"accuracy\", \"precision\", \"recall\"],\n",
    "    \"log_gradients\": False,  # Deshabilitado para ahorrar espacio\n",
    "    \"log_parameters\": True,\n",
    "    \"log_frequency\": 1,  # Cada época\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haj4mxa1BC20"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DETECTAR ENTORNO Y CONFIGURAR RUTAS\n",
    "# ============================================================\n",
    "\n",
    "# Este import funciona desde cualquier subdirectorio del proyecto\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar y agregar la raíz del proyecto al path\n",
    "current_dir = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (current_dir / \"modules\").exists():\n",
    "        if str(current_dir) not in sys.path:\n",
    "            sys.path.insert(0, str(current_dir))\n",
    "        break\n",
    "    current_dir = current_dir.parent\n",
    "\n",
    "# Importar la función de configuración de notebooks\n",
    "from modules.core.notebook_setup import setup_notebook\n",
    "\n",
    "# Configurar automáticamente: path + entorno (Local/Colab) + rutas\n",
    "ENV, PATHS = setup_notebook(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4OEii7wttPz"
   },
   "source": [
    "## 1. Setup y Configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRECCIÓN DE IMPORTS DE WANDB\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CORRIGIENDO IMPORTS DE WANDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# El path ya está configurado correctamente arriba con:\n",
    "# project_root = Path.cwd().parent\n",
    "# sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    # Importar usando la ruta correcta (modules/ está en el directorio padre)\n",
    "    from modules.core.training_monitor import create_training_monitor, test_wandb_connection\n",
    "    from modules.core.wandb_training import create_training_config, train_with_wandb_monitoring, setup_wandb_training\n",
    "    \n",
    "    print(\"✅ Imports de wandb corregidos exitosamente\")\n",
    "    print(\"   - create_training_monitor: ✓\")\n",
    "    print(\"   - test_wandb_connection: ✓\")\n",
    "    print(\"   - create_training_config: ✓\")\n",
    "    print(\"   - train_with_wandb_monitoring: ✓\")\n",
    "    print(\"   - setup_wandb_training: ✓\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error en imports de wandb: {e}\")\n",
    "    print(f\"   Path actual: {Path.cwd()}\")\n",
    "    print(f\"   Project root: {project_root}\")\n",
    "    print(f\"   Módulos en sys.path: {[p for p in sys.path if 'modules' in p]}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0QJVCFHg6z8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRUEBA DE IMPORTACIONES DE WANDB\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROBANDO IMPORTACIONES DE WANDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Importar directamente desde los archivos\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Agregar el directorio modules al path si no está\n",
    "    modules_path = str(Path.cwd() / \"modules\")\n",
    "    if modules_path not in sys.path:\n",
    "        sys.path.append(modules_path)\n",
    "\n",
    "    from core.training_monitor import create_training_monitor, test_wandb_connection\n",
    "    from core.wandb_training import create_training_config, train_with_wandb_monitoring, setup_wandb_training\n",
    "\n",
    "    print(\"✅ Todas las importaciones de wandb exitosas\")\n",
    "    print(\"   - create_training_monitor: ✓\")\n",
    "    print(\"   - test_wandb_connection: ✓\")\n",
    "    print(\"   - create_training_config: ✓\")\n",
    "    print(\"   - train_with_wandb_monitoring: ✓\")\n",
    "    print(\"   - setup_wandb_training: ✓\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando módulos de wandb: {e}\")\n",
    "    print(\"   Verificando estructura de archivos...\")\n",
    "\n",
    "    # Verificar que los archivos existen\n",
    "    training_monitor_path = Path.cwd() / \"modules\" / \"core\" / \"training_monitor.py\"\n",
    "    wandb_training_path = Path.cwd() / \"modules\" / \"core\" / \"wandb_training.py\"\n",
    "\n",
    "    print(f\"   - training_monitor.py existe: {training_monitor_path.exists()}\")\n",
    "    print(f\"   - wandb_training.py existe: {wandb_training_path.exists()}\")\n",
    "    print(f\"   - Path actual: {Path.cwd()}\")\n",
    "    print(f\"   - sys.path incluye modules: {modules_path in sys.path}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgRftyW3ttP0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y CONFIGURACIÓN\n",
    "# ============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Agregar módulos propios al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar módulos propios\n",
    "from modules.models.cnn2d.model import CNN2D\n",
    "from modules.models.common.training_utils import print_model_summary\n",
    "from modules.models.cnn2d.training import train_model, detailed_evaluation, print_evaluation_report\n",
    "from modules.models.cnn2d.visualization import plot_training_history, analyze_spectrogram_stats\n",
    "from modules.models.cnn2d.utils import plot_confusion_matrix\n",
    "from modules.core.utils import create_10fold_splits_by_speaker\n",
    "from modules.core.dataset import (\n",
    "    load_spectrograms_cache,\n",
    "    to_pytorch_tensors,\n",
    "    DictDataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Imports para Optuna (optimización de hiperparámetros - reemplaza Optuna)\n",
    "from modules.core.cnn2d_optuna_wrapper import optimize_cnn2d, create_cnn2d_optimizer\n",
    "from modules.core.optuna_optimization import OptunaOptimizer\n",
    "\n",
    "# Imports para Weights & Biases (monitoreo en tiempo real)\n",
    "# Importar directamente desde los archivos\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio modules al path si no está\n",
    "if str(Path.cwd() / \"modules\") not in sys.path:\n",
    "    sys.path.append(str(Path.cwd() / \"modules\"))\n",
    "\n",
    "from core.training_monitor import create_training_monitor, test_wandb_connection\n",
    "from core.wandb_training import create_training_config, train_with_wandb_monitoring, setup_wandb_training\n",
    "\n",
    "# Configuración de matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configuración de PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Reporte de configuración\n",
    "print(\"=\"*70)\n",
    "print(\"CNN 2D TRAINING - BASELINE CON AUGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Librerías cargadas correctamente\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Data augmentation: ACTIVADO (~5x datos)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xinu3UYHttP1"
   },
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Carga de datos preprocesados CON augmentation para mejorar generalización del modelo baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBHKgFgnttP1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS HEALTHY DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Healthy desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from modules.core.dataset import load_spectrograms_cache\n",
    "\n",
    "# Cargar datos healthy desde cache original usando rutas dinámicas\n",
    "cache_healthy_path = PATHS['cache_original'] / \"healthy_ibarra.pkl\"\n",
    "healthy_dataset = load_spectrograms_cache(str(cache_healthy_path))\n",
    "\n",
    "if healthy_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontró el cache de datos healthy en {cache_healthy_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_healthy, y_task_healthy, y_domain_healthy, meta_healthy = to_pytorch_tensors(healthy_dataset)\n",
    "\n",
    "print(f\"Healthy cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_healthy.shape[0]}\")\n",
    "print(f\"   - Shape: {X_healthy.shape}\")\n",
    "print(f\"   - Ruta: {cache_healthy_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSBflzwNBC22"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS PARKINSON DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Parkinson desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar datos parkinson desde cache original usando rutas dinámicas\n",
    "cache_parkinson_path = PATHS['cache_original'] / \"parkinson_ibarra.pkl\"\n",
    "parkinson_dataset = load_spectrograms_cache(str(cache_parkinson_path))\n",
    "\n",
    "if parkinson_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontró el cache de datos parkinson en {cache_parkinson_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_parkinson, y_task_parkinson, y_domain_parkinson, meta_parkinson = to_pytorch_tensors(parkinson_dataset)\n",
    "\n",
    "print(f\"Parkinson cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_parkinson.shape[0]}\")\n",
    "print(f\"   - Shape: {X_parkinson.shape}\")\n",
    "print(f\"   - Ruta: {cache_parkinson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtR4xu4ttP2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFORMACIÓN DE DATOS CARGADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMACIÓN DE DATOS CARGADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Datos Healthy (desde cache original):\")\n",
    "print(f\"   - Muestras: {len(healthy_dataset)}\")\n",
    "print(f\"   - Shape de espectrogramas: {X_healthy.shape}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI77QhXExoq0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECKPOINT INICIAL YA CREADO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKPOINT INICIAL YA CREADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"✅ El checkpoint inicial ya fue creado ejecutando:\")\n",
    "print(\"   python research/create_initial_checkpoint.py\")\n",
    "print(\"\\n📊 Resumen del checkpoint:\")\n",
    "print(\"   - Trials guardados: 24\")\n",
    "print(\"   - Trials completados: 13\")\n",
    "print(\"   - Mejor F1: 0.7313\")\n",
    "print(\"   - Mejor trial: 15\")\n",
    "print(\"   - Directorio: checkpoints/\")\n",
    "\n",
    "print(\"\\n🚀 Ahora puedes continuar con la optimización de Optuna\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl80VAx4ttP3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÁLISIS ESTADÍSTICO BÁSICO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS ESTADÍSTICO BÁSICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Análisis estadístico básico\n",
    "healthy_stats = analyze_spectrogram_stats(healthy_dataset, \"HEALTHY\")\n",
    "parkinson_stats = analyze_spectrogram_stats(parkinson_dataset, \"PARKINSON\")\n",
    "\n",
    "# Comparar diferencias\n",
    "print(f\"\\nDIFERENCIAS ENTRE CLASES:\")\n",
    "print(f\"   - Diferencia en media: {abs(healthy_stats['mean'] - parkinson_stats['mean']):.3f}\")\n",
    "print(f\"   - Diferencia en std: {abs(healthy_stats['std'] - parkinson_stats['std']):.3f}\")\n",
    "\n",
    "print(\"\\nConfiguración del experimento:\")\n",
    "print(\"   - Healthy: datos originales (baseline)\")\n",
    "print(\"   - Parkinson: datos con augmentation (mejor generalización)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ad1dCwieTUo"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBINAR DATASETS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMBINANDO DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combinar espectrogramas\n",
    "X_combined = torch.cat([X_healthy, X_parkinson], dim=0)\n",
    "\n",
    "# Crear labels: 0=Healthy, 1=Parkinson\n",
    "y_combined = torch.cat([\n",
    "    torch.zeros(len(X_healthy), dtype=torch.long),  # Healthy = 0\n",
    "    torch.ones(len(X_parkinson), dtype=torch.long)  # Parkinson = 1\n",
    "], dim=0)\n",
    "\n",
    "print(f\"\\nDATASET COMBINADO:\")\n",
    "print(f\"   - Total muestras: {len(X_combined)}\")\n",
    "print(f\"   - Shape: {X_combined.shape}\")\n",
    "print(f\"   - Healthy (0): {(y_combined == 0).sum().item()} ({(y_combined == 0).sum()/len(y_combined)*100:.1f}%)\")\n",
    "print(f\"   - Parkinson (1): {(y_combined == 1).sum().item()} ({(y_combined == 1).sum()/len(y_combined)*100:.1f}%)\")\n",
    "\n",
    "balance_pct = (y_combined == 1).sum() / len(y_combined) * 100\n",
    "if abs(balance_pct - 50) < 10:\n",
    "    print(f\"   ✓ Dataset razonablemente balanceado\")\n",
    "else:\n",
    "    print(f\"   ⚠ Dataset desbalanceado - class weights habilitados en config\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIspxysweTUp"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECCIONAR METADATOS PARA SPEAKER IDS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO METADATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar estructura de metadatos\n",
    "if meta_healthy and len(meta_healthy) > 0:\n",
    "    print(f\"\\n✓ meta_healthy disponible: {len(meta_healthy)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_healthy[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ✗ meta_healthy no disponible o vacío\")\n",
    "\n",
    "if meta_parkinson and len(meta_parkinson) > 0:\n",
    "    print(f\"\\n✓ meta_parkinson disponible: {len(meta_parkinson)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_parkinson[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ✗ meta_parkinson no disponible o vacío\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMZElStXttP3"
   },
   "source": [
    "## 3. Split Train/Val/Test\n",
    "\n",
    "Split estratificado 70/15/15 para mantener proporciones de clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj5q0QU5ttP4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10-FOLD CROSS-VALIDATION ESTRATIFICADO POR HABLANTE\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"10-FOLD CROSS-VALIDATION (PAPER IBARRA 2023)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparar metadata combinada para create_10fold_splits_by_speaker\n",
    "# La metadata ya fue cargada antes con meta_healthy y meta_parkinson\n",
    "\n",
    "# Crear lista de metadata combinada con labels\n",
    "metadata_combined = []\n",
    "\n",
    "# Agregar metadata de healthy (label=0)\n",
    "for meta in meta_healthy:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 0,  # Healthy\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "# Agregar metadata de parkinson (label=1)\n",
    "for meta in meta_parkinson:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 1,  # Parkinson\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "print(f\"\\n📊 Dataset info:\")\n",
    "print(f\"   • Total samples: {len(X_combined)}\")\n",
    "print(f\"   • Metadata entries: {len(metadata_combined)}\")\n",
    "\n",
    "# Crear 10-fold splits usando la función centralizada\n",
    "# Esta función asegura que todos los samples de un speaker están en el mismo fold\n",
    "fold_splits = create_10fold_splits_by_speaker(\n",
    "    metadata_list=metadata_combined,\n",
    "    n_folds=KFOLD_CONFIG[\"n_splits\"],\n",
    "    seed=KFOLD_CONFIG[\"random_state\"]\n",
    ")\n",
    "\n",
    "# Para este notebook, usaremos el primer fold como ejemplo\n",
    "# En el paper real se promedian los resultados de los 10 folds\n",
    "train_indices = fold_splits[0][\"train\"]\n",
    "val_indices = fold_splits[0][\"val\"]\n",
    "\n",
    "# Crear splits de train/val usando los índices\n",
    "X_train = X_combined[train_indices]\n",
    "y_train = y_combined[train_indices]\n",
    "X_val = X_combined[val_indices]\n",
    "y_val = y_combined[val_indices]\n",
    "\n",
    "# Para test, usamos un split separado del 15%\n",
    "# TODO: Esto debería también usar split por speaker para evitar leakage\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_combined, y_combined,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nTAMAÑOS DE SPLITS:\")\n",
    "print(f\"   - Train: {len(X_train)} ({len(X_train)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Val:   {len(X_val)} ({len(X_val)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Test:  {len(X_test)} ({len(X_test)/len(X_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDISTRIBUCIÓN POR SPLIT:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    n_healthy = (y_split == 0).sum().item()\n",
    "    n_parkinson = (y_split == 1).sum().item()\n",
    "    print(f\"   {split_name:5s}: HC={n_healthy:4d} ({n_healthy/len(y_split)*100:.1f}%), PD={n_parkinson:4d} ({n_parkinson/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asWglEGTttP5"
   },
   "outputs": [],
   "source": [
    "# Agregar módulos propios al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================\n",
    "# CREAR DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📦 CREANDO DATALOADERS...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Importar DictDataset desde el módulo core\n",
    "\n",
    "# Crear datasets con formato de diccionario\n",
    "train_dataset = DictDataset(X_train, y_train)\n",
    "val_dataset = DictDataset(X_val, y_val)\n",
    "test_dataset = DictDataset(X_test, y_test)\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"✅ DataLoaders creados:\")\n",
    "print(f\"   • Train batches: {len(train_loader)}\")\n",
    "print(f\"   • Val batches:   {len(val_loader)}\")\n",
    "print(f\"   • Test batches:  {len(test_loader)}\")\n",
    "print(f\"   • Batch size:    {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yfkF_k2fujw"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÁLISIS DE RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que optuna_results esté disponible\n",
    "if 'optuna_results' not in globals():\n",
    "    print(\"❌ Error: optuna_results no está disponible.\")\n",
    "    print(\"   Ejecuta primero el bloque de optimización de Optuna.\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    # Extraer resultados\n",
    "    results_df = optuna_results[\"results_df\"]\n",
    "    best_params = optuna_results[\"best_params\"]\n",
    "    # analysis no está disponible en optuna_results, se omite\n",
    "\n",
    "    print(f\"📊 Resumen de la optimización:\")\n",
    "    print(f\"   - Total configuraciones evaluadas: {len(results_df)}\")\n",
    "    print(f\"   - Mejor F1-score encontrado: {results_df['f1'].max():.4f}\")\n",
    "    print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "            print(f\"   - {param}: {value}\")\n",
    "\n",
    "    # Mostrar top 10 configuraciones\n",
    "    print(f\"\\n📈 Top 10 configuraciones:\")\n",
    "    print(\"-\" * 80)\n",
    "    top_10 = results_df.nlargest(10, 'f1')\n",
    "    for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "        print(f\"{i:2d}. F1: {row['f1']:.4f} | \"\n",
    "              f\"Acc: {row['accuracy']:.4f} | \"\n",
    "              f\"Batch: {row['batch_size']} | \"\n",
    "              f\"LR: {row['learning_rate']} | \"\n",
    "              f\"Dropout: {row['p_drop_conv']}\")\n",
    "\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW21Fj0QttP6"
   },
   "source": [
    "## 4. Optimización de Hiperparámetros con Optuna\n",
    "\n",
    "Optimización automática de hiperparámetros usando Optuna para encontrar la mejor configuración del modelo CNN2D.\n",
    "\n",
    "### Configuración Optimizada:\n",
    "- **Método**: Optuna con búsqueda aleatoria + pruning agresivo\n",
    "- **Configuraciones**: 30 trials\n",
    "- **Épocas por config**: 10 épocas (reducido de 20 para mayor eficiencia)\n",
    "- **Pruning agresivo**: Cortar trial si no mejora en 3 épocas (después de 2 épocas mínimas)\n",
    "- **Métrica**: F1-score en validación\n",
    "- **Espacio de búsqueda**: Según especificaciones del paper de Ibarra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ukwl5yPgMvI"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR WEIGHTS & BIASES\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO WEIGHTS & BIASES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Las funciones ya están importadas en el bloque principal\n",
    "\n",
    "# Crear configuración del experimento\n",
    "training_config = create_training_config(\n",
    "    experiment_name=\"cnn2d_optuna_final_training\",\n",
    "    use_wandb=True,\n",
    "    plot_every=5,\n",
    "    save_plots=True,\n",
    "    model_architecture=\"CNN2D\",\n",
    "    dataset=\"Parkinson Voice\",\n",
    "    optimization=\"Optuna\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Configuración de wandb:\")\n",
    "print(f\"   - Proyecto: {WANDB_CONFIG['project_name']}\")\n",
    "print(f\"   - Experimento: {training_config['experiment_name']}\")\n",
    "print(f\"   - API Key: {'*' * 20}...{WANDB_CONFIG['api_key'][-4:]}\")\n",
    "print(f\"   - Tags: {WANDB_CONFIG['tags']}\")\n",
    "print(f\"   - Monitoreo cada: {training_config['plot_every']} épocas\")\n",
    "\n",
    "# Probar conexión con wandb\n",
    "print(f\"\\n🔗 Probando conexión con Weights & Biases...\")\n",
    "connection_success = test_wandb_connection(WANDB_CONFIG['api_key'])\n",
    "\n",
    "if connection_success:\n",
    "    print(\"✅ Conexión exitosa - Listo para monitorear entrenamiento\")\n",
    "else:\n",
    "    print(\"⚠️  Error en conexión - Continuando sin wandb\")\n",
    "    training_config['use_wandb'] = False\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEgNdkXKeB6z"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR WEIGHTS & BIASES\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO WEIGHTS & BIASES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración del experimento actual\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"experiment_name\": \"cnn2d_optuna_final_training\",\n",
    "    \"use_wandb\": True,\n",
    "    \"plot_every\": 5,  # Cada 5 épocas\n",
    "    \"save_plots\": True,\n",
    "    \"model_architecture\": \"CNN2D\",\n",
    "    \"dataset\": \"Parkinson Voice\",\n",
    "    \"optimization\": \"Optuna\",\n",
    "}\n",
    "\n",
    "print(f\"✅ Configuración de wandb:\")\n",
    "print(f\"   - Proyecto: {WANDB_CONFIG['project_name']}\")\n",
    "print(f\"   - Experimento: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
    "print(f\"   - API Key: {'*' * 20}...{WANDB_CONFIG['api_key'][-4:]}\")\n",
    "print(f\"   - Tags: {WANDB_CONFIG['tags']}\")\n",
    "print(f\"   - Monitoreo cada: {EXPERIMENT_CONFIG['plot_every']} épocas\")\n",
    "\n",
    "# Probar conexión con wandb\n",
    "print(f\"\\n🔗 Probando conexión con Weights & Biases...\")\n",
    "connection_success = test_wandb_connection(WANDB_CONFIG['api_key'])\n",
    "\n",
    "if connection_success:\n",
    "    print(\"✅ Conexión exitosa - Listo para monitorear entrenamiento\")\n",
    "else:\n",
    "    print(\"⚠️  Error en conexión - Continuando sin wandb\")\n",
    "    EXPERIMENT_CONFIG['use_wandb'] = False\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijmy_JiDttP6"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR OPTIMIZACIÓN CON OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO OPTIMIZACIÓN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear directorio para resultados de Optuna usando rutas dinámicas\n",
    "optuna_results_dir = PATHS['results'] / \"cnn_optuna_optimization\"\n",
    "optuna_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Módulos de Optuna importados\")\n",
    "print(f\"Directorio de resultados: {optuna_results_dir}\")\n",
    "print(f\"Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPNAfUw_ttP7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARAR DATOS PARA OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARANDO DATOS PARA OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optuna trabaja directamente con PyTorch tensors (no requiere numpy)\n",
    "# Los tensors ya están listos desde la carga de datos\n",
    "\n",
    "print(f\"📊 Datos preparados para Optuna:\")\n",
    "print(f\"   - Train: {X_train.shape} (labels: {y_train.shape})\")\n",
    "print(f\"   - Val:   {X_val.shape} (labels: {y_val.shape})\")\n",
    "print(f\"   - Test:  {X_test.shape} (labels: {y_test.shape})\")\n",
    "\n",
    "# Verificar distribución de clases\n",
    "print(f\"\\n📈 Distribución de clases:\")\n",
    "print(f\"   Train - HC: {(y_train == 0).sum().item()}, PD: {(y_train == 1).sum().item()}\")\n",
    "print(f\"   Val   - HC: {(y_val == 0).sum().item()}, PD: {(y_val == 1).sum().item()}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EulPu2cHttP8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR SI YA EXISTEN RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO RESULTADOS PREVIOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración de la optimización usando configuración centralizada\n",
    "# (OPTUNA_CONFIG ya está definido en la configuración centralizada)\n",
    "\n",
    "# Verificar si ya existen resultados previos\n",
    "results_csv_path = optuna_results_dir / \"optuna_trials_results.csv\"\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "\n",
    "if results_csv_path.exists() and best_params_path.exists():\n",
    "    print(\"✅ Se encontraron resultados previos de Optuna\")\n",
    "    print(f\"   - Archivo de resultados: {results_csv_path}\")\n",
    "    print(f\"   - Archivo de mejores parámetros: {best_params_path}\")\n",
    "\n",
    "    # Cargar resultados previos\n",
    "    results_df = pd.read_csv(results_csv_path)\n",
    "    with open(best_params_path, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "    print(f\"\\n📊 Resultados previos encontrados:\")\n",
    "    print(f\"   - Total trials evaluados: {len(results_df)}\")\n",
    "    print(f\"   - Mejor F1-score encontrado: {results_df['value'].max():.4f}\")\n",
    "    print(f\"   - F1-score promedio: {results_df['value'].mean():.4f} ± {results_df['value'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "    # Crear diccionario de resultados para compatibilidad\n",
    "    optuna_results = {\n",
    "        \"results_df\": results_df,\n",
    "        \"best_params\": best_params,\n",
    "        \"study\": None  # El study se carga separadamente si es necesario\n",
    "    }\n",
    "\n",
    "    print(f\"\\n⏭️  Saltando optimización - usando resultados previos\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No se encontraron resultados previos de Optuna\")\n",
    "    print(\"   - Iniciando optimización desde cero\")\n",
    "\n",
    "    print(f\"\\n⚙️  Configuración:\")\n",
    "    print(f\"   - Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "    print(f\"   - Épocas por trial: {OPTUNA_CONFIG['n_epochs_per_trial']}\")\n",
    "    print(f\"   - Métrica a optimizar: {OPTUNA_CONFIG['metric']} ({OPTUNA_CONFIG['direction']})\")\n",
    "\n",
    "    print(f\"\\n🚀 Iniciando búsqueda de hiperparámetros con Optuna...\")\n",
    "    print(\"   (Esto puede tomar varios minutos)\")\n",
    "\n",
    "    # Ejecutar optimización con checkpointing\n",
    "    optuna_results = optimize_cnn2d(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        input_shape=(1, 65, 41),  # (C, H, W)\n",
    "        n_trials=OPTUNA_CONFIG[\"n_trials\"],\n",
    "        n_epochs_per_trial=OPTUNA_CONFIG[\"n_epochs_per_trial\"],\n",
    "        device=device,\n",
    "        save_dir=str(optuna_results_dir),\n",
    "        checkpoint_dir=\"checkpoints\",  # ← NUEVO: Directorio para checkpoints\n",
    "        resume=True  # ← NUEVO: Reanudar desde checkpoint si existe\n",
    "    )\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"OPTIMIZACIÓN COMPLETADA\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88jfG5bVttP8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÁLISIS DE RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extraer resultados\n",
    "results_df = optuna_results[\"results_df\"]\n",
    "best_params = optuna_results[\"best_params\"]\n",
    "analysis = optuna_results[\"analysis\"]\n",
    "\n",
    "print(f\"📊 Resumen de la optimización:\")\n",
    "print(f\"   - Total configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score encontrado: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "# Mostrar top 10 configuraciones\n",
    "print(f\"\\n📈 Top 10 configuraciones:\")\n",
    "print(\"-\" * 80)\n",
    "top_10 = results_df.nlargest(10, 'f1')\n",
    "for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "    print(f\"{i:2d}. F1: {row['f1']:.4f} | \"\n",
    "          f\"Acc: {row['accuracy']:.4f} | \"\n",
    "          f\"Batch: {row['batch_size']} | \"\n",
    "          f\"LR: {row['learning_rate']} | \"\n",
    "          f\"Dropout: {row['p_drop_conv']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o888G9VGttP9"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar DataFrame completo con todas las configuraciones\n",
    "results_csv_path = optuna_results_dir / \"optuna_scan_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"💾 Resultados completos guardados: {results_csv_path}\")\n",
    "\n",
    "# Guardar mejores parámetros\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"💾 Mejores parámetros guardados: {best_params_path}\")\n",
    "\n",
    "# Guardar resumen de optimización\n",
    "summary_path = optuna_results_dir / \"optimization_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"RESUMEN DE OPTIMIZACIÓN OPTUNA\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Total configuraciones evaluadas: {len(results_df)}\\n\")\n",
    "    f.write(f\"Mejor F1-score: {results_df['f1'].max():.4f}\\n\")\n",
    "    f.write(f\"F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\\n\\n\")\n",
    "    f.write(\"MEJORES HIPERPARÁMETROS:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    for param, value in best_params.items():\n",
    "        if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(\"\\nTOP 5 CONFIGURACIONES:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    top_5 = results_df.nlargest(5, 'f1')\n",
    "    for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "        f.write(f\"{i}. F1: {row['f1']:.4f} | Acc: {row['accuracy']:.4f} | \"\n",
    "                f\"Batch: {row['batch_size']} | LR: {row['learning_rate']}\\n\")\n",
    "\n",
    "print(f\"💾 Resumen guardado: {summary_path}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wihb557SttP-"
   },
   "source": [
    "## 5. Re-entrenamiento con Mejores Hiperparámetros\n",
    "\n",
    "Re-entrenar el modelo CNN2D usando los mejores hiperparámetros encontrados por Optuna, con early stopping para obtener el modelo final optimizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ScVlmBmeB60"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MONITOR DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREANDO MONITOR DE ENTRENAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear configuración de entrenamiento con mejores parámetros\n",
    "training_config = create_training_config(\n",
    "    experiment_name=\"cnn2d_optuna_final_training\",\n",
    "    use_wandb=True,\n",
    "    plot_every=5,\n",
    "    save_plots=True,\n",
    "    model_architecture=\"CNN2D\",\n",
    "    dataset=\"Parkinson Voice\",\n",
    "    optimization=\"Optuna\",\n",
    "    best_params=best_params\n",
    ")\n",
    "\n",
    "# Configurar monitoreo con wandb\n",
    "monitor = setup_wandb_training(\n",
    "    config=training_config,\n",
    "    wandb_config=WANDB_CONFIG,\n",
    "    model=best_model,\n",
    "    input_shape=(1, 65, 41)\n",
    ")\n",
    "\n",
    "print(f\"📊 Monitor configurado:\")\n",
    "print(f\"   - Proyecto: {monitor.project_name}\")\n",
    "print(f\"   - Experimento: {monitor.experiment_name}\")\n",
    "print(f\"   - Wandb habilitado: {monitor.use_wandb}\")\n",
    "print(f\"   - Plot cada: {monitor.plot_every} épocas\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NVWEzLkttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO CON MEJORES HIPERPARÁMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREANDO MODELO CON MEJORES HIPERPARÁMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear modelo con mejores parámetros encontrados por Optuna\n",
    "best_model = CNN2D(\n",
    "    n_classes=2,\n",
    "    p_drop_conv=best_params[\"p_drop_conv\"],\n",
    "    p_drop_fc=best_params[\"p_drop_fc\"],\n",
    "    input_shape=(65, 41),\n",
    "    filters_1=best_params[\"filters_1\"],\n",
    "    filters_2=best_params[\"filters_2\"],\n",
    "    kernel_size_1=best_params[\"kernel_size_1\"],\n",
    "    kernel_size_2=best_params[\"kernel_size_2\"],\n",
    "    dense_units=best_params[\"dense_units\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"✅ Modelo creado con mejores hiperparámetros:\")\n",
    "print(f\"   - Filters 1: {best_params['filters_1']}\")\n",
    "print(f\"   - Filters 2: {best_params['filters_2']}\")\n",
    "print(f\"   - Kernel 1: {best_params['kernel_size_1']}\")\n",
    "print(f\"   - Kernel 2: {best_params['kernel_size_2']}\")\n",
    "print(f\"   - Dense units: {best_params['dense_units']}\")\n",
    "print(f\"   - Dropout conv: {best_params['p_drop_conv']}\")\n",
    "print(f\"   - Dropout fc: {best_params['p_drop_fc']}\")\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print_model_summary(best_model)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPhb_AXfeB60"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTAR FUNCIONES DE ENTRENAMIENTO CON WANDB\n",
    "# ============================================================\n",
    "\n",
    "from modules.core.wandb_training import (\n",
    "    train_with_wandb_monitoring,\n",
    "    create_training_config,\n",
    "    setup_wandb_training\n",
    ")\n",
    "\n",
    "print(\"✅ Funciones de entrenamiento con wandb importadas\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86kelbHEttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTRENAMIENTO CON MEJORES PARÁMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO ENTRENAMIENTO FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración de entrenamiento final usando configuración centralizada\n",
    "FINAL_TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 (recomendación)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": best_params[\"batch_size\"]\n",
    "}\n",
    "\n",
    "# Crear DataLoaders con el mejor batch size\n",
    "train_loader_final = DataLoader(\n",
    "    train_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader_final = DataLoader(\n",
    "    val_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader_final = DataLoader(\n",
    "    test_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Optimizador SGD con momentum usando configuración centralizada\n",
    "# CORREGIDO: Agregado nesterov=True y weight_decay=1e-4\n",
    "optimizer_final = optim.SGD(\n",
    "    best_model.parameters(),\n",
    "    lr=FINAL_TRAINING_CONFIG['learning_rate'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,  # Cambiado de 0.0 a 1e-4\n",
    "    nesterov=True  # Agregado Nesterov momentum\n",
    ")\n",
    "\n",
    "# Calcular class weights para balancear las clases usando configuración centralizada\n",
    "if CLASS_WEIGHTS_CONFIG[\"enabled\"]:\n",
    "    class_counts = torch.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    print(f\"✅ Class weights habilitados: {class_weights.tolist()}\")\n",
    "else:\n",
    "    criterion_final = nn.CrossEntropyLoss()\n",
    "    print(\"⚠️  Class weights deshabilitados\")\n",
    "\n",
    "# Scheduler StepLR usando configuración centralizada\n",
    "scheduler_final = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer_final,\n",
    "    step_size=SCHEDULER_CONFIG[\"step_size\"],\n",
    "    gamma=SCHEDULER_CONFIG[\"gamma\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n⚙️  Configuración final:\")\n",
    "print(f\"   - Learning rate inicial: {FINAL_TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"   - Momentum: 0.9 (Nesterov: True)\")\n",
    "print(f\"   - Weight decay: 1e-4\")\n",
    "print(f\"   - Scheduler: StepLR (step={SCHEDULER_CONFIG['step_size']}, gamma={SCHEDULER_CONFIG['gamma']})\")\n",
    "print(f\"   - Batch size: {FINAL_TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   - Épocas máximas: {FINAL_TRAINING_CONFIG['n_epochs']}\")\n",
    "print(f\"   - Early stopping patience: {FINAL_TRAINING_CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YML3HyseB61"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO CON MONITOREO WANDB\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO MODELO CON MONITOREO WANDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ejecutar entrenamiento con monitoreo\n",
    "training_results = train_with_wandb_monitoring(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader_final,\n",
    "    val_loader=val_loader_final,\n",
    "    optimizer=optimizer_final,\n",
    "    criterion=criterion_final,\n",
    "    scheduler=scheduler_final,\n",
    "    monitor=monitor,\n",
    "    device=device,\n",
    "    epochs=FINAL_TRAINING_CONFIG['n_epochs'],\n",
    "    early_stopping_patience=FINAL_TRAINING_CONFIG['early_stopping_patience'],\n",
    "    save_dir=optuna_results_dir,\n",
    "    model_name=\"best_model_wandb.pth\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Extraer resultados\n",
    "final_model = training_results[\"model\"]\n",
    "best_val_f1 = training_results[\"best_val_f1\"]\n",
    "final_epoch = training_results[\"final_epoch\"]\n",
    "training_history = training_results[\"history\"]\n",
    "early_stopped = training_results[\"early_stopped\"]\n",
    "\n",
    "print(f\"\\n🎉 Entrenamiento completado:\")\n",
    "print(f\"   - Mejor val_f1: {best_val_f1:.4f}\")\n",
    "print(f\"   - Épocas entrenadas: {final_epoch}\")\n",
    "print(f\"   - Early stopping: {'Sí' if early_stopped else 'No'}\")\n",
    "print(f\"   - Modelo guardado: best_model_wandb.pth\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fg5tzkUttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE DE ENTRENAMIENTO ORIGINAL ELIMINADO\n",
    "# ============================================================\n",
    "# Este bloque fue eliminado para evitar duplicación.\n",
    "# Se usa el entrenamiento con WANDB que es más completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY8wk12WeB61"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACIÓN FINAL CON WANDB\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACIÓN FINAL CON WANDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluar modelo final en test set\n",
    "from modules.models.cnn2d.training import detailed_evaluation, print_evaluation_report\n",
    "\n",
    "final_test_metrics = detailed_evaluation(\n",
    "    model=final_model,\n",
    "    loader=test_loader_final,\n",
    "    device=device,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"]\n",
    ")\n",
    "\n",
    "# Imprimir reporte\n",
    "print_evaluation_report(final_test_metrics, class_names=[\"Healthy\", \"Parkinson\"])\n",
    "\n",
    "# Loggear métricas finales a wandb\n",
    "if EXPERIMENT_CONFIG[\"use_wandb\"]:\n",
    "    monitor.log(\n",
    "        epoch=final_epoch,\n",
    "        test_accuracy=final_test_metrics[\"accuracy\"],\n",
    "        test_f1_macro=final_test_metrics[\"f1_macro\"],\n",
    "        test_precision_macro=final_test_metrics[\"classification_report\"][\"macro avg\"][\"precision\"],\n",
    "        test_recall_macro=final_test_metrics[\"classification_report\"][\"macro avg\"][\"recall\"],\n",
    "        test_f1_weighted=final_test_metrics[\"classification_report\"][\"weighted avg\"][\"f1-score\"]\n",
    "    )\n",
    "    print(\"✅ Métricas finales loggeadas a wandb\")\n",
    "\n",
    "# Guardar métricas finales\n",
    "final_metrics_path = optuna_results_dir / \"test_metrics_wandb.json\"\n",
    "final_metrics_to_save = {\n",
    "    \"accuracy\": float(final_test_metrics[\"accuracy\"]),\n",
    "    \"f1_macro\": float(final_test_metrics[\"f1_macro\"]),\n",
    "    \"precision_macro\": float(final_test_metrics[\"classification_report\"][\"macro avg\"][\"precision\"]),\n",
    "    \"recall_macro\": float(final_test_metrics[\"classification_report\"][\"macro avg\"][\"recall\"]),\n",
    "    \"f1_weighted\": float(final_test_metrics[\"classification_report\"][\"weighted avg\"][\"f1-score\"]),\n",
    "    \"confusion_matrix\": final_test_metrics[\"confusion_matrix\"].tolist(),\n",
    "    \"best_hyperparameters\": best_params,\n",
    "    \"training_config\": FINAL_TRAINING_CONFIG,\n",
    "    \"final_epoch\": final_epoch,\n",
    "    \"best_val_f1\": best_val_f1,\n",
    "    \"wandb_enabled\": EXPERIMENT_CONFIG[\"use_wandb\"]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(final_metrics_path, \"w\") as f:\n",
    "    json.dump(final_metrics_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Métricas finales guardadas en: {final_metrics_path}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43bNxSHMttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOQUE DE EVALUACIÓN ORIGINAL ELIMINADO\n",
    "# ============================================================\n",
    "# Este bloque fue eliminado para evitar duplicación.\n",
    "# Se usa la evaluación con WANDB que es más completa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD_1sizseB62"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL CON WANDB\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMEN FINAL CON WANDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n🔍 PROCESO DE OPTIMIZACIÓN:\")\n",
    "print(f\"   - Configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score en validación: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 MEJORES HIPERPARÁMETROS ENCONTRADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 RESULTADOS FINALES EN TEST SET:\")\n",
    "print(f\"   - Accuracy:  {final_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {final_test_metrics['classification_report']['macro avg']['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {final_test_metrics['classification_report']['macro avg']['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {final_test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if training_config[\"use_wandb\"]:\n",
    "    print(f\"\\n📊 VISUALIZACIÓN EN WANDB:\")\n",
    "    print(f\"   - Proyecto: {WANDB_CONFIG['project_name']}\")\n",
    "    print(f\"   - Experimento: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
    "    print(f\"   - URL: https://wandb.ai/{WANDB_CONFIG['project_name']}\")\n",
    "    print(f\"   - Métricas en tiempo real disponibles\")\n",
    "\n",
    "print(f\"\\n💾 ARCHIVOS GUARDADOS:\")\n",
    "print(f\"   - best_model_wandb.pth           # Modelo final optimizado\")\n",
    "print(f\"   - test_metrics_wandb.json        # Métricas en test set\")\n",
    "print(f\"   - training_progress_optuna.png   # Gráfica de entrenamiento local\")\n",
    "print(f\"   - confusion_matrix_optuna.png    # Matriz de confusión\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO CON WANDB COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28JxR5lgttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÓN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERANDO VISUALIZACIONES FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Graficar progreso del entrenamiento final\n",
    "final_progress_fig = plot_training_history(\n",
    "    final_history,\n",
    "    save_path=optuna_results_dir / \"training_progress_optuna.png\"\n",
    ")\n",
    "\n",
    "# Matriz de confusión final\n",
    "final_cm = final_test_metrics[\"confusion_matrix\"]\n",
    "final_cm_fig = plot_confusion_matrix(\n",
    "    final_cm,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"],\n",
    "    title=\"Matriz de Confusión - Test Set (CNN2D Optimizado con Optuna)\",\n",
    "    save_path=optuna_results_dir / \"confusion_matrix_optuna.png\",\n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(f\"💾 Visualizaciones guardadas:\")\n",
    "print(f\"   - Progreso de entrenamiento: {optuna_results_dir / 'training_progress_optuna.png'}\")\n",
    "print(f\"   - Matriz de confusión: {optuna_results_dir / 'confusion_matrix_optuna.png'}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQP-xbe3ttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL DE OPTIMIZACIÓN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMEN FINAL DE OPTIMIZACIÓN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n🔍 PROCESO DE OPTIMIZACIÓN:\")\n",
    "print(f\"   - Configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score en validación: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 MEJORES HIPERPARÁMETROS ENCONTRADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 RESULTADOS FINALES EN TEST SET:\")\n",
    "final_report = final_test_metrics[\"classification_report\"]\n",
    "print(f\"   - Accuracy:  {final_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {final_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {final_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {final_test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 ARCHIVOS GUARDADOS EN {optuna_results_dir}:\")\n",
    "print(f\"   - optuna_scan_results.csv          # Todas las configuraciones probadas\")\n",
    "print(f\"   - best_params.json                # Mejores hiperparámetros\")\n",
    "print(f\"   - optimization_summary.txt        # Resumen de optimización\")\n",
    "print(f\"   - best_model_optuna.pth           # Modelo final optimizado\")\n",
    "print(f\"   - test_metrics_optuna.json        # Métricas en test set\")\n",
    "print(f\"   - training_progress_optuna.png    # Gráfica de entrenamiento\")\n",
    "print(f\"   - confusion_matrix_optuna.png   # Matriz de confusión\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZACIÓN CON OPTUNA COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxUMjAU2ttQF"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5glXtwxNttQG"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stg5s-gEttQH"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3DWVC1JttQI"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "parkinson_env",
   "language": "python",
   "name": "parkinson_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
