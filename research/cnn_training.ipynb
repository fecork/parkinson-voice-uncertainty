{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH7omjwwttPo"
   },
   "source": [
    "# CNN 2D para Detección de Parkinson (Baseline con Augmentation + Optuna)\n",
    "## Baseline Model - Train/Val/Test Split + Hyperparameter Optimization\n",
    "\n",
    "Este notebook entrena un modelo **CNN2D simple** (sin Domain Adaptation) para clasificación binaria Parkinson vs Healthy **usando data augmentation** y **optimización automática de hiperparámetros con Optuna**.\n",
    "\n",
    "### Pipeline:\n",
    "1. **Setup**: Configuración del entorno\n",
    "2. **Data Loading**: Carga de datos CON augmentation\n",
    "3. **Split**: Train/Val/Test estratificado (70/15/15)\n",
    "4. **Optuna Optimization**: Optimización automática de hiperparámetros (20 configuraciones)\n",
    "5. **Final Training**: Re-entrenamiento con mejores hiperparámetros + early stopping\n",
    "6. **Evaluation**: Métricas completas en test set\n",
    "7. **Visualization**: Gráficas de progreso y resultados\n",
    "\n",
    "### Arquitectura:\n",
    "Este modelo usa el **mismo Feature Extractor** que CNN2D_DA (arquitectura Ibarra 2023) pero **sin Domain Adaptation**:\n",
    "- 2 bloques Conv2D → BN → ReLU → MaxPool(3×3) → Dropout\n",
    "- Solo cabeza de clasificación PD (sin GRL ni cabeza de dominio)\n",
    "\n",
    "### Data Augmentation:\n",
    "- Pitch shifting\n",
    "- Time stretching\n",
    "- Noise injection\n",
    "- SpecAugment (máscaras de frecuencia/tiempo)\n",
    "- Factor: ~5x más datos\n",
    "\n",
    "### Comparación:\n",
    "- **Este notebook**: Modelo CNN2D con augmentation (mejora generalización)\n",
    "- **cnn_da_training.ipynb**: Modelo CNN2D_DA sin augmentation (paper exacto)\n",
    "- El augmentation permite entrenar con más datos y mejorar robustez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYIbsc8VBC2x",
    "outputId": "6c17d221-3188-404f-ce74-b828151e3660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "$ git config --global --add safe.directory /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty fetch --all --prune\n",
      "Fetching origin\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty branch --show-current\n",
      "feature/feature/firstTraining\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty checkout feature/feature/firstTraining\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN PARA GOOGLE COLAB\n",
    "# ============================================================\n",
    "# DESCOMENTA TODO EL BLOQUE SI EJECUTAS EN COLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Configuración - AJUSTA ESTOS VALORES SI ES NECESARIO\n",
    "COMPUTER_NAME = \"ZenBook\"\n",
    "PROJECT_DIR = \"parkinson-voice-uncertainty\"\n",
    "BRANCH = \"feature/feature/firstTraining\"\n",
    "\n",
    "BASE = \"/content/drive/Othercomputers\"\n",
    "PROJ = os.path.join(BASE, COMPUTER_NAME, PROJECT_DIR)\n",
    "\n",
    "# Función auxiliar\n",
    "def sh(*args, check=False):\n",
    "    print(\"$\", \" \".join(args))\n",
    "    res = subprocess.run(args, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    print(res.stdout)\n",
    "    if check and res.returncode != 0:\n",
    "        raise RuntimeError(\"Command failed\")\n",
    "    return res.returncode\n",
    "\n",
    "# Verificaciones\n",
    "assert os.path.isdir(os.path.join(BASE, COMPUTER_NAME)), f\"No encuentro {COMPUTER_NAME} en {BASE}\"\n",
    "assert os.path.isdir(PROJ), f\"No encuentro el repo en: {PROJ}\"\n",
    "\n",
    "# Agregar al path\n",
    "if PROJ not in sys.path:\n",
    "    sys.path.insert(0, PROJ)\n",
    "\n",
    "# Configurar Git\n",
    "sh(\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", PROJ)\n",
    "sh(\"git\", \"-C\", PROJ, \"fetch\", \"--all\", \"--prune\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n",
    "\n",
    "# Cambiar a rama\n",
    "rc = sh(\"git\", \"-C\", PROJ, \"checkout\", BRANCH)\n",
    "if rc != 0:\n",
    "    sh(\"git\", \"-C\", PROJ, \"checkout\", \"-b\", BRANCH, f\"origin/{BRANCH}\")\n",
    "\n",
    "# Actualizar\n",
    "sh(\"git\", \"-C\", PROJ, \"pull\", \"origin\", BRANCH)\n",
    "\n",
    "# Instalar dependencias con manejo de errores mejorado\n",
    "req = os.path.join(PROJ, \"requirements.txt\")\n",
    "if os.path.exists(req):\n",
    "    os.chdir(\"/content\")\n",
    "    print(\"Instalando dependencias...\")\n",
    "    # Instalar dependencias críticas primero\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"optuna>=3.0.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"torch>=1.9.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"torchvision>=0.10.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"scikit-learn>=1.0.0\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"librosa>=0.8.1\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"soundfile>=0.10.3\")\n",
    "    # Instalar el resto\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req)\n",
    "    print(\"Dependencias instaladas correctamente\")\n",
    "else:\n",
    "    print(\"⚠️  No se encontró requirements.txt, instalando dependencias básicas...\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"optuna>=3.0.0\", \"torch>=1.9.0\", \"scikit-learn>=1.0.0\")\n",
    "\n",
    "os.chdir(PROJ)\n",
    "\n",
    "# Verificar que optuna esté disponible\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"✅ Optuna {optuna.__version__} disponible\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando optuna: {e}\")\n",
    "    print(\"Reinstalando optuna...\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"optuna>=3.0.0\")\n",
    "\n",
    "# Autoreload\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
    "    print(\"Autoreload activo\")\n",
    "except Exception as e:\n",
    "    print(f\"No se activó autoreload: {e}\")\n",
    "\n",
    "print(f\"Repo listo en: {PROJ}\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICACIÓN DE DEPENDENCIAS CRÍTICAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO DEPENDENCIAS CRÍTICAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar imports críticos\n",
    "critical_imports = [\n",
    "    (\"torch\", \"PyTorch\"),\n",
    "    (\"optuna\", \"Optuna\"),\n",
    "    (\"sklearn\", \"Scikit-learn\"),\n",
    "    (\"numpy\", \"NumPy\"),\n",
    "    (\"pandas\", \"Pandas\"),\n",
    "    (\"matplotlib\", \"Matplotlib\"),\n",
    "    (\"librosa\", \"Librosa\"),\n",
    "    (\"soundfile\", \"SoundFile\")\n",
    "]\n",
    "\n",
    "missing_imports = []\n",
    "for module, name in critical_imports:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"✅ {name}: OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ {name}: FALTA - {e}\")\n",
    "        missing_imports.append((module, name))\n",
    "\n",
    "if missing_imports:\n",
    "    print(f\"\\n⚠️  Faltan {len(missing_imports)} dependencias críticas:\")\n",
    "    for module, name in missing_imports:\n",
    "        print(f\"   - {name} ({module})\")\n",
    "    print(\"\\n🔧 Reinstalando dependencias faltantes...\")\n",
    "    \n",
    "    # Reinstalar dependencias faltantes\n",
    "    for module, name in missing_imports:\n",
    "        if module == \"torch\":\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"torch>=1.9.0\"], check=False)\n",
    "        elif module == \"optuna\":\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"optuna>=3.0.0\"], check=False)\n",
    "        elif module == \"sklearn\":\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"scikit-learn>=1.0.0\"], check=False)\n",
    "        elif module == \"librosa\":\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"librosa>=0.8.1\"], check=False)\n",
    "        elif module == \"soundfile\":\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"soundfile>=0.10.3\"], check=False)\n",
    "        else:\n",
    "            subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--force-reinstall\", module], check=False)\n",
    "    \n",
    "    print(\"✅ Reinstalación completada\")\n",
    "else:\n",
    "    print(\"\\n🎉 Todas las dependencias críticas están disponibles\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICACIÓN DE MÓDULOS PROPIOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO MÓDULOS PROPIOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que los módulos propios estén disponibles\n",
    "try:\n",
    "    from modules.core.cnn2d_optuna_wrapper import CNN2DOptunaWrapper, optimize_cnn2d\n",
    "    print(\"✅ CNN2D Optuna Wrapper: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ CNN2D Optuna Wrapper: ERROR - {e}\")\n",
    "    print(\"Verificando estructura del proyecto...\")\n",
    "    import os\n",
    "    print(f\"Directorio actual: {os.getcwd()}\")\n",
    "    print(f\"Contenido: {os.listdir('.')}\")\n",
    "    if os.path.exists('modules'):\n",
    "        print(\"✅ Carpeta modules encontrada\")\n",
    "        print(f\"Contenido de modules: {os.listdir('modules')}\")\n",
    "        if os.path.exists('modules/core'):\n",
    "            print(\"✅ Carpeta modules/core encontrada\")\n",
    "            print(f\"Contenido de modules/core: {os.listdir('modules/core')}\")\n",
    "        else:\n",
    "            print(\"❌ Carpeta modules/core no encontrada\")\n",
    "    else:\n",
    "        print(\"❌ Carpeta modules no encontrada\")\n",
    "\n",
    "try:\n",
    "    from modules.core.optuna_optimization import OptunaOptimizer\n",
    "    print(\"✅ Optuna Optimizer: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Optuna Optimizer: ERROR - {e}\")\n",
    "\n",
    "try:\n",
    "    from modules.models.cnn2d.model import CNN2D\n",
    "    print(\"✅ CNN2D Model: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ CNN2D Model: ERROR - {e}\")\n",
    "\n",
    "# Verificar que optuna esté funcionando correctamente\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"✅ Optuna {optuna.__version__}: OK\")\n",
    "    \n",
    "    # Crear un trial de prueba\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    trial = study.ask()\n",
    "    print(\"✅ Optuna Trial: OK\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Optuna: ERROR - {e}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SOLUCIÓN DE RESPALDO PARA PROBLEMAS DE IMPORTACIÓN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IMPLEMENTANDO SOLUCIÓN DE RESPALDO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Si hay problemas con los módulos, implementar una versión simplificada\n",
    "try:\n",
    "    from modules.core.cnn2d_optuna_wrapper import optimize_cnn2d\n",
    "    print(\"✅ Módulo principal disponible\")\n",
    "    USE_BACKUP = False\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Problema con módulo principal: {e}\")\n",
    "    print(\"🔧 Implementando solución de respaldo...\")\n",
    "    USE_BACKUP = True\n",
    "    \n",
    "    # Implementar una versión simplificada de optimize_cnn2d\n",
    "    def optimize_cnn2d_backup(X_train, y_train, X_val, y_val, input_shape, n_trials=30, n_epochs_per_trial=20, device=\"cpu\", save_dir=None):\n",
    "        \"\"\"\n",
    "        Versión de respaldo de optimize_cnn2d que funciona sin módulos externos.\n",
    "        \"\"\"\n",
    "        import optuna\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "        import pandas as pd\n",
    "        import json\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Importar CNN2D directamente\n",
    "        from modules.models.cnn2d.model import CNN2D\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Hiperparámetros\n",
    "            filters_1 = trial.suggest_categorical(\"filters_1\", [16, 32, 64])\n",
    "            filters_2 = trial.suggest_categorical(\"filters_2\", [32, 64, 128])\n",
    "            kernel_size_1 = trial.suggest_categorical(\"kernel_size_1\", [3, 5])\n",
    "            kernel_size_2 = trial.suggest_categorical(\"kernel_size_2\", [3, 5])\n",
    "            p_drop_conv = trial.suggest_float(\"p_drop_conv\", 0.2, 0.5)\n",
    "            p_drop_fc = trial.suggest_float(\"p_drop_fc\", 0.3, 0.6)\n",
    "            dense_units = trial.suggest_categorical(\"dense_units\", [32, 64, 128])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "            weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "            optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "            \n",
    "            # Crear modelo\n",
    "            model = CNN2D(\n",
    "                input_shape=input_shape[1:],  # (H, W) sin el canal\n",
    "                filters_1=filters_1,\n",
    "                filters_2=filters_2,\n",
    "                kernel_size_1=kernel_size_1,\n",
    "                kernel_size_2=kernel_size_2,\n",
    "                p_drop_conv=p_drop_conv,\n",
    "                p_drop_fc=p_drop_fc,\n",
    "                dense_units=dense_units,\n",
    "            ).to(device)\n",
    "            \n",
    "            # Crear DataLoaders\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            val_dataset = TensorDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Optimizador\n",
    "            if optimizer_name == \"adam\":\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            else:  # sgd\n",
    "                optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Entrenamiento\n",
    "            best_f1 = 0.0\n",
    "            for epoch in range(n_epochs_per_trial):\n",
    "                # Training\n",
    "                model.train()\n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_preds = []\n",
    "                val_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for batch_x, batch_y in val_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_x)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        val_preds.extend(predicted.cpu().numpy())\n",
    "                        val_labels.extend(batch_y.cpu().numpy())\n",
    "                \n",
    "                f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                \n",
    "                # Reportar a Optuna\n",
    "                trial.report(f1, epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "            \n",
    "            return best_f1\n",
    "        \n",
    "        # Crear estudio\n",
    "        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        # Preparar resultados\n",
    "        results_df = pd.DataFrame([\n",
    "            {\n",
    "                'trial': trial.number,\n",
    "                'value': trial.value,\n",
    "                'params': trial.params,\n",
    "                'f1': trial.value,\n",
    "                'accuracy': 0.0,  # Placeholder\n",
    "                'precision': 0.0,  # Placeholder\n",
    "                'recall': 0.0,  # Placeholder\n",
    "                **trial.params\n",
    "            }\n",
    "            for trial in study.trials\n",
    "        ])\n",
    "        \n",
    "        results = {\n",
    "            \"best_params\": study.best_params,\n",
    "            \"best_value\": study.best_value,\n",
    "            \"best_trial\": study.best_trial.number,\n",
    "            \"results_df\": results_df,\n",
    "            \"analysis\": {\"best_trial\": study.best_trial}\n",
    "        }\n",
    "        \n",
    "        # Guardar si se especificó directorio\n",
    "        if save_dir:\n",
    "            save_path = Path(save_dir)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            results_df.to_csv(save_path / \"optuna_trials_results.csv\", index=False)\n",
    "            with open(save_path / \"best_params.json\", 'w') as f:\n",
    "                json.dump(study.best_params, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Reemplazar la función original\n",
    "    optimize_cnn2d = optimize_cnn2d_backup\n",
    "    print(\"✅ Solución de respaldo implementada\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRUEBA RÁPIDA DE OPTUNA (1 TRIAL)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRUEBA RÁPIDA DE OPTUNA - 1 TRIAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Importar dependencias necesarias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Dispositivo: {device}\")\n",
    "\n",
    "# Crear datos de prueba pequeños para verificar que todo funciona\n",
    "print(\"🔧 Creando datos de prueba...\")\n",
    "\n",
    "# Datos de prueba (muy pequeños para verificación rápida)\n",
    "X_test_small = torch.randn(20, 1, 65, 41)  # 20 muestras\n",
    "y_test_small = torch.randint(0, 2, (20,))  # Labels aleatorios\n",
    "\n",
    "# Split de prueba\n",
    "X_train_test = X_test_small[:15]\n",
    "y_train_test = y_test_small[:15]\n",
    "X_val_test = X_test_small[15:]\n",
    "y_val_test = y_test_small[15:]\n",
    "\n",
    "print(f\"✅ Datos de prueba creados:\")\n",
    "print(f\"   - Train: {X_train_test.shape} (labels: {y_train_test.shape})\")\n",
    "print(f\"   - Val:   {X_val_test.shape} (labels: {y_val_test.shape})\")\n",
    "\n",
    "# Probar la función optimize_cnn2d con 1 trial\n",
    "print(f\"\\n🧪 Probando optimize_cnn2d con 1 trial...\")\n",
    "print(\"   (Esto debería tomar menos de 1 minuto)\")\n",
    "\n",
    "try:\n",
    "    # Ejecutar prueba con 1 trial y 2 épocas\n",
    "    test_results = optimize_cnn2d(\n",
    "        X_train=X_train_test,\n",
    "        y_train=y_train_test,\n",
    "        X_val=X_val_test,\n",
    "        y_val=y_val_test,\n",
    "        input_shape=(1, 65, 41),\n",
    "        n_trials=1,  # Solo 1 trial\n",
    "        n_epochs_per_trial=2,  # Solo 2 épocas\n",
    "        device=device,\n",
    "        save_dir=None  # No guardar\n",
    "    )\n",
    "    \n",
    "    print(\"✅ PRUEBA EXITOSA!\")\n",
    "    print(f\"   - Mejor F1: {test_results['best_value']:.4f}\")\n",
    "    print(f\"   - Mejores params: {test_results['best_params']}\")\n",
    "    print(\"   - La función optimize_cnn2d funciona correctamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR EN LA PRUEBA:\")\n",
    "    print(f\"   - Error: {e}\")\n",
    "    print(f\"   - Tipo: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    print(f\"   - Traceback completo:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n🔧 Soluciones posibles:\")\n",
    "    print(\"   1. Verificar que optuna esté instalado correctamente\")\n",
    "    print(\"   2. Verificar que los módulos propios estén disponibles\")\n",
    "    print(\"   3. Revisar la configuración del entorno\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRUEBA ALTERNATIVA - VERIFICAR IMPORTS Y FUNCIONES\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICACIÓN DETALLADA DE IMPORTS Y FUNCIONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar que todas las importaciones funcionen\n",
    "print(\"🔍 Verificando importaciones...\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"✅ optuna {optuna.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ optuna: {e}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✅ torch {torch.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ torch: {e}\")\n",
    "\n",
    "try:\n",
    "    from modules.models.cnn2d.model import CNN2D\n",
    "    print(\"✅ CNN2D model\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ CNN2D model: {e}\")\n",
    "\n",
    "# Verificar que la función optimize_cnn2d esté disponible\n",
    "print(f\"\\n🔍 Verificando función optimize_cnn2d...\")\n",
    "try:\n",
    "    # Verificar que la función esté definida\n",
    "    if 'optimize_cnn2d' in globals():\n",
    "        print(\"✅ optimize_cnn2d está disponible\")\n",
    "        print(f\"   - Tipo: {type(optimize_cnn2d)}\")\n",
    "    else:\n",
    "        print(\"❌ optimize_cnn2d no está disponible\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error verificando optimize_cnn2d: {e}\")\n",
    "\n",
    "# Verificar configuración de Optuna\n",
    "print(f\"\\n🔍 Verificando configuración de Optuna...\")\n",
    "try:\n",
    "    # Crear un estudio de prueba\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    print(\"✅ Optuna study creado correctamente\")\n",
    "    \n",
    "    # Crear un trial de prueba\n",
    "    trial = study.ask()\n",
    "    print(\"✅ Optuna trial creado correctamente\")\n",
    "    \n",
    "    # Verificar que TrialPruned esté disponible\n",
    "    if hasattr(optuna, 'TrialPruned'):\n",
    "        print(\"✅ optuna.TrialPruned disponible\")\n",
    "    else:\n",
    "        print(\"❌ optuna.TrialPruned no disponible\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error con Optuna: {e}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRUEBA FINAL - OPTUNA CON DATOS MÍNIMOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRUEBA FINAL - OPTUNA CON DATOS MÍNIMOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear datos mínimos para prueba\n",
    "print(\"🔧 Creando datos mínimos...\")\n",
    "X_mini = torch.randn(10, 1, 65, 41)  # 10 muestras\n",
    "y_mini = torch.randint(0, 2, (10,))  # Labels aleatorios\n",
    "\n",
    "# Split mínimo\n",
    "X_train_mini = X_mini[:7]\n",
    "y_train_mini = y_mini[:7]\n",
    "X_val_mini = X_mini[7:]\n",
    "y_val_mini = y_mini[7:]\n",
    "\n",
    "print(f\"✅ Datos mínimos creados:\")\n",
    "print(f\"   - Train: {X_train_mini.shape}\")\n",
    "print(f\"   - Val:   {X_val_mini.shape}\")\n",
    "\n",
    "# Función de prueba simplificada\n",
    "def test_optuna_simple():\n",
    "    \"\"\"Prueba muy simple de Optuna\"\"\"\n",
    "    import optuna\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hiperparámetros simples\n",
    "        lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [2, 4])\n",
    "        \n",
    "        # Crear modelo simple\n",
    "        from modules.models.cnn2d.model import CNN2D\n",
    "        model = CNN2D(\n",
    "            input_shape=(65, 41),\n",
    "            filters_1=16,\n",
    "            filters_2=32,\n",
    "            kernel_size_1=3,\n",
    "            kernel_size_2=3,\n",
    "            p_drop_conv=0.2,\n",
    "            p_drop_fc=0.3,\n",
    "            dense_units=32,\n",
    "        ).to(device)\n",
    "        \n",
    "        # DataLoader simple\n",
    "        train_dataset = TensorDataset(X_train_mini, y_train_mini)\n",
    "        val_dataset = TensorDataset(X_val_mini, y_val_mini)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Optimizador\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Entrenamiento mínimo (1 época)\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Métrica simple\n",
    "        f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "        return f1\n",
    "    \n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    \n",
    "    return study.best_value, study.best_params\n",
    "\n",
    "# Ejecutar prueba\n",
    "print(f\"\\n🧪 Ejecutando prueba simple...\")\n",
    "try:\n",
    "    best_value, best_params = test_optuna_simple()\n",
    "    print(\"✅ PRUEBA EXITOSA!\")\n",
    "    print(f\"   - Mejor F1: {best_value:.4f}\")\n",
    "    print(f\"   - Mejores params: {best_params}\")\n",
    "    print(\"   - Optuna funciona correctamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR EN LA PRUEBA:\")\n",
    "    print(f\"   - Error: {e}\")\n",
    "    print(f\"   - Tipo: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    print(f\"   - Traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jjfqJs0ttPt"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTORNO Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio raíz del proyecto al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar el gestor de dependencias centralizado\n",
    "from modules.core.dependency_manager import setup_notebook_environment\n",
    "\n",
    "# Configurar el entorno automáticamente\n",
    "# Esto verifica e instala todas las dependencias necesarias\n",
    "success = setup_notebook_environment(auto_install=True, verbose=True)\n",
    "\n",
    "if not success:\n",
    "    print(\"Error configurando el entorno\")\n",
    "    print(\"Intenta instalar manualmente: pip install -r requirements.txt\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPHApIZxttPy"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACIÓN COMPLETA DEL EXPERIMENTO (PAPER IBARRA 2023)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACIÓN DEL EXPERIMENTO - PAPER IBARRA 2023\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL OPTIMIZADOR (SGD como en el paper)\n",
    "# ============================================================\n",
    "OPTIMIZER_CONFIG = {\n",
    "    \"type\": \"SGD\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,  # Cambiado de 0.0 a 1e-4 para regularización\n",
    "    \"nesterov\": True  # Agregado Nesterov momentum para mejor convergencia\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL SCHEDULER (StepLR como en el paper)\n",
    "# ============================================================\n",
    "SCHEDULER_CONFIG = {\n",
    "    \"type\": \"StepLR\",\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.1\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DEL K-FOLD CROSS-VALIDATION\n",
    "# ============================================================\n",
    "KFOLD_CONFIG = {\n",
    "    \"n_splits\": 10,\n",
    "    \"shuffle\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify_by_speaker\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE CLASS WEIGHTS (para balancear clases)\n",
    "# ============================================================\n",
    "CLASS_WEIGHTS_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"method\": \"inverse_frequency\"  # 1/frequency\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE FILTRADO DE VOCAL /a/\n",
    "# ============================================================\n",
    "VOCAL_FILTER_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"target_vocal\": \"a\",  # Solo vocal /a/ como en el paper\n",
    "    \"filter_healthy\": True,\n",
    "    \"filter_parkinson\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 para evitar overfitting\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"save_best_model\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE OPTUNA (OPTIMIZACIÓN DE HIPERPARÁMETROS)\n",
    "# ============================================================\n",
    "# Optuna reemplaza a Optuna - más moderno, sin problemas de instalación\n",
    "OPTUNA_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"experiment_name\": \"cnn2d_optuna_optimization\",\n",
    "    \"n_trials\": 30,  # Número de configuraciones a probar\n",
    "    \"n_epochs_per_trial\": 20,  # Épocas por configuración\n",
    "    \"metric\": \"f1\",  # Métrica a optimizar\n",
    "    \"direction\": \"maximize\"  # maximize o minimize\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACIÓN DE DATOS\n",
    "# ============================================================\n",
    "DATA_CONFIG = {\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haj4mxa1BC20"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DETECTAR ENTORNO Y CONFIGURAR RUTAS\n",
    "# ============================================================\n",
    "\n",
    "# Este import funciona desde cualquier subdirectorio del proyecto\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar y agregar la raíz del proyecto al path\n",
    "current_dir = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (current_dir / \"modules\").exists():\n",
    "        if str(current_dir) not in sys.path:\n",
    "            sys.path.insert(0, str(current_dir))\n",
    "        break\n",
    "    current_dir = current_dir.parent\n",
    "\n",
    "# Importar la función de configuración de notebooks\n",
    "from modules.core.notebook_setup import setup_notebook\n",
    "\n",
    "# Configurar automáticamente: path + entorno (Local/Colab) + rutas\n",
    "ENV, PATHS = setup_notebook(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4OEii7wttPz"
   },
   "source": [
    "## 1. Setup y Configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgRftyW3ttP0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y CONFIGURACIÓN\n",
    "# ============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Agregar módulos propios al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar módulos propios\n",
    "from modules.models.cnn2d.model import CNN2D\n",
    "from modules.models.common.training_utils import print_model_summary\n",
    "from modules.models.cnn2d.training import train_model, detailed_evaluation, print_evaluation_report\n",
    "from modules.models.cnn2d.visualization import plot_training_history, analyze_spectrogram_stats\n",
    "from modules.models.cnn2d.utils import plot_confusion_matrix\n",
    "from modules.core.utils import create_10fold_splits_by_speaker\n",
    "from modules.core.dataset import (\n",
    "    load_spectrograms_cache,\n",
    "    to_pytorch_tensors,\n",
    "    DictDataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Imports para Optuna (optimización de hiperparámetros - reemplaza Optuna)\n",
    "from modules.core.cnn2d_optuna_wrapper import optimize_cnn2d, create_cnn2d_optimizer\n",
    "from modules.core.optuna_optimization import OptunaOptimizer\n",
    "\n",
    "# Configuración de matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configuración de PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Reporte de configuración\n",
    "print(\"=\"*70)\n",
    "print(\"CNN 2D TRAINING - BASELINE CON AUGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Librerías cargadas correctamente\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Data augmentation: ACTIVADO (~5x datos)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xinu3UYHttP1"
   },
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Carga de datos preprocesados CON augmentation para mejorar generalización del modelo baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBHKgFgnttP1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS HEALTHY DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Healthy desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from modules.core.dataset import load_spectrograms_cache\n",
    "\n",
    "# Cargar datos healthy desde cache original usando rutas dinámicas\n",
    "cache_healthy_path = PATHS['cache_original'] / \"healthy_ibarra.pkl\"\n",
    "healthy_dataset = load_spectrograms_cache(str(cache_healthy_path))\n",
    "\n",
    "if healthy_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontró el cache de datos healthy en {cache_healthy_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_healthy, y_task_healthy, y_domain_healthy, meta_healthy = to_pytorch_tensors(healthy_dataset)\n",
    "\n",
    "print(f\"Healthy cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_healthy.shape[0]}\")\n",
    "print(f\"   - Shape: {X_healthy.shape}\")\n",
    "print(f\"   - Ruta: {cache_healthy_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSBflzwNBC22"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS PARKINSON DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Parkinson desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar datos parkinson desde cache original usando rutas dinámicas\n",
    "cache_parkinson_path = PATHS['cache_original'] / \"parkinson_ibarra.pkl\"\n",
    "parkinson_dataset = load_spectrograms_cache(str(cache_parkinson_path))\n",
    "\n",
    "if parkinson_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontró el cache de datos parkinson en {cache_parkinson_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_parkinson, y_task_parkinson, y_domain_parkinson, meta_parkinson = to_pytorch_tensors(parkinson_dataset)\n",
    "\n",
    "print(f\"Parkinson cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_parkinson.shape[0]}\")\n",
    "print(f\"   - Shape: {X_parkinson.shape}\")\n",
    "print(f\"   - Ruta: {cache_parkinson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtR4xu4ttP2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFORMACIÓN DE DATOS CARGADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMACIÓN DE DATOS CARGADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Datos Healthy (desde cache original):\")\n",
    "print(f\"   - Muestras: {len(healthy_dataset)}\")\n",
    "print(f\"   - Shape de espectrogramas: {X_healthy.shape}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl80VAx4ttP3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÁLISIS ESTADÍSTICO BÁSICO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS ESTADÍSTICO BÁSICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Análisis estadístico básico\n",
    "healthy_stats = analyze_spectrogram_stats(healthy_dataset, \"HEALTHY\")\n",
    "parkinson_stats = analyze_spectrogram_stats(parkinson_dataset, \"PARKINSON\")\n",
    "\n",
    "# Comparar diferencias\n",
    "print(f\"\\nDIFERENCIAS ENTRE CLASES:\")\n",
    "print(f\"   - Diferencia en media: {abs(healthy_stats['mean'] - parkinson_stats['mean']):.3f}\")\n",
    "print(f\"   - Diferencia en std: {abs(healthy_stats['std'] - parkinson_stats['std']):.3f}\")\n",
    "\n",
    "print(\"\\nConfiguración del experimento:\")\n",
    "print(\"   - Healthy: datos originales (baseline)\")\n",
    "print(\"   - Parkinson: datos con augmentation (mejor generalización)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ad1dCwieTUo"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBINAR DATASETS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMBINANDO DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combinar espectrogramas\n",
    "X_combined = torch.cat([X_healthy, X_parkinson], dim=0)\n",
    "\n",
    "# Crear labels: 0=Healthy, 1=Parkinson\n",
    "y_combined = torch.cat([\n",
    "    torch.zeros(len(X_healthy), dtype=torch.long),  # Healthy = 0\n",
    "    torch.ones(len(X_parkinson), dtype=torch.long)  # Parkinson = 1\n",
    "], dim=0)\n",
    "\n",
    "print(f\"\\nDATASET COMBINADO:\")\n",
    "print(f\"   - Total muestras: {len(X_combined)}\")\n",
    "print(f\"   - Shape: {X_combined.shape}\")\n",
    "print(f\"   - Healthy (0): {(y_combined == 0).sum().item()} ({(y_combined == 0).sum()/len(y_combined)*100:.1f}%)\")\n",
    "print(f\"   - Parkinson (1): {(y_combined == 1).sum().item()} ({(y_combined == 1).sum()/len(y_combined)*100:.1f}%)\")\n",
    "\n",
    "balance_pct = (y_combined == 1).sum() / len(y_combined) * 100\n",
    "if abs(balance_pct - 50) < 10:\n",
    "    print(f\"   ✓ Dataset razonablemente balanceado\")\n",
    "else:\n",
    "    print(f\"   ⚠ Dataset desbalanceado - class weights habilitados en config\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIspxysweTUp"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECCIONAR METADATOS PARA SPEAKER IDS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO METADATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar estructura de metadatos\n",
    "if meta_healthy and len(meta_healthy) > 0:\n",
    "    print(f\"\\n✓ meta_healthy disponible: {len(meta_healthy)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_healthy[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ✗ meta_healthy no disponible o vacío\")\n",
    "\n",
    "if meta_parkinson and len(meta_parkinson) > 0:\n",
    "    print(f\"\\n✓ meta_parkinson disponible: {len(meta_parkinson)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_parkinson[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ✗ meta_parkinson no disponible o vacío\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMZElStXttP3"
   },
   "source": [
    "## 3. Split Train/Val/Test\n",
    "\n",
    "Split estratificado 70/15/15 para mantener proporciones de clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj5q0QU5ttP4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10-FOLD CROSS-VALIDATION ESTRATIFICADO POR HABLANTE\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"10-FOLD CROSS-VALIDATION (PAPER IBARRA 2023)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparar metadata combinada para create_10fold_splits_by_speaker\n",
    "# La metadata ya fue cargada antes con meta_healthy y meta_parkinson\n",
    "\n",
    "# Crear lista de metadata combinada con labels\n",
    "metadata_combined = []\n",
    "\n",
    "# Agregar metadata de healthy (label=0)\n",
    "for meta in meta_healthy:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 0,  # Healthy\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "# Agregar metadata de parkinson (label=1)\n",
    "for meta in meta_parkinson:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 1,  # Parkinson\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "print(f\"\\n📊 Dataset info:\")\n",
    "print(f\"   • Total samples: {len(X_combined)}\")\n",
    "print(f\"   • Metadata entries: {len(metadata_combined)}\")\n",
    "\n",
    "# Crear 10-fold splits usando la función centralizada\n",
    "# Esta función asegura que todos los samples de un speaker están en el mismo fold\n",
    "fold_splits = create_10fold_splits_by_speaker(\n",
    "    metadata_list=metadata_combined,\n",
    "    n_folds=KFOLD_CONFIG[\"n_splits\"],\n",
    "    seed=KFOLD_CONFIG[\"random_state\"]\n",
    ")\n",
    "\n",
    "# Para este notebook, usaremos el primer fold como ejemplo\n",
    "# En el paper real se promedian los resultados de los 10 folds\n",
    "train_indices = fold_splits[0][\"train\"]\n",
    "val_indices = fold_splits[0][\"val\"]\n",
    "\n",
    "# Crear splits de train/val usando los índices\n",
    "X_train = X_combined[train_indices]\n",
    "y_train = y_combined[train_indices]\n",
    "X_val = X_combined[val_indices]\n",
    "y_val = y_combined[val_indices]\n",
    "\n",
    "# Para test, usamos un split separado del 15%\n",
    "# TODO: Esto debería también usar split por speaker para evitar leakage\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_combined, y_combined,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nTAMAÑOS DE SPLITS:\")\n",
    "print(f\"   - Train: {len(X_train)} ({len(X_train)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Val:   {len(X_val)} ({len(X_val)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Test:  {len(X_test)} ({len(X_test)/len(X_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDISTRIBUCIÓN POR SPLIT:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    n_healthy = (y_split == 0).sum().item()\n",
    "    n_parkinson = (y_split == 1).sum().item()\n",
    "    print(f\"   {split_name:5s}: HC={n_healthy:4d} ({n_healthy/len(y_split)*100:.1f}%), PD={n_parkinson:4d} ({n_parkinson/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asWglEGTttP5"
   },
   "outputs": [],
   "source": [
    "# Agregar módulos propios al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================\n",
    "# CREAR DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📦 CREANDO DATALOADERS...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Importar DictDataset desde el módulo core\n",
    "\n",
    "# Crear datasets con formato de diccionario\n",
    "train_dataset = DictDataset(X_train, y_train)\n",
    "val_dataset = DictDataset(X_val, y_val)\n",
    "test_dataset = DictDataset(X_test, y_test)\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"✅ DataLoaders creados:\")\n",
    "print(f\"   • Train batches: {len(train_loader)}\")\n",
    "print(f\"   • Val batches:   {len(val_loader)}\")\n",
    "print(f\"   • Test batches:  {len(test_loader)}\")\n",
    "print(f\"   • Batch size:    {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW21Fj0QttP6"
   },
   "source": [
    "## 4. Optimización de Hiperparámetros con Optuna\n",
    "\n",
    "Optimización automática de hiperparámetros usando Optuna para encontrar la mejor configuración del modelo CNN2D.\n",
    "\n",
    "### Configuración:\n",
    "- **Método**: Optuna con búsqueda aleatoria\n",
    "- **Configuraciones**: 30% de todas las combinaciones posibles\n",
    "- **Épocas por config**: 20 épocas (búsqueda rápida)\n",
    "- **Métrica**: F1-score en validación\n",
    "- **Espacio de búsqueda**: Según tabla del paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijmy_JiDttP6"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR OPTIMIZACIÓN CON OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO OPTIMIZACIÓN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear directorio para resultados de Optuna usando rutas dinámicas\n",
    "optuna_results_dir = PATHS['results'] / \"cnn_optuna_optimization\"\n",
    "optuna_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Módulos de Optuna importados\")\n",
    "print(f\"Directorio de resultados: {optuna_results_dir}\")\n",
    "print(f\"Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPNAfUw_ttP7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARAR DATOS PARA OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARANDO DATOS PARA OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optuna trabaja directamente con PyTorch tensors (no requiere numpy)\n",
    "# Los tensors ya están listos desde la carga de datos\n",
    "\n",
    "print(f\"📊 Datos preparados para Optuna:\")\n",
    "print(f\"   - Train: {X_train.shape} (labels: {y_train.shape})\")\n",
    "print(f\"   - Val:   {X_val.shape} (labels: {y_val.shape})\")\n",
    "print(f\"   - Test:  {X_test.shape} (labels: {y_test.shape})\")\n",
    "\n",
    "# Verificar distribución de clases\n",
    "print(f\"\\n📈 Distribución de clases:\")\n",
    "print(f\"   Train - HC: {(y_train == 0).sum().item()}, PD: {(y_train == 1).sum().item()}\")\n",
    "print(f\"   Val   - HC: {(y_val == 0).sum().item()}, PD: {(y_val == 1).sum().item()}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EulPu2cHttP8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR SI YA EXISTEN RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO RESULTADOS PREVIOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración de la optimización usando configuración centralizada\n",
    "# (OPTUNA_CONFIG ya está definido en la configuración centralizada)\n",
    "\n",
    "# Verificar si ya existen resultados previos\n",
    "results_csv_path = optuna_results_dir / \"optuna_trials_results.csv\"\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "\n",
    "if results_csv_path.exists() and best_params_path.exists():\n",
    "    print(\"✅ Se encontraron resultados previos de Optuna\")\n",
    "    print(f\"   - Archivo de resultados: {results_csv_path}\")\n",
    "    print(f\"   - Archivo de mejores parámetros: {best_params_path}\")\n",
    "\n",
    "    # Cargar resultados previos\n",
    "    results_df = pd.read_csv(results_csv_path)\n",
    "    with open(best_params_path, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "    print(f\"\\n📊 Resultados previos encontrados:\")\n",
    "    print(f\"   - Total trials evaluados: {len(results_df)}\")\n",
    "    print(f\"   - Mejor F1-score encontrado: {results_df['value'].max():.4f}\")\n",
    "    print(f\"   - F1-score promedio: {results_df['value'].mean():.4f} ± {results_df['value'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "    # Crear diccionario de resultados para compatibilidad\n",
    "    optuna_results = {\n",
    "        \"results_df\": results_df,\n",
    "        \"best_params\": best_params,\n",
    "        \"study\": None  # El study se carga separadamente si es necesario\n",
    "    }\n",
    "\n",
    "    print(f\"\\n⏭️  Saltando optimización - usando resultados previos\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No se encontraron resultados previos de Optuna\")\n",
    "    print(\"   - Iniciando optimización desde cero\")\n",
    "\n",
    "    print(f\"\\n⚙️  Configuración:\")\n",
    "    print(f\"   - Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "    print(f\"   - Épocas por trial: {OPTUNA_CONFIG['n_epochs_per_trial']}\")\n",
    "    print(f\"   - Métrica a optimizar: {OPTUNA_CONFIG['metric']} ({OPTUNA_CONFIG['direction']})\")\n",
    "\n",
    "    print(f\"\\n🚀 Iniciando búsqueda de hiperparámetros con Optuna...\")\n",
    "    print(\"   (Esto puede tomar varios minutos)\")\n",
    "\n",
    "    # Ejecutar optimización\n",
    "    optuna_results = optimize_cnn2d(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        input_shape=(1, 65, 41),  # (C, H, W)\n",
    "        n_trials=OPTUNA_CONFIG[\"n_trials\"],\n",
    "        n_epochs_per_trial=OPTUNA_CONFIG[\"n_epochs_per_trial\"],\n",
    "        device=device,\n",
    "        save_dir=str(optuna_results_dir)\n",
    "    )\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"OPTIMIZACIÓN COMPLETADA\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88jfG5bVttP8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANÁLISIS DE RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extraer resultados\n",
    "results_df = optuna_results[\"results_df\"]\n",
    "best_params = optuna_results[\"best_params\"]\n",
    "analysis = optuna_results[\"analysis\"]\n",
    "\n",
    "print(f\"📊 Resumen de la optimización:\")\n",
    "print(f\"   - Total configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score encontrado: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 Mejores hiperparámetros encontrados:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "# Mostrar top 10 configuraciones\n",
    "print(f\"\\n📈 Top 10 configuraciones:\")\n",
    "print(\"-\" * 80)\n",
    "top_10 = results_df.nlargest(10, 'f1')\n",
    "for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "    print(f\"{i:2d}. F1: {row['f1']:.4f} | \"\n",
    "          f\"Acc: {row['accuracy']:.4f} | \"\n",
    "          f\"Batch: {row['batch_size']} | \"\n",
    "          f\"LR: {row['learning_rate']} | \"\n",
    "          f\"Dropout: {row['p_drop_conv']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o888G9VGttP9"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar DataFrame completo con todas las configuraciones\n",
    "results_csv_path = optuna_results_dir / \"optuna_scan_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"💾 Resultados completos guardados: {results_csv_path}\")\n",
    "\n",
    "# Guardar mejores parámetros\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"💾 Mejores parámetros guardados: {best_params_path}\")\n",
    "\n",
    "# Guardar resumen de optimización\n",
    "summary_path = optuna_results_dir / \"optimization_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"RESUMEN DE OPTIMIZACIÓN OPTUNA\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Total configuraciones evaluadas: {len(results_df)}\\n\")\n",
    "    f.write(f\"Mejor F1-score: {results_df['f1'].max():.4f}\\n\")\n",
    "    f.write(f\"F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\\n\\n\")\n",
    "    f.write(\"MEJORES HIPERPARÁMETROS:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    for param, value in best_params.items():\n",
    "        if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(\"\\nTOP 5 CONFIGURACIONES:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    top_5 = results_df.nlargest(5, 'f1')\n",
    "    for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "        f.write(f\"{i}. F1: {row['f1']:.4f} | Acc: {row['accuracy']:.4f} | \"\n",
    "                f\"Batch: {row['batch_size']} | LR: {row['learning_rate']}\\n\")\n",
    "\n",
    "print(f\"💾 Resumen guardado: {summary_path}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0dr_0HlttP-"
   },
   "outputs": [],
   "source": [
    "# Agregar módulos propios al path\n",
    "# El notebook está en research/, pero modules/ está en el directorio raíz\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================\n",
    "# EVALUAR RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACIÓN DE RESULTADOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Importar evaluador\n",
    "\n",
    "# Evaluar el proceso de optimización\n",
    "# evaluation = check_optuna_results(str(optuna_results_dir))\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wihb557SttP-"
   },
   "source": [
    "## 5. Re-entrenamiento con Mejores Hiperparámetros\n",
    "\n",
    "Re-entrenar el modelo CNN2D usando los mejores hiperparámetros encontrados por Optuna, con early stopping para obtener el modelo final optimizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NVWEzLkttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO CON MEJORES HIPERPARÁMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREANDO MODELO CON MEJORES HIPERPARÁMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear modelo con mejores parámetros encontrados por Optuna\n",
    "best_model = CNN2D(\n",
    "    n_classes=2,\n",
    "    p_drop_conv=best_params[\"p_drop_conv\"],\n",
    "    p_drop_fc=best_params[\"p_drop_fc\"],\n",
    "    input_shape=(65, 41),\n",
    "    filters_1=best_params[\"filters_1\"],\n",
    "    filters_2=best_params[\"filters_2\"],\n",
    "    kernel_size_1=best_params[\"kernel_size_1\"],\n",
    "    kernel_size_2=best_params[\"kernel_size_2\"],\n",
    "    dense_units=best_params[\"dense_units\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"✅ Modelo creado con mejores hiperparámetros:\")\n",
    "print(f\"   - Filters 1: {best_params['filters_1']}\")\n",
    "print(f\"   - Filters 2: {best_params['filters_2']}\")\n",
    "print(f\"   - Kernel 1: {best_params['kernel_size_1']}\")\n",
    "print(f\"   - Kernel 2: {best_params['kernel_size_2']}\")\n",
    "print(f\"   - Dense units: {best_params['dense_units']}\")\n",
    "print(f\"   - Dropout conv: {best_params['p_drop_conv']}\")\n",
    "print(f\"   - Dropout fc: {best_params['p_drop_fc']}\")\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print_model_summary(best_model)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86kelbHEttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTRENAMIENTO CON MEJORES PARÁMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO ENTRENAMIENTO FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración de entrenamiento final usando configuración centralizada\n",
    "FINAL_TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 (recomendación)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": best_params[\"batch_size\"]\n",
    "}\n",
    "\n",
    "# Crear DataLoaders con el mejor batch size\n",
    "train_loader_final = DataLoader(\n",
    "    train_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader_final = DataLoader(\n",
    "    val_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader_final = DataLoader(\n",
    "    test_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Optimizador SGD con momentum usando configuración centralizada\n",
    "# CORREGIDO: Agregado nesterov=True y weight_decay=1e-4\n",
    "optimizer_final = optim.SGD(\n",
    "    best_model.parameters(),\n",
    "    lr=FINAL_TRAINING_CONFIG['learning_rate'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,  # Cambiado de 0.0 a 1e-4\n",
    "    nesterov=True  # Agregado Nesterov momentum\n",
    ")\n",
    "\n",
    "# Calcular class weights para balancear las clases usando configuración centralizada\n",
    "if CLASS_WEIGHTS_CONFIG[\"enabled\"]:\n",
    "    class_counts = torch.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    print(f\"✅ Class weights habilitados: {class_weights.tolist()}\")\n",
    "else:\n",
    "    criterion_final = nn.CrossEntropyLoss()\n",
    "    print(\"⚠️  Class weights deshabilitados\")\n",
    "\n",
    "# Scheduler StepLR usando configuración centralizada\n",
    "scheduler_final = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer_final,\n",
    "    step_size=SCHEDULER_CONFIG[\"step_size\"],\n",
    "    gamma=SCHEDULER_CONFIG[\"gamma\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n⚙️  Configuración final:\")\n",
    "print(f\"   - Learning rate inicial: {FINAL_TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"   - Momentum: 0.9 (Nesterov: True)\")\n",
    "print(f\"   - Weight decay: 1e-4\")\n",
    "print(f\"   - Scheduler: StepLR (step={SCHEDULER_CONFIG['step_size']}, gamma={SCHEDULER_CONFIG['gamma']})\")\n",
    "print(f\"   - Batch size: {FINAL_TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   - Épocas máximas: {FINAL_TRAINING_CONFIG['n_epochs']}\")\n",
    "print(f\"   - Early stopping patience: {FINAL_TRAINING_CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fg5tzkUttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO FINAL CON EARLY STOPPING\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO MODELO FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entrenar modelo con mejores hiperparámetros\n",
    "# CORREGIDO: Usar keyword arguments y monitorear val_f1 (mejor para datasets desbalanceados)\n",
    "final_training_results = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader_final,\n",
    "    val_loader=val_loader_final,\n",
    "    optimizer=optimizer_final,\n",
    "    criterion=criterion_final,\n",
    "    device=device,\n",
    "    n_epochs=FINAL_TRAINING_CONFIG['n_epochs'],\n",
    "    early_stopping_patience=FINAL_TRAINING_CONFIG['early_stopping_patience'],\n",
    "    save_dir=optuna_results_dir,\n",
    "    verbose=True,\n",
    "    scheduler=scheduler_final,\n",
    "    monitor_metric=\"f1\"  # Monitorear F1 en lugar de loss (recomendado para desbalance)\n",
    ")\n",
    "\n",
    "# Extraer resultados\n",
    "final_model = final_training_results[\"model\"]\n",
    "final_history = final_training_results[\"history\"]\n",
    "final_best_val_loss = final_training_results[\"best_val_loss\"]\n",
    "final_total_time = final_training_results[\"total_time\"]\n",
    "\n",
    "# Calcular mejor época\n",
    "final_best_epoch = final_history[\"val_loss\"].index(min(final_history[\"val_loss\"])) + 1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO FINAL COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✅ Resultados:\")\n",
    "print(f\"   - Mejor época: {final_best_epoch}\")\n",
    "print(f\"   - Mejor val loss: {final_best_val_loss:.4f}\")\n",
    "print(f\"   - Tiempo total: {final_total_time/60:.1f} minutos\")\n",
    "print(f\"   - Modelo guardado en: {optuna_results_dir / 'best_model_optuna.pth'}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43bNxSHMttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACIÓN FINAL EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACIÓN FINAL EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluar modelo final en test set\n",
    "final_test_metrics = detailed_evaluation(\n",
    "    model=final_model,\n",
    "    loader=test_loader_final,\n",
    "    device=device,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"]\n",
    ")\n",
    "\n",
    "# Imprimir reporte\n",
    "print_evaluation_report(final_test_metrics, class_names=[\"Healthy\", \"Parkinson\"])\n",
    "\n",
    "# Guardar métricas finales\n",
    "final_metrics_path = optuna_results_dir / \"test_metrics_optuna.json\"\n",
    "\n",
    "# Extraer métricas del classification_report\n",
    "final_report = final_test_metrics[\"classification_report\"]\n",
    "final_metrics_to_save = {\n",
    "    \"accuracy\": float(final_test_metrics[\"accuracy\"]),\n",
    "    \"f1_macro\": float(final_test_metrics[\"f1_macro\"]),\n",
    "    \"precision_macro\": float(final_report[\"macro avg\"][\"precision\"]),\n",
    "    \"recall_macro\": float(final_report[\"macro avg\"][\"recall\"]),\n",
    "    \"f1_weighted\": float(final_report[\"weighted avg\"][\"f1-score\"]),\n",
    "    \"confusion_matrix\": final_test_metrics[\"confusion_matrix\"].tolist(),\n",
    "    \"classification_report\": final_report,\n",
    "    \"best_hyperparameters\": best_params,\n",
    "    \"training_config\": FINAL_TRAINING_CONFIG,\n",
    "    \"final_epoch\": final_best_epoch,\n",
    "    \"final_val_loss\": final_best_val_loss,\n",
    "    \"training_time_minutes\": final_total_time / 60\n",
    "}\n",
    "\n",
    "with open(final_metrics_path, \"w\") as f:\n",
    "    json.dump(final_metrics_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Métricas finales guardadas en: {final_metrics_path}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28JxR5lgttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACIÓN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERANDO VISUALIZACIONES FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Graficar progreso del entrenamiento final\n",
    "final_progress_fig = plot_training_history(\n",
    "    final_history,\n",
    "    save_path=optuna_results_dir / \"training_progress_optuna.png\"\n",
    ")\n",
    "\n",
    "# Matriz de confusión final\n",
    "final_cm = final_test_metrics[\"confusion_matrix\"]\n",
    "final_cm_fig = plot_confusion_matrix(\n",
    "    final_cm,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"],\n",
    "    title=\"Matriz de Confusión - Test Set (CNN2D Optimizado con Optuna)\",\n",
    "    save_path=optuna_results_dir / \"confusion_matrix_optuna.png\",\n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(f\"💾 Visualizaciones guardadas:\")\n",
    "print(f\"   - Progreso de entrenamiento: {optuna_results_dir / 'training_progress_optuna.png'}\")\n",
    "print(f\"   - Matriz de confusión: {optuna_results_dir / 'confusion_matrix_optuna.png'}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQP-xbe3ttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL DE OPTIMIZACIÓN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMEN FINAL DE OPTIMIZACIÓN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n🔍 PROCESO DE OPTIMIZACIÓN:\")\n",
    "print(f\"   - Configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score en validación: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 MEJORES HIPERPARÁMETROS ENCONTRADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 RESULTADOS FINALES EN TEST SET:\")\n",
    "final_report = final_test_metrics[\"classification_report\"]\n",
    "print(f\"   - Accuracy:  {final_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {final_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {final_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {final_test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 ARCHIVOS GUARDADOS EN {optuna_results_dir}:\")\n",
    "print(f\"   - optuna_scan_results.csv          # Todas las configuraciones probadas\")\n",
    "print(f\"   - best_params.json                # Mejores hiperparámetros\")\n",
    "print(f\"   - optimization_summary.txt        # Resumen de optimización\")\n",
    "print(f\"   - best_model_optuna.pth           # Modelo final optimizado\")\n",
    "print(f\"   - test_metrics_optuna.json        # Métricas en test set\")\n",
    "print(f\"   - training_progress_optuna.png    # Gráfica de entrenamiento\")\n",
    "print(f\"   - confusion_matrix_optuna.png   # Matriz de confusión\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZACIÓN CON OPTUNA COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxUMjAU2ttQF"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5glXtwxNttQG"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stg5s-gEttQH"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3DWVC1JttQI"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "parkinson_env",
   "language": "python",
   "name": "parkinson_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
