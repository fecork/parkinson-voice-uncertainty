{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH7omjwwttPo"
   },
   "source": [
    "# CNN 2D para Detecci√≥n de Parkinson (Baseline con Augmentation + Optuna)\n",
    "## Baseline Model - Train/Val/Test Split + Hyperparameter Optimization\n",
    "\n",
    "Este notebook entrena un modelo **CNN2D simple** (sin Domain Adaptation) para clasificaci√≥n binaria Parkinson vs Healthy **usando data augmentation** y **optimizaci√≥n autom√°tica de hiperpar√°metros con Optuna**.\n",
    "\n",
    "### Pipeline:\n",
    "1. **Setup**: Configuraci√≥n del entorno\n",
    "2. **Data Loading**: Carga de datos CON augmentation\n",
    "3. **Split**: Train/Val/Test estratificado (70/15/15)\n",
    "4. **Optuna Optimization**: Optimizaci√≥n autom√°tica de hiperpar√°metros (20 configuraciones)\n",
    "5. **Final Training**: Re-entrenamiento con mejores hiperpar√°metros + early stopping\n",
    "6. **Evaluation**: M√©tricas completas en test set\n",
    "7. **Visualization**: Gr√°ficas de progreso y resultados\n",
    "\n",
    "### Arquitectura:\n",
    "Este modelo usa el **mismo Feature Extractor** que CNN2D_DA (arquitectura Ibarra 2023) pero **sin Domain Adaptation**:\n",
    "- 2 bloques Conv2D ‚Üí BN ‚Üí ReLU ‚Üí MaxPool(3√ó3) ‚Üí Dropout\n",
    "- Solo cabeza de clasificaci√≥n PD (sin GRL ni cabeza de dominio)\n",
    "\n",
    "### Data Augmentation:\n",
    "- Pitch shifting\n",
    "- Time stretching\n",
    "- Noise injection\n",
    "- SpecAugment (m√°scaras de frecuencia/tiempo)\n",
    "- Factor: ~5x m√°s datos\n",
    "\n",
    "### Comparaci√≥n:\n",
    "- **Este notebook**: Modelo CNN2D con augmentation (mejora generalizaci√≥n)\n",
    "- **cnn_da_training.ipynb**: Modelo CNN2D_DA sin augmentation (paper exacto)\n",
    "- El augmentation permite entrenar con m√°s datos y mejorar robustez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278017,
     "status": "ok",
     "timestamp": 1761407113580,
     "user": {
      "displayName": "Ferney Cordoba Canchala",
      "userId": "08591795942797397856"
     },
     "user_tz": 300
    },
    "id": "LYIbsc8VBC2x",
    "outputId": "3a2382ef-ec53-4ec3-856f-e21106cc684d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "$ git config --global --add safe.directory /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty fetch --all --prune\n",
      "Fetching origin\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty branch --show-current\n",
      "feature/feature/firstTraining\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty checkout feature/feature/firstTraining\n",
      "Already on 'feature/feature/firstTraining'\n",
      "M\t.gitignore\n",
      "M\t.ipynb_checkpoints/parkinson_voice_analysis-checkpoint.ipynb\n",
      "M\tREADME.md\n",
      "M\tbash.exe.stackdump\n",
      "M\tdata/README.md\n",
      "M\tdata/vowels_healthy/healthy_diverse_metadata.json\n",
      "M\tdata_preparation/sample_healthy_data.py\n",
      "M\tdata_preparation/verify_sampling.py\n",
      "M\tdocs/MIGRATION_TALOS_TO_OPTUNA.md\n",
      "M\texamples/environment_example.py\n",
      "M\tmodules/README.md\n",
      "M\tmodules/__init__.py\n",
      "M\tmodules/core/README.md\n",
      "M\tmodules/core/__init__.py\n",
      "M\tmodules/core/cnn2d_optuna_wrapper.py\n",
      "M\tmodules/core/cnn2d_talos_wrapper.py\n",
      "M\tmodules/core/data_augmentation.ipynb\n",
      "M\tmodules/core/dataset.py\n",
      "M\tmodules/core/dependency_manager.py\n",
      "M\tmodules/core/environment.py\n",
      "M\tmodules/core/model_evaluation.py\n",
      "M\tmodules/core/notebook_setup.py\n",
      "M\tmodules/core/optuna_optimization.py\n",
      "M\tmodules/core/preprocessing.py\n",
      "M\tmodules/core/sequence_dataset.py\n",
      "M\tmodules/core/talos_analysis.py\n",
      "M\tmodules/core/talos_evaluator.py\n",
      "M\tmodules/core/talos_optimization.py\n",
      "M\tmodules/core/talos_visualization.py\n",
      "M\tmodules/core/utils.py\n",
      "M\tmodules/core/visualization.py\n",
      "M\tmodules/data/__init__.py\n",
      "M\tmodules/data/augmentation.py\n",
      "M\tmodules/data/cache_utils.py\n",
      "M\tmodules/models/__init__.py\n",
      "M\tmodules/models/cnn1d/__init__.py\n",
      "M\tmodules/models/cnn1d/model.py\n",
      "M\tmodules/models/cnn1d/training.py\n",
      "M\tmodules/models/cnn1d/visualization.py\n",
      "M\tmodules/models/cnn2d/__init__.py\n",
      "M\tmodules/models/cnn2d/inference.py\n",
      "M\tmodules/models/cnn2d/model.py\n",
      "M\tmodules/models/cnn2d/talos_optimization.py\n",
      "M\tmodules/models/cnn2d/training.py\n",
      "M\tmodules/models/cnn2d/utils.py\n",
      "M\tmodules/models/cnn2d/visualization.py\n",
      "M\tmodules/models/common/__init__.py\n",
      "M\tmodules/models/common/layers.py\n",
      "M\tmodules/models/common/training_utils.py\n",
      "M\tmodules/models/common/visualization_utils.py\n",
      "M\tmodules/models/lstm_da/__init__.py\n",
      "M\tmodules/models/lstm_da/model.py\n",
      "M\tmodules/models/lstm_da/training.py\n",
      "M\tmodules/models/lstm_da/visualization.py\n",
      "M\tmodules/models/uncertainty/__init__.py\n",
      "M\tmodules/models/uncertainty/loss.py\n",
      "M\tmodules/models/uncertainty/model.py\n",
      "M\tmodules/models/uncertainty/training.py\n",
      "M\tmodules/models/uncertainty/visualization.py\n",
      "M\tnotebooks/cnn_uncertainty_training.ipynb\n",
      "M\tnotebooks/colab_setup_example.py\n",
      "M\tnotebooks/data_augmentation.ipynb\n",
      "M\tnotebooks/gradcam_inference.ipynb\n",
      "M\tnotebooks/svdd_data_preparation.ipynb\n",
      "M\tpipelines/README.md\n",
      "M\tpipelines/generate_lstm_sequences.py\n",
      "M\tpipelines/train_cnn.py\n",
      "M\tpipelines/train_cnn_da_kfold.py\n",
      "M\tpipelines/train_cnn_uncertainty.py\n",
      "M\tpipelines/train_lstm_da_kfold.py\n",
      "M\trequirements.txt\n",
      "M\tresearch/README.md\n",
      "M\tresearch/cnn1d_da_training.ipynb\n",
      "M\tresearch/cnn_da_training.ipynb\n",
      "M\tresearch/cnn_training.ipynb\n",
      "M\tresearch/lstm_da_training.ipynb\n",
      "M\tresults/README.md\n",
      "M\tresults/cnn1d_da/test_metrics_1d_da.json\n",
      "M\tresults/cnn_da/test_metrics_da.json\n",
      "M\tresults/cnn_no_da/test_metrics.json\n",
      "M\tresults/cnn_talos_optimization/best_params.json\n",
      "M\tresults/cnn_talos_optimization/talos_scan_results.csv\n",
      "M\tresults/cnn_uncertainty/gradcam_config.json\n",
      "M\tresults/cnn_uncertainty/test_metrics_uncertainty.json\n",
      "M\ttest/Feature_Extractor_2D_CNN_Visualization.ipynb\n",
      "M\ttest/PAPER_VALIDATION_REPORT.md\n",
      "M\ttest/README.md\n",
      "M\ttest/README_TESTS.md\n",
      "M\ttest/evaluate_talos.py\n",
      "M\ttest/paper_requirements.json\n",
      "M\ttest/test_cnn1d_attention.py\n",
      "M\ttest/test_cnn1d_implementation.py\n",
      "M\ttest/test_cnn2d_optuna.py\n",
      "M\ttest/test_cnn_architectures.py\n",
      "M\ttest/test_core_talos.py\n",
      "M\ttest/test_environment.py\n",
      "M\ttest/test_gradcam_math.py\n",
      "M\ttest/test_grl_completo.py\n",
      "M\ttest/test_ibarra_implementation.py\n",
      "M\ttest/test_ibarra_preprocessing.py\n",
      "M\ttest/test_learning_rate_scheduler.py\n",
      "M\ttest/test_lstm_da_implementation.py\n",
      "M\ttest/test_lstm_sequences.py\n",
      "M\ttest/test_optuna_basic.py\n",
      "M\ttest/test_paper_compliance.py\n",
      "M\ttest/test_talos_basic.py\n",
      "M\ttest/test_talos_file_generation.py\n",
      "M\ttest/test_talos_integration.py\n",
      "M\ttest/test_talos_optimization.py\n",
      "M\ttest/test_talos_real_files.py\n",
      "M\ttest/test_uncertainty_math.py\n",
      "M\ttest/test_yarin_gal_implementation.py\n",
      "M\ttest/validate_paper_replication.py\n",
      "M\ttest/verify_labels.py\n",
      "\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty pull origin feature/feature/firstTraining\n",
      "From https://github.com/fecork/parkinson-voice-uncertainty\n",
      " * branch            feature/feature/firstTraining -> FETCH_HEAD\n",
      "Already up to date.\n",
      "\n",
      "$ python -m pip install -q -r /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty/requirements.txt\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400.9/400.9 kB 33.9 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250.6/250.6 kB 25.5 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 12.3/12.3 MB 102.0 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.6/1.6 MB 92.0 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 76.7/76.7 kB 8.1 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59.8/59.8 kB 6.6 MB/s eta 0:00:00\n",
      "\n",
      "No se activ√≥ autoreload: No module named 'imp'\n",
      "Repo listo en: /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty\n",
      "$ git -C /content/drive/Othercomputers/ZenBook/parkinson-voice-uncertainty branch --show-current\n",
      "feature/feature/firstTraining\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACI√ìN PARA GOOGLE COLAB\n",
    "# ============================================================\n",
    "# DESCOMENTA TODO EL BLOQUE SI EJECUTAS EN COLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Configuraci√≥n - AJUSTA ESTOS VALORES SI ES NECESARIO\n",
    "COMPUTER_NAME = \"ZenBook\"\n",
    "PROJECT_DIR = \"parkinson-voice-uncertainty\"\n",
    "BRANCH = \"feature/feature/firstTraining\"\n",
    "\n",
    "BASE = \"/content/drive/Othercomputers\"\n",
    "PROJ = os.path.join(BASE, COMPUTER_NAME, PROJECT_DIR)\n",
    "\n",
    "# Funci√≥n auxiliar\n",
    "def sh(*args, check=False):\n",
    "    print(\"$\", \" \".join(args))\n",
    "    res = subprocess.run(args, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    print(res.stdout)\n",
    "    if check and res.returncode != 0:\n",
    "        raise RuntimeError(\"Command failed\")\n",
    "    return res.returncode\n",
    "\n",
    "# Verificaciones\n",
    "assert os.path.isdir(os.path.join(BASE, COMPUTER_NAME)), f\"No encuentro {COMPUTER_NAME} en {BASE}\"\n",
    "assert os.path.isdir(PROJ), f\"No encuentro el repo en: {PROJ}\"\n",
    "\n",
    "# Agregar al path\n",
    "if PROJ not in sys.path:\n",
    "    sys.path.insert(0, PROJ)\n",
    "\n",
    "# Configurar Git\n",
    "sh(\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", PROJ)\n",
    "sh(\"git\", \"-C\", PROJ, \"fetch\", \"--all\", \"--prune\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n",
    "\n",
    "# Cambiar a rama\n",
    "rc = sh(\"git\", \"-C\", PROJ, \"checkout\", BRANCH)\n",
    "if rc != 0:\n",
    "    sh(\"git\", \"-C\", PROJ, \"checkout\", \"-b\", BRANCH, f\"origin/{BRANCH}\")\n",
    "\n",
    "# Actualizar\n",
    "sh(\"git\", \"-C\", PROJ, \"pull\", \"origin\", BRANCH)\n",
    "\n",
    "# Instalar dependencias\n",
    "req = os.path.join(PROJ, \"requirements.txt\")\n",
    "if os.path.exists(req):\n",
    "    os.chdir(\"/content\")\n",
    "    sh(\"python\", \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req)\n",
    "os.chdir(PROJ)\n",
    "\n",
    "# Autoreload\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
    "    print(\"Autoreload activo\")\n",
    "except Exception as e:\n",
    "    print(f\"No se activ√≥ autoreload: {e}\")\n",
    "\n",
    "print(f\"Repo listo en: {PROJ}\")\n",
    "sh(\"git\", \"-C\", PROJ, \"branch\", \"--show-current\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 48832,
     "status": "error",
     "timestamp": 1761407162424,
     "user": {
      "displayName": "Ferney Cordoba Canchala",
      "userId": "08591795942797397856"
     },
     "user_tz": 300
    },
    "id": "-jjfqJs0ttPt",
    "outputId": "271c401a-ca32-47e6-93b0-df28f0753e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configurando entorno para notebook...\n",
      "üîç Informaci√≥n del entorno:\n",
      "   python_version: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]\n",
      "   platform: win32\n",
      "   is_colab: False\n",
      "   is_jupyter: True\n",
      "   working_directory: c:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\research\n",
      "   torch_version: 2.8.0+cpu\n",
      "   cuda_available: False\n",
      "üîç Estado de dependencias:\n",
      "   ‚úÖ PyTorch\n",
      "   ‚úÖ TorchVision\n",
      "   ‚úÖ NumPy\n",
      "   ‚úÖ Pandas\n",
      "   ‚úÖ Scikit-learn\n",
      "   ‚úÖ Matplotlib\n",
      "   ‚úÖ Seaborn\n",
      "   ‚úÖ Librosa\n",
      "   ‚úÖ SoundFile\n",
      "   ‚úÖ Optuna\n",
      "   ‚úÖ Jupyter\n",
      "\n",
      "‚úÖ Entorno listo - todas las dependencias disponibles\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTORNO Y DEPENDENCIAS\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio ra√≠z del proyecto al path\n",
    "# El notebook est√° en research/, pero modules/ est√° en el directorio ra√≠z\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar el gestor de dependencias centralizado\n",
    "from modules.core.dependency_manager import setup_notebook_environment\n",
    "\n",
    "# Configurar el entorno autom√°ticamente\n",
    "# Esto verifica e instala todas las dependencias necesarias\n",
    "success = setup_notebook_environment(auto_install=True, verbose=True)\n",
    "\n",
    "if not success:\n",
    "    print(\"Error configurando el entorno\")\n",
    "    print(\"Intenta instalar manualmente: pip install -r requirements.txt\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPHApIZxttPy",
    "outputId": "6d2e9ecf-4ed6-4ee3-d9ce-30956bf6e5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACI√ìN DEL EXPERIMENTO - PAPER IBARRA 2023\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACI√ìN COMPLETA DEL EXPERIMENTO (PAPER IBARRA 2023)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACI√ìN DEL EXPERIMENTO - PAPER IBARRA 2023\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DEL OPTIMIZADOR (SGD como en el paper)\n",
    "# ============================================================\n",
    "OPTIMIZER_CONFIG = {\n",
    "    \"type\": \"SGD\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,  # Cambiado de 0.0 a 1e-4 para regularizaci√≥n\n",
    "    \"nesterov\": True  # Agregado Nesterov momentum para mejor convergencia\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DEL SCHEDULER (StepLR como en el paper)\n",
    "# ============================================================\n",
    "SCHEDULER_CONFIG = {\n",
    "    \"type\": \"StepLR\",\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.1\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DEL K-FOLD CROSS-VALIDATION\n",
    "# ============================================================\n",
    "KFOLD_CONFIG = {\n",
    "    \"n_splits\": 10,\n",
    "    \"shuffle\": True,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify_by_speaker\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE CLASS WEIGHTS (para balancear clases)\n",
    "# ============================================================\n",
    "CLASS_WEIGHTS_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"method\": \"inverse_frequency\"  # 1/frequency\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE FILTRADO DE VOCAL /a/\n",
    "# ============================================================\n",
    "VOCAL_FILTER_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"target_vocal\": \"a\",  # Solo vocal /a/ como en el paper\n",
    "    \"filter_healthy\": True,\n",
    "    \"filter_parkinson\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE ENTRENAMIENTO\n",
    "# ============================================================\n",
    "TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 para evitar overfitting\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"save_best_model\": True\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE OPTUNA (OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS)\n",
    "# ============================================================\n",
    "# Optuna reemplaza a Optuna - m√°s moderno, sin problemas de instalaci√≥n\n",
    "OPTUNA_CONFIG = {\n",
    "    \"enabled\": True,\n",
    "    \"experiment_name\": \"cnn2d_optuna_optimization\",\n",
    "    \"n_trials\": 30,  # N√∫mero de configuraciones a probar\n",
    "    \"n_epochs_per_trial\": 20,  # √âpocas por configuraci√≥n\n",
    "    \"metric\": \"f1\",  # M√©trica a optimizar\n",
    "    \"direction\": \"maximize\"  # maximize o minimize\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN DE DATOS\n",
    "# ============================================================\n",
    "DATA_CONFIG = {\n",
    "    \"test_size\": 0.15,\n",
    "    \"val_size\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "    \"stratify\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "haj4mxa1BC20",
    "outputId": "98d01e10-1a3e-43ce-9bb0-c7143e8fda28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ra√≠z del proyecto agregada al path: c:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\n",
      "======================================================================\n",
      "CONFIGURACI√ìN DE ENTORNO\n",
      "======================================================================\n",
      "Entorno detectado: LOCAL\n",
      "Ruta base: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\n",
      "Cache original: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\original\n",
      "Cache augmented: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\augmented\n",
      "\n",
      "MODO LOCAL: Usando rutas relativas\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DETECTAR ENTORNO Y CONFIGURAR RUTAS\n",
    "# ============================================================\n",
    "\n",
    "# Este import funciona desde cualquier subdirectorio del proyecto\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar y agregar la ra√≠z del proyecto al path\n",
    "current_dir = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (current_dir / \"modules\").exists():\n",
    "        if str(current_dir) not in sys.path:\n",
    "            sys.path.insert(0, str(current_dir))\n",
    "        break\n",
    "    current_dir = current_dir.parent\n",
    "\n",
    "# Importar la funci√≥n de configuraci√≥n de notebooks\n",
    "from modules.core.notebook_setup import setup_notebook\n",
    "\n",
    "# Configurar autom√°ticamente: path + entorno (Local/Colab) + rutas\n",
    "ENV, PATHS = setup_notebook(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4OEii7wttPz"
   },
   "source": [
    "## 1. Setup y Configuraci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LgRftyW3ttP0",
    "outputId": "efbb67b2-5b15-43ee-c261-7edc7374c96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CNN 2D TRAINING - BASELINE CON AUGMENTATION\n",
      "======================================================================\n",
      "Librer√≠as cargadas correctamente\n",
      "Dispositivo: cpu\n",
      "PyTorch: 2.8.0+cpu\n",
      "Data augmentation: ACTIVADO (~5x datos)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS Y CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Agregar m√≥dulos propios al path\n",
    "# El notebook est√° en research/, pero modules/ est√° en el directorio ra√≠z\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Importar m√≥dulos propios\n",
    "from modules.models.cnn2d.model import CNN2D\n",
    "from modules.models.common.training_utils import print_model_summary\n",
    "from modules.models.cnn2d.training import train_model, detailed_evaluation, print_evaluation_report\n",
    "from modules.models.cnn2d.visualization import plot_training_history, analyze_spectrogram_stats\n",
    "from modules.models.cnn2d.utils import plot_confusion_matrix\n",
    "from modules.core.utils import create_10fold_splits_by_speaker\n",
    "from modules.core.dataset import (\n",
    "    load_spectrograms_cache,\n",
    "    to_pytorch_tensors,\n",
    "    DictDataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Imports para Optuna (optimizaci√≥n de hiperpar√°metros - reemplaza Optuna)\n",
    "from modules.core.cnn2d_optuna_wrapper import optimize_cnn2d, create_cnn2d_optimizer\n",
    "from modules.core.optuna_optimization import OptunaOptimizer\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configuraci√≥n de PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Reporte de configuraci√≥n\n",
    "print(\"=\"*70)\n",
    "print(\"CNN 2D TRAINING - BASELINE CON AUGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Librer√≠as cargadas correctamente\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Data augmentation: ACTIVADO (~5x datos)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xinu3UYHttP1"
   },
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Carga de datos preprocesados CON augmentation para mejorar generalizaci√≥n del modelo baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LBHKgFgnttP1",
    "outputId": "3e2c1c58-05db-455a-c266-3ab06228c139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos Healthy desde cache original...\n",
      "============================================================\n",
      "Cache cargado: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\original\\healthy_ibarra.pkl\n",
      "   Tama√±o: 933.5 MB\n",
      "   Muestras: 12029\n",
      "PyTorch tensors listos:\n",
      "  - X: (12029, 1, 65, 41)\n",
      "  - y_task: (12029,)  (dist={0: 12029})\n",
      "  - y_domain: (12029,)  (K dominios=4)\n",
      "Healthy cargado exitosamente:\n",
      "   - Espectrogramas: 12029\n",
      "   - Shape: torch.Size([12029, 1, 65, 41])\n",
      "   - Ruta: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\original\\healthy_ibarra.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS HEALTHY DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Healthy desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from modules.core.dataset import load_spectrograms_cache\n",
    "\n",
    "# Cargar datos healthy desde cache original usando rutas din√°micas\n",
    "cache_healthy_path = PATHS['cache_original'] / \"healthy_ibarra.pkl\"\n",
    "healthy_dataset = load_spectrograms_cache(str(cache_healthy_path))\n",
    "\n",
    "if healthy_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ el cache de datos healthy en {cache_healthy_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_healthy, y_task_healthy, y_domain_healthy, meta_healthy = to_pytorch_tensors(healthy_dataset)\n",
    "\n",
    "print(f\"Healthy cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_healthy.shape[0]}\")\n",
    "print(f\"   - Shape: {X_healthy.shape}\")\n",
    "print(f\"   - Ruta: {cache_healthy_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YSBflzwNBC22",
    "outputId": "0f7cae11-8fd1-4b0c-9bd4-0b26c8704277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos Parkinson desde cache original...\n",
      "============================================================\n",
      "Cache cargado: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\original\\parkinson_ibarra.pkl\n",
      "   Tama√±o: 1792.3 MB\n",
      "   Muestras: 23097\n",
      "PyTorch tensors listos:\n",
      "  - X: (23097, 1, 65, 41)\n",
      "  - y_task: (23097,)  (dist={0: 23097})\n",
      "  - y_domain: (23097,)  (K dominios=4)\n",
      "Parkinson cargado exitosamente:\n",
      "   - Espectrogramas: 23097\n",
      "   - Shape: torch.Size([23097, 1, 65, 41])\n",
      "   - Ruta: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\cache\\original\\parkinson_ibarra.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS PARKINSON DESDE CACHE ORIGINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando datos Parkinson desde cache original...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar datos parkinson desde cache original usando rutas din√°micas\n",
    "cache_parkinson_path = PATHS['cache_original'] / \"parkinson_ibarra.pkl\"\n",
    "parkinson_dataset = load_spectrograms_cache(str(cache_parkinson_path))\n",
    "\n",
    "if parkinson_dataset is None:\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ el cache de datos parkinson en {cache_parkinson_path}\")\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_parkinson, y_task_parkinson, y_domain_parkinson, meta_parkinson = to_pytorch_tensors(parkinson_dataset)\n",
    "\n",
    "print(f\"Parkinson cargado exitosamente:\")\n",
    "print(f\"   - Espectrogramas: {X_parkinson.shape[0]}\")\n",
    "print(f\"   - Shape: {X_parkinson.shape}\")\n",
    "print(f\"   - Ruta: {cache_parkinson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yUtR4xu4ttP2",
    "outputId": "40ca70dc-8ce4-4126-fb23-bbbacdaaea7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INFORMACI√ìN DE DATOS CARGADOS\n",
      "======================================================================\n",
      "Datos Healthy (desde cache original):\n",
      "   - Muestras: 12029\n",
      "   - Shape de espectrogramas: torch.Size([12029, 1, 65, 41])\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INFORMACI√ìN DE DATOS CARGADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMACI√ìN DE DATOS CARGADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Datos Healthy (desde cache original):\")\n",
    "print(f\"   - Muestras: {len(healthy_dataset)}\")\n",
    "print(f\"   - Shape de espectrogramas: {X_healthy.shape}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fl80VAx4ttP3",
    "outputId": "dbab07ba-5df8-4b48-8544-a954a30a68d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AN√ÅLISIS ESTAD√çSTICO B√ÅSICO\n",
      "======================================================================\n",
      "\n",
      "HEALTHY:\n",
      "   ‚Ä¢ N√∫mero de espectrogramas: 12029\n",
      "   ‚Ä¢ Shape t√≠pico: (65, 41)\n",
      "   ‚Ä¢ Media: 0.000\n",
      "   ‚Ä¢ Desviaci√≥n est√°ndar: 1.000\n",
      "   ‚Ä¢ Min: -2.465\n",
      "   ‚Ä¢ Max: 6.155\n",
      "\n",
      "PARKINSON:\n",
      "   ‚Ä¢ N√∫mero de espectrogramas: 23097\n",
      "   ‚Ä¢ Shape t√≠pico: (65, 41)\n",
      "   ‚Ä¢ Media: 0.000\n",
      "   ‚Ä¢ Desviaci√≥n est√°ndar: 1.000\n",
      "   ‚Ä¢ Min: -3.360\n",
      "   ‚Ä¢ Max: 3.726\n",
      "\n",
      "DIFERENCIAS ENTRE CLASES:\n",
      "   - Diferencia en media: 0.000\n",
      "   - Diferencia en std: 0.000\n",
      "\n",
      "Configuraci√≥n del experimento:\n",
      "   - Healthy: datos originales (baseline)\n",
      "   - Parkinson: datos con augmentation (mejor generalizaci√≥n)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AN√ÅLISIS ESTAD√çSTICO B√ÅSICO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS ESTAD√çSTICO B√ÅSICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# An√°lisis estad√≠stico b√°sico\n",
    "healthy_stats = analyze_spectrogram_stats(healthy_dataset, \"HEALTHY\")\n",
    "parkinson_stats = analyze_spectrogram_stats(parkinson_dataset, \"PARKINSON\")\n",
    "\n",
    "# Comparar diferencias\n",
    "print(f\"\\nDIFERENCIAS ENTRE CLASES:\")\n",
    "print(f\"   - Diferencia en media: {abs(healthy_stats['mean'] - parkinson_stats['mean']):.3f}\")\n",
    "print(f\"   - Diferencia en std: {abs(healthy_stats['std'] - parkinson_stats['std']):.3f}\")\n",
    "\n",
    "print(\"\\nConfiguraci√≥n del experimento:\")\n",
    "print(\"   - Healthy: datos originales (baseline)\")\n",
    "print(\"   - Parkinson: datos con augmentation (mejor generalizaci√≥n)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMBINANDO DATASETS\n",
      "======================================================================\n",
      "\n",
      "DATASET COMBINADO:\n",
      "   - Total muestras: 35126\n",
      "   - Shape: torch.Size([35126, 1, 65, 41])\n",
      "   - Healthy (0): 12029 (34.2%)\n",
      "   - Parkinson (1): 23097 (65.8%)\n",
      "   ‚ö† Dataset desbalanceado - class weights habilitados en config\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMBINAR DATASETS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMBINANDO DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combinar espectrogramas\n",
    "X_combined = torch.cat([X_healthy, X_parkinson], dim=0)\n",
    "\n",
    "# Crear labels: 0=Healthy, 1=Parkinson\n",
    "y_combined = torch.cat([\n",
    "    torch.zeros(len(X_healthy), dtype=torch.long),  # Healthy = 0\n",
    "    torch.ones(len(X_parkinson), dtype=torch.long)  # Parkinson = 1\n",
    "], dim=0)\n",
    "\n",
    "print(f\"\\nDATASET COMBINADO:\")\n",
    "print(f\"   - Total muestras: {len(X_combined)}\")\n",
    "print(f\"   - Shape: {X_combined.shape}\")\n",
    "print(f\"   - Healthy (0): {(y_combined == 0).sum().item()} ({(y_combined == 0).sum()/len(y_combined)*100:.1f}%)\")\n",
    "print(f\"   - Parkinson (1): {(y_combined == 1).sum().item()} ({(y_combined == 1).sum()/len(y_combined)*100:.1f}%)\")\n",
    "\n",
    "balance_pct = (y_combined == 1).sum() / len(y_combined) * 100\n",
    "if abs(balance_pct - 50) < 10:\n",
    "    print(f\"   ‚úì Dataset razonablemente balanceado\")\n",
    "else:\n",
    "    print(f\"   ‚ö† Dataset desbalanceado - class weights habilitados en config\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICANDO METADATOS\n",
      "======================================================================\n",
      "\n",
      "‚úì meta_healthy disponible: 12029 muestras\n",
      "  Ejemplo de metadata[0]:\n",
      "    Tipo: <class 'modules.core.dataset.SampleMeta'>\n",
      "    Valor: SampleMeta(subject_id='10', vowel_type='a_n', condition='unknown', filename='10-a_n.wav', segment_id=0, sr=44100)\n",
      "\n",
      "‚úì meta_parkinson disponible: 23097 muestras\n",
      "  Ejemplo de metadata[0]:\n",
      "    Tipo: <class 'modules.core.dataset.SampleMeta'>\n",
      "    Valor: SampleMeta(subject_id='1037', vowel_type='a_h', condition='unknown', filename='1037-a_h.wav', segment_id=0, sr=44100)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INSPECCIONAR METADATOS PARA SPEAKER IDS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO METADATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar estructura de metadatos\n",
    "if meta_healthy and len(meta_healthy) > 0:\n",
    "    print(f\"\\n‚úì meta_healthy disponible: {len(meta_healthy)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_healthy[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ‚úó meta_healthy no disponible o vac√≠o\")\n",
    "\n",
    "if meta_parkinson and len(meta_parkinson) > 0:\n",
    "    print(f\"\\n‚úì meta_parkinson disponible: {len(meta_parkinson)} muestras\")\n",
    "    print(f\"  Ejemplo de metadata[0]:\")\n",
    "    sample_meta = meta_parkinson[0]\n",
    "    if isinstance(sample_meta, dict):\n",
    "        for key, value in list(sample_meta.items())[:5]:\n",
    "            print(f\"    - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"    Tipo: {type(sample_meta)}\")\n",
    "        print(f\"    Valor: {sample_meta}\")\n",
    "else:\n",
    "    print(\"  ‚úó meta_parkinson no disponible o vac√≠o\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMZElStXttP3"
   },
   "source": [
    "## 3. Split Train/Val/Test\n",
    "\n",
    "Split estratificado 70/15/15 para mantener proporciones de clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rj5q0QU5ttP4",
    "outputId": "f6d29c7e-b96c-4e71-8633-3d305d8208d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "10-FOLD CROSS-VALIDATION (PAPER IBARRA 2023)\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset info:\n",
      "   ‚Ä¢ Total samples: 35126\n",
      "   ‚Ä¢ Metadata entries: 35126\n",
      "\n",
      "üìä 10-Fold CV speaker-independent creado:\n",
      "   Total hablantes: 2042\n",
      "   Total muestras: 35126\n",
      "   Folds: 10\n",
      "\n",
      "   Fold 1 (ejemplo):\n",
      "      Train: 31789 muestras\n",
      "      Val:   3337 muestras\n",
      "\n",
      "TAMA√ëOS DE SPLITS:\n",
      "   - Train: 31789 (90.5%)\n",
      "   - Val:   3337 (9.5%)\n",
      "   - Test:  5269 (15.0%)\n",
      "\n",
      "DISTRIBUCI√ìN POR SPLIT:\n",
      "   Train: HC=10916 (34.3%), PD=20873 (65.7%)\n",
      "   Val  : HC=1113 (33.4%), PD=2224 (66.6%)\n",
      "   Test : HC=1804 (34.2%), PD=3465 (65.8%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10-FOLD CROSS-VALIDATION ESTRATIFICADO POR HABLANTE\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"10-FOLD CROSS-VALIDATION (PAPER IBARRA 2023)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparar metadata combinada para create_10fold_splits_by_speaker\n",
    "# La metadata ya fue cargada antes con meta_healthy y meta_parkinson\n",
    "\n",
    "# Crear lista de metadata combinada con labels\n",
    "metadata_combined = []\n",
    "\n",
    "# Agregar metadata de healthy (label=0)\n",
    "for meta in meta_healthy:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 0,  # Healthy\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "# Agregar metadata de parkinson (label=1)\n",
    "for meta in meta_parkinson:\n",
    "    metadata_combined.append({\n",
    "        \"subject_id\": meta.subject_id,\n",
    "        \"label\": 1,  # Parkinson\n",
    "        \"filename\": meta.filename\n",
    "    })\n",
    "\n",
    "print(f\"\\nüìä Dataset info:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(X_combined)}\")\n",
    "print(f\"   ‚Ä¢ Metadata entries: {len(metadata_combined)}\")\n",
    "\n",
    "# Crear 10-fold splits usando la funci√≥n centralizada\n",
    "# Esta funci√≥n asegura que todos los samples de un speaker est√°n en el mismo fold\n",
    "fold_splits = create_10fold_splits_by_speaker(\n",
    "    metadata_list=metadata_combined,\n",
    "    n_folds=KFOLD_CONFIG[\"n_splits\"],\n",
    "    seed=KFOLD_CONFIG[\"random_state\"]\n",
    ")\n",
    "\n",
    "# Para este notebook, usaremos el primer fold como ejemplo\n",
    "# En el paper real se promedian los resultados de los 10 folds\n",
    "train_indices = fold_splits[0][\"train\"]\n",
    "val_indices = fold_splits[0][\"val\"]\n",
    "\n",
    "# Crear splits de train/val usando los √≠ndices\n",
    "X_train = X_combined[train_indices]\n",
    "y_train = y_combined[train_indices]\n",
    "X_val = X_combined[val_indices]\n",
    "y_val = y_combined[val_indices]\n",
    "\n",
    "# Para test, usamos un split separado del 15%\n",
    "# TODO: Esto deber√≠a tambi√©n usar split por speaker para evitar leakage\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_combined, y_combined,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nTAMA√ëOS DE SPLITS:\")\n",
    "print(f\"   - Train: {len(X_train)} ({len(X_train)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Val:   {len(X_val)} ({len(X_val)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   - Test:  {len(X_test)} ({len(X_test)/len(X_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDISTRIBUCI√ìN POR SPLIT:\")\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    n_healthy = (y_split == 0).sum().item()\n",
    "    n_parkinson = (y_split == 1).sum().item()\n",
    "    print(f\"   {split_name:5s}: HC={n_healthy:4d} ({n_healthy/len(y_split)*100:.1f}%), PD={n_parkinson:4d} ({n_parkinson/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "asWglEGTttP5",
    "outputId": "8ea632c2-c2c3-4995-d579-74d04af1e0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ CREANDO DATALOADERS...\n",
      "‚úÖ DataLoaders creados:\n",
      "   ‚Ä¢ Train batches: 994\n",
      "   ‚Ä¢ Val batches:   53\n",
      "   ‚Ä¢ Test batches:  83\n",
      "   ‚Ä¢ Batch size:    32\n"
     ]
    }
   ],
   "source": [
    "# Agregar m√≥dulos propios al path\n",
    "# El notebook est√° en research/, pero modules/ est√° en el directorio ra√≠z\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================\n",
    "# CREAR DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüì¶ CREANDO DATALOADERS...\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Importar DictDataset desde el m√≥dulo core\n",
    "\n",
    "# Crear datasets con formato de diccionario\n",
    "train_dataset = DictDataset(X_train, y_train)\n",
    "val_dataset = DictDataset(X_val, y_val)\n",
    "test_dataset = DictDataset(X_test, y_test)\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados:\")\n",
    "print(f\"   ‚Ä¢ Train batches: {len(train_loader)}\")\n",
    "print(f\"   ‚Ä¢ Val batches:   {len(val_loader)}\")\n",
    "print(f\"   ‚Ä¢ Test batches:  {len(test_loader)}\")\n",
    "print(f\"   ‚Ä¢ Batch size:    {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW21Fj0QttP6"
   },
   "source": [
    "## 4. Optimizaci√≥n de Hiperpar√°metros con Optuna\n",
    "\n",
    "Optimizaci√≥n autom√°tica de hiperpar√°metros usando Optuna para encontrar la mejor configuraci√≥n del modelo CNN2D.\n",
    "\n",
    "### Configuraci√≥n:\n",
    "- **M√©todo**: Optuna con b√∫squeda aleatoria\n",
    "- **Configuraciones**: 30% de todas las combinaciones posibles\n",
    "- **√âpocas por config**: 20 √©pocas (b√∫squeda r√°pida)\n",
    "- **M√©trica**: F1-score en validaci√≥n\n",
    "- **Espacio de b√∫squeda**: Seg√∫n tabla del paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ijmy_JiDttP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURANDO OPTIMIZACI√ìN CON OPTUNA\n",
      "======================================================================\n",
      "M√≥dulos de Optuna importados\n",
      "Directorio de resultados: C:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\results\\cnn_optuna_optimization\n",
      "Trials a ejecutar: 30\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR OPTIMIZACI√ìN CON OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO OPTIMIZACI√ìN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear directorio para resultados de Optuna usando rutas din√°micas\n",
    "optuna_results_dir = PATHS['results'] / \"cnn_optuna_optimization\"\n",
    "optuna_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"M√≥dulos de Optuna importados\")\n",
    "print(f\"Directorio de resultados: {optuna_results_dir}\")\n",
    "print(f\"Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dPNAfUw_ttP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARANDO DATOS PARA OPTUNA\n",
      "======================================================================\n",
      "üìä Datos preparados para Optuna:\n",
      "   - Train: torch.Size([31789, 1, 65, 41]) (labels: torch.Size([31789]))\n",
      "   - Val:   torch.Size([3337, 1, 65, 41]) (labels: torch.Size([3337]))\n",
      "   - Test:  torch.Size([5269, 1, 65, 41]) (labels: torch.Size([5269]))\n",
      "\n",
      "üìà Distribuci√≥n de clases:\n",
      "   Train - HC: 10916, PD: 20873\n",
      "   Val   - HC: 1113, PD: 2224\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREPARAR DATOS PARA OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARANDO DATOS PARA OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optuna trabaja directamente con PyTorch tensors (no requiere numpy)\n",
    "# Los tensors ya est√°n listos desde la carga de datos\n",
    "\n",
    "print(f\"üìä Datos preparados para Optuna:\")\n",
    "print(f\"   - Train: {X_train.shape} (labels: {y_train.shape})\")\n",
    "print(f\"   - Val:   {X_val.shape} (labels: {y_val.shape})\")\n",
    "print(f\"   - Test:  {X_test.shape} (labels: {y_test.shape})\")\n",
    "\n",
    "# Verificar distribuci√≥n de clases\n",
    "print(f\"\\nüìà Distribuci√≥n de clases:\")\n",
    "print(f\"   Train - HC: {(y_train == 0).sum().item()}, PD: {(y_train == 1).sum().item()}\")\n",
    "print(f\"   Val   - HC: {(y_val == 0).sum().item()}, PD: {(y_val == 1).sum().item()}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EulPu2cHttP8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICANDO RESULTADOS PREVIOS DE OPTUNA\n",
      "======================================================================\n",
      "‚ùå No se encontraron resultados previos de Optuna\n",
      "   - Iniciando optimizaci√≥n desde cero\n",
      "\n",
      "‚öôÔ∏è  Configuraci√≥n:\n",
      "   - Trials a ejecutar: 30\n",
      "   - √âpocas por trial: 20\n",
      "   - M√©trica a optimizar: f1 (maximize)\n",
      "\n",
      "üöÄ Iniciando b√∫squeda de hiperpar√°metros con Optuna...\n",
      "   (Esto puede tomar varios minutos)\n",
      "Iniciando optimizaci√≥n con 30 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 12:29:05,385] A new study created in memory with name: cnn2d_optuna\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6f72d3be34bf682b3caa0d8a0af43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-25 12:29:37,609] Trial 0 failed with parameters: {'filters_1': 32, 'filters_2': 32, 'kernel_size_1': 5, 'kernel_size_2': 5, 'p_drop_conv': 0.20617534828874073, 'p_drop_fc': 0.5909729556485983, 'dense_units': 32, 'learning_rate': 3.5498788321965036e-05, 'weight_decay': 8.179499475211674e-06, 'optimizer': 'adam', 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"c:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\optuna_optimization.py\", line 240, in <lambda>\n",
      "    lambda trial: self._objective(trial, X_train, y_train, X_val, y_val),\n",
      "  File \"c:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\optuna_optimization.py\", line 201, in _objective\n",
      "    metric_value, metrics_dict = self.model_wrapper.train_model(\n",
      "  File \"c:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\cnn2d_optuna_wrapper.py\", line 149, in train_model\n",
      "    loss = criterion(outputs, batch_y)\n",
      "  File \"c:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1310, in forward\n",
      "    return F.cross_entropy(\n",
      "  File \"c:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py\", line 3462, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-25 12:29:37,748] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   (Esto puede tomar varios minutos)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Ejecutar optimizaci√≥n\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m optuna_results \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_cnn2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m41\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (C, H, W)\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTUNA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_trials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTUNA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_epochs_per_trial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptuna_results_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPTIMIZACI√ìN COMPLETADA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\cnn2d_optuna_wrapper.py:291\u001b[0m, in \u001b[0;36moptimize_cnn2d\u001b[1;34m(X_train, y_train, X_val, y_val, input_shape, n_trials, n_epochs_per_trial, device, save_dir)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Ejecutar optimizaci√≥n\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando optimizaci√≥n con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trials...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Obtener resultados\u001b[39;00m\n\u001b[0;32m    294\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mget_best_params(),\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mget_best_value(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39manalyze_results(),\n\u001b[0;32m    300\u001b[0m }\n",
      "File \u001b[1;32mc:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\optuna_optimization.py:239\u001b[0m, in \u001b[0;36mOptunaOptimizer.optimize\u001b[1;34m(self, X_train, y_train, X_val, y_val, show_progress)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    230\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_name,\n\u001b[0;32m    231\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Ejecutar optimizaci√≥n\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Convertir resultados a DataFrame\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mtrials_dataframe()\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32mc:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\optuna_optimization.py:240\u001b[0m, in \u001b[0;36mOptunaOptimizer.optimize.<locals>.<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    230\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_name,\n\u001b[0;32m    231\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Ejecutar optimizaci√≥n\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    241\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials,\n\u001b[0;32m    242\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Convertir resultados a DataFrame\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mtrials_dataframe()\n",
      "File \u001b[1;32mc:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\optuna_optimization.py:201\u001b[0m, in \u001b[0;36mOptunaOptimizer._objective\u001b[1;34m(self, trial, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    198\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m metric_value, metrics_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs_per_trial\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric_value\n",
      "File \u001b[1;32mc:\\Proyectos\\PHD- Parkinson - Incertidumbre - Prototipo\\parkinson-voice-uncertainty\\modules\\core\\cnn2d_optuna_wrapper.py:149\u001b[0m, in \u001b[0;36mCNN2DOptunaWrapper.train_model\u001b[1;34m(self, model, train_loader, val_loader, trial, n_epochs)\u001b[0m\n\u001b[0;32m    147\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    148\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[1;32m--> 149\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    151\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1310\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fecor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3462\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3461\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VERIFICAR SI YA EXISTEN RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICANDO RESULTADOS PREVIOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuraci√≥n de la optimizaci√≥n usando configuraci√≥n centralizada\n",
    "# (OPTUNA_CONFIG ya est√° definido en la configuraci√≥n centralizada)\n",
    "\n",
    "# Verificar si ya existen resultados previos\n",
    "results_csv_path = optuna_results_dir / \"optuna_trials_results.csv\"\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "\n",
    "if results_csv_path.exists() and best_params_path.exists():\n",
    "    print(\"‚úÖ Se encontraron resultados previos de Optuna\")\n",
    "    print(f\"   - Archivo de resultados: {results_csv_path}\")\n",
    "    print(f\"   - Archivo de mejores par√°metros: {best_params_path}\")\n",
    "\n",
    "    # Cargar resultados previos\n",
    "    results_df = pd.read_csv(results_csv_path)\n",
    "    with open(best_params_path, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "    print(f\"\\nüìä Resultados previos encontrados:\")\n",
    "    print(f\"   - Total trials evaluados: {len(results_df)}\")\n",
    "    print(f\"   - Mejor F1-score encontrado: {results_df['value'].max():.4f}\")\n",
    "    print(f\"   - F1-score promedio: {results_df['value'].mean():.4f} ¬± {results_df['value'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\nüèÜ Mejores hiperpar√°metros encontrados:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "    # Crear diccionario de resultados para compatibilidad\n",
    "    optuna_results = {\n",
    "        \"results_df\": results_df,\n",
    "        \"best_params\": best_params,\n",
    "        \"study\": None  # El study se carga separadamente si es necesario\n",
    "    }\n",
    "\n",
    "    print(f\"\\n‚è≠Ô∏è  Saltando optimizaci√≥n - usando resultados previos\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron resultados previos de Optuna\")\n",
    "    print(\"   - Iniciando optimizaci√≥n desde cero\")\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è  Configuraci√≥n:\")\n",
    "    print(f\"   - Trials a ejecutar: {OPTUNA_CONFIG['n_trials']}\")\n",
    "    print(f\"   - √âpocas por trial: {OPTUNA_CONFIG['n_epochs_per_trial']}\")\n",
    "    print(f\"   - M√©trica a optimizar: {OPTUNA_CONFIG['metric']} ({OPTUNA_CONFIG['direction']})\")\n",
    "\n",
    "    print(f\"\\nüöÄ Iniciando b√∫squeda de hiperpar√°metros con Optuna...\")\n",
    "    print(\"   (Esto puede tomar varios minutos)\")\n",
    "\n",
    "    # Ejecutar optimizaci√≥n\n",
    "    optuna_results = optimize_cnn2d(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        input_shape=(1, 65, 41),  # (C, H, W)\n",
    "        n_trials=OPTUNA_CONFIG[\"n_trials\"],\n",
    "        n_epochs_per_trial=OPTUNA_CONFIG[\"n_epochs_per_trial\"],\n",
    "        device=device,\n",
    "        save_dir=str(optuna_results_dir)\n",
    "    )\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"OPTIMIZACI√ìN COMPLETADA\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88jfG5bVttP8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AN√ÅLISIS DE RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extraer resultados\n",
    "results_df = optuna_results[\"results_df\"]\n",
    "best_params = optuna_results[\"best_params\"]\n",
    "analysis = optuna_results[\"analysis\"]\n",
    "\n",
    "print(f\"üìä Resumen de la optimizaci√≥n:\")\n",
    "print(f\"   - Total configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score encontrado: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ¬± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Mejores hiperpar√°metros encontrados:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "# Mostrar top 10 configuraciones\n",
    "print(f\"\\nüìà Top 10 configuraciones:\")\n",
    "print(\"-\" * 80)\n",
    "top_10 = results_df.nlargest(10, 'f1')\n",
    "for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "    print(f\"{i:2d}. F1: {row['f1']:.4f} | \"\n",
    "          f\"Acc: {row['accuracy']:.4f} | \"\n",
    "          f\"Batch: {row['batch_size']} | \"\n",
    "          f\"LR: {row['learning_rate']} | \"\n",
    "          f\"Dropout: {row['p_drop_conv']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o888G9VGttP9"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDAR RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar DataFrame completo con todas las configuraciones\n",
    "results_csv_path = optuna_results_dir / \"optuna_scan_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"üíæ Resultados completos guardados: {results_csv_path}\")\n",
    "\n",
    "# Guardar mejores par√°metros\n",
    "best_params_path = optuna_results_dir / \"best_params.json\"\n",
    "with open(best_params_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"üíæ Mejores par√°metros guardados: {best_params_path}\")\n",
    "\n",
    "# Guardar resumen de optimizaci√≥n\n",
    "summary_path = optuna_results_dir / \"optimization_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"RESUMEN DE OPTIMIZACI√ìN OPTUNA\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Total configuraciones evaluadas: {len(results_df)}\\n\")\n",
    "    f.write(f\"Mejor F1-score: {results_df['f1'].max():.4f}\\n\")\n",
    "    f.write(f\"F1-score promedio: {results_df['f1'].mean():.4f} ¬± {results_df['f1'].std():.4f}\\n\\n\")\n",
    "    f.write(\"MEJORES HIPERPAR√ÅMETROS:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    for param, value in best_params.items():\n",
    "        if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(\"\\nTOP 5 CONFIGURACIONES:\\n\")\n",
    "    f.write(\"-\"*30 + \"\\n\")\n",
    "    top_5 = results_df.nlargest(5, 'f1')\n",
    "    for i, (idx, row) in enumerate(top_5.iterrows(), 1):\n",
    "        f.write(f\"{i}. F1: {row['f1']:.4f} | Acc: {row['accuracy']:.4f} | \"\n",
    "                f\"Batch: {row['batch_size']} | LR: {row['learning_rate']}\\n\")\n",
    "\n",
    "print(f\"üíæ Resumen guardado: {summary_path}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0dr_0HlttP-"
   },
   "outputs": [],
   "source": [
    "# Agregar m√≥dulos propios al path\n",
    "# El notebook est√° en research/, pero modules/ est√° en el directorio ra√≠z\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================\n",
    "# EVALUAR RESULTADOS DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACI√ìN DE RESULTADOS DE OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Importar evaluador\n",
    "\n",
    "# Evaluar el proceso de optimizaci√≥n\n",
    "# evaluation = check_optuna_results(str(optuna_results_dir))\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wihb557SttP-"
   },
   "source": [
    "## 5. Re-entrenamiento con Mejores Hiperpar√°metros\n",
    "\n",
    "Re-entrenar el modelo CNN2D usando los mejores hiperpar√°metros encontrados por Optuna, con early stopping para obtener el modelo final optimizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NVWEzLkttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREAR MODELO CON MEJORES HIPERPAR√ÅMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREANDO MODELO CON MEJORES HIPERPAR√ÅMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear modelo con mejores par√°metros encontrados por Optuna\n",
    "best_model = CNN2D(\n",
    "    n_classes=2,\n",
    "    p_drop_conv=best_params[\"p_drop_conv\"],\n",
    "    p_drop_fc=best_params[\"p_drop_fc\"],\n",
    "    input_shape=(65, 41),\n",
    "    filters_1=best_params[\"filters_1\"],\n",
    "    filters_2=best_params[\"filters_2\"],\n",
    "    kernel_size_1=best_params[\"kernel_size_1\"],\n",
    "    kernel_size_2=best_params[\"kernel_size_2\"],\n",
    "    dense_units=best_params[\"dense_units\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Modelo creado con mejores hiperpar√°metros:\")\n",
    "print(f\"   - Filters 1: {best_params['filters_1']}\")\n",
    "print(f\"   - Filters 2: {best_params['filters_2']}\")\n",
    "print(f\"   - Kernel 1: {best_params['kernel_size_1']}\")\n",
    "print(f\"   - Kernel 2: {best_params['kernel_size_2']}\")\n",
    "print(f\"   - Dense units: {best_params['dense_units']}\")\n",
    "print(f\"   - Dropout conv: {best_params['p_drop_conv']}\")\n",
    "print(f\"   - Dropout fc: {best_params['p_drop_fc']}\")\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print_model_summary(best_model)\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86kelbHEttP_"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURAR ENTRENAMIENTO CON MEJORES PAR√ÅMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURANDO ENTRENAMIENTO FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuraci√≥n de entrenamiento final usando configuraci√≥n centralizada\n",
    "FINAL_TRAINING_CONFIG = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,  # Reducido de 15 a 10 (recomendaci√≥n)\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": best_params[\"batch_size\"]\n",
    "}\n",
    "\n",
    "# Crear DataLoaders con el mejor batch size\n",
    "train_loader_final = DataLoader(\n",
    "    train_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader_final = DataLoader(\n",
    "    val_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader_final = DataLoader(\n",
    "    test_dataset,\n",
    "    best_params[\"batch_size\"],  # Usar batch_size de Optuna\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Optimizador SGD con momentum usando configuraci√≥n centralizada\n",
    "# CORREGIDO: Agregado nesterov=True y weight_decay=1e-4\n",
    "optimizer_final = optim.SGD(\n",
    "    best_model.parameters(),\n",
    "    lr=FINAL_TRAINING_CONFIG['learning_rate'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,  # Cambiado de 0.0 a 1e-4\n",
    "    nesterov=True  # Agregado Nesterov momentum\n",
    ")\n",
    "\n",
    "# Calcular class weights para balancear las clases usando configuraci√≥n centralizada\n",
    "if CLASS_WEIGHTS_CONFIG[\"enabled\"]:\n",
    "    class_counts = torch.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    print(f\"‚úÖ Class weights habilitados: {class_weights.tolist()}\")\n",
    "else:\n",
    "    criterion_final = nn.CrossEntropyLoss()\n",
    "    print(\"‚ö†Ô∏è  Class weights deshabilitados\")\n",
    "\n",
    "# Scheduler StepLR usando configuraci√≥n centralizada\n",
    "scheduler_final = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer_final,\n",
    "    step_size=SCHEDULER_CONFIG[\"step_size\"],\n",
    "    gamma=SCHEDULER_CONFIG[\"gamma\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuraci√≥n final:\")\n",
    "print(f\"   - Learning rate inicial: {FINAL_TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"   - Momentum: 0.9 (Nesterov: True)\")\n",
    "print(f\"   - Weight decay: 1e-4\")\n",
    "print(f\"   - Scheduler: StepLR (step={SCHEDULER_CONFIG['step_size']}, gamma={SCHEDULER_CONFIG['gamma']})\")\n",
    "print(f\"   - Batch size: {FINAL_TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   - √âpocas m√°ximas: {FINAL_TRAINING_CONFIG['n_epochs']}\")\n",
    "print(f\"   - Early stopping patience: {FINAL_TRAINING_CONFIG['early_stopping_patience']}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fg5tzkUttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENTRENAR MODELO FINAL CON EARLY STOPPING\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO MODELO FINAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Entrenar modelo con mejores hiperpar√°metros\n",
    "# CORREGIDO: Usar keyword arguments y monitorear val_f1 (mejor para datasets desbalanceados)\n",
    "final_training_results = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader_final,\n",
    "    val_loader=val_loader_final,\n",
    "    optimizer=optimizer_final,\n",
    "    criterion=criterion_final,\n",
    "    device=device,\n",
    "    n_epochs=FINAL_TRAINING_CONFIG['n_epochs'],\n",
    "    early_stopping_patience=FINAL_TRAINING_CONFIG['early_stopping_patience'],\n",
    "    save_dir=optuna_results_dir,\n",
    "    verbose=True,\n",
    "    scheduler=scheduler_final,\n",
    "    monitor_metric=\"f1\"  # Monitorear F1 en lugar de loss (recomendado para desbalance)\n",
    ")\n",
    "\n",
    "# Extraer resultados\n",
    "final_model = final_training_results[\"model\"]\n",
    "final_history = final_training_results[\"history\"]\n",
    "final_best_val_loss = final_training_results[\"best_val_loss\"]\n",
    "final_total_time = final_training_results[\"total_time\"]\n",
    "\n",
    "# Calcular mejor √©poca\n",
    "final_best_epoch = final_history[\"val_loss\"].index(min(final_history[\"val_loss\"])) + 1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENAMIENTO FINAL COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Resultados:\")\n",
    "print(f\"   - Mejor √©poca: {final_best_epoch}\")\n",
    "print(f\"   - Mejor val loss: {final_best_val_loss:.4f}\")\n",
    "print(f\"   - Tiempo total: {final_total_time/60:.1f} minutos\")\n",
    "print(f\"   - Modelo guardado en: {optuna_results_dir / 'best_model_optuna.pth'}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43bNxSHMttQA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUACI√ìN FINAL EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACI√ìN FINAL EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluar modelo final en test set\n",
    "final_test_metrics = detailed_evaluation(\n",
    "    model=final_model,\n",
    "    loader=test_loader_final,\n",
    "    device=device,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"]\n",
    ")\n",
    "\n",
    "# Imprimir reporte\n",
    "print_evaluation_report(final_test_metrics, class_names=[\"Healthy\", \"Parkinson\"])\n",
    "\n",
    "# Guardar m√©tricas finales\n",
    "final_metrics_path = optuna_results_dir / \"test_metrics_optuna.json\"\n",
    "\n",
    "# Extraer m√©tricas del classification_report\n",
    "final_report = final_test_metrics[\"classification_report\"]\n",
    "final_metrics_to_save = {\n",
    "    \"accuracy\": float(final_test_metrics[\"accuracy\"]),\n",
    "    \"f1_macro\": float(final_test_metrics[\"f1_macro\"]),\n",
    "    \"precision_macro\": float(final_report[\"macro avg\"][\"precision\"]),\n",
    "    \"recall_macro\": float(final_report[\"macro avg\"][\"recall\"]),\n",
    "    \"f1_weighted\": float(final_report[\"weighted avg\"][\"f1-score\"]),\n",
    "    \"confusion_matrix\": final_test_metrics[\"confusion_matrix\"].tolist(),\n",
    "    \"classification_report\": final_report,\n",
    "    \"best_hyperparameters\": best_params,\n",
    "    \"training_config\": FINAL_TRAINING_CONFIG,\n",
    "    \"final_epoch\": final_best_epoch,\n",
    "    \"final_val_loss\": final_best_val_loss,\n",
    "    \"training_time_minutes\": final_total_time / 60\n",
    "}\n",
    "\n",
    "with open(final_metrics_path, \"w\") as f:\n",
    "    json.dump(final_metrics_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ M√©tricas finales guardadas en: {final_metrics_path}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28JxR5lgttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACI√ìN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERANDO VISUALIZACIONES FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Graficar progreso del entrenamiento final\n",
    "final_progress_fig = plot_training_history(\n",
    "    final_history,\n",
    "    save_path=optuna_results_dir / \"training_progress_optuna.png\"\n",
    ")\n",
    "\n",
    "# Matriz de confusi√≥n final\n",
    "final_cm = final_test_metrics[\"confusion_matrix\"]\n",
    "final_cm_fig = plot_confusion_matrix(\n",
    "    final_cm,\n",
    "    class_names=[\"Healthy\", \"Parkinson\"],\n",
    "    title=\"Matriz de Confusi√≥n - Test Set (CNN2D Optimizado con Optuna)\",\n",
    "    save_path=optuna_results_dir / \"confusion_matrix_optuna.png\",\n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(f\"üíæ Visualizaciones guardadas:\")\n",
    "print(f\"   - Progreso de entrenamiento: {optuna_results_dir / 'training_progress_optuna.png'}\")\n",
    "print(f\"   - Matriz de confusi√≥n: {optuna_results_dir / 'confusion_matrix_optuna.png'}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQP-xbe3ttQB"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUMEN FINAL DE OPTIMIZACI√ìN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESUMEN FINAL DE OPTIMIZACI√ìN CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüîç PROCESO DE OPTIMIZACI√ìN:\")\n",
    "print(f\"   - Configuraciones evaluadas: {len(results_df)}\")\n",
    "print(f\"   - Mejor F1-score en validaci√≥n: {results_df['f1'].max():.4f}\")\n",
    "print(f\"   - F1-score promedio: {results_df['f1'].mean():.4f} ¬± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\")\n",
    "for param, value in best_params.items():\n",
    "    if param not in ['f1', 'accuracy', 'precision', 'recall', 'val_loss', 'train_loss']:\n",
    "        print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS FINALES EN TEST SET:\")\n",
    "final_report = final_test_metrics[\"classification_report\"]\n",
    "print(f\"   - Accuracy:  {final_test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {final_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {final_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {final_test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ ARCHIVOS GUARDADOS EN {optuna_results_dir}:\")\n",
    "print(f\"   - optuna_scan_results.csv          # Todas las configuraciones probadas\")\n",
    "print(f\"   - best_params.json                # Mejores hiperpar√°metros\")\n",
    "print(f\"   - optimization_summary.txt        # Resumen de optimizaci√≥n\")\n",
    "print(f\"   - best_model_optuna.pth           # Modelo final optimizado\")\n",
    "print(f\"   - test_metrics_optuna.json        # M√©tricas en test set\")\n",
    "print(f\"   - training_progress_optuna.png    # Gr√°fica de entrenamiento\")\n",
    "print(f\"   - confusion_matrix_optuna.png   # Matriz de confusi√≥n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZACI√ìN CON OPTUNA COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxUMjAU2ttQF"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5glXtwxNttQG"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stg5s-gEttQH"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3DWVC1JttQI"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "parkinson_env",
   "language": "python",
   "name": "parkinson_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
