# ============================================================
# CONFIGURAR ENTRENAMIENTO CON HIPERPAR√ÅMETROS SELECCIONADOS
# ============================================================

print("=" * 70)
print("CONFIGURANDO ENTRENAMIENTO CON HIPERPAR√ÅMETROS SELECCIONADOS")
print("=" * 70)

# Verificar que las variables est√©n definidas
if "BEST_PARAMS" not in globals():
    print("‚ùå Error: BEST_PARAMS no est√° definido.")
    print("   Ejecuta primero la celda del selector de hiperpar√°metros.")
    print("=" * 70)
else:
    print(f"‚úÖ Configurando entrenamiento con par√°metros de: {HYPERPARAMETER_SOURCE}")

    # Configuraci√≥n de entrenamiento usando los par√°metros seleccionados
    FINAL_TRAINING_CONFIG = {
        "n_epochs": BEST_PARAMS["n_epochs"],
        "early_stopping_patience": BEST_PARAMS["early_stopping_patience"],
        "learning_rate": BEST_PARAMS["learning_rate"],
        "batch_size": BEST_PARAMS["batch_size"],
    }

    # Crear DataLoaders con el batch size seleccionado
    train_loader_final = DataLoader(
        train_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=True,
        num_workers=0,
    )
    val_loader_final = DataLoader(
        val_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=False,
        num_workers=0,
    )
    test_loader_final = DataLoader(
        test_dataset,
        BEST_PARAMS["batch_size"],  # Usar batch_size seleccionado
        shuffle=False,
        num_workers=0,
    )

    # Optimizador SGD con momentum usando los par√°metros seleccionados
    optimizer_final = optim.SGD(
        best_model.parameters(),
        lr=FINAL_TRAINING_CONFIG["learning_rate"],
        momentum=BEST_PARAMS["momentum"],
        weight_decay=BEST_PARAMS["weight_decay"],
        nesterov=True,  # Mejora sobre Ibarra
    )

    # Calcular class weights para balancear las clases
    if CLASS_WEIGHTS_CONFIG["enabled"]:
        class_counts = torch.bincount(y_train)
        class_weights = 1.0 / class_counts.float()
        class_weights = class_weights / class_weights.sum()
        criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))
        print(f"‚úÖ Class weights habilitados: {class_weights.tolist()}")
    else:
        criterion_final = nn.CrossEntropyLoss()
        print("‚ö†Ô∏è  Class weights deshabilitados")

    # Scheduler StepLR usando los par√°metros seleccionados
    scheduler_final = torch.optim.lr_scheduler.StepLR(
        optimizer_final, step_size=BEST_PARAMS["step_size"], gamma=BEST_PARAMS["gamma"]
    )

    print(f"\n‚öôÔ∏è  Configuraci√≥n final ({HYPERPARAMETER_SOURCE}):")
    print(f"   - Learning rate inicial: {FINAL_TRAINING_CONFIG['learning_rate']}")
    print(f"   - Momentum: {BEST_PARAMS['momentum']} (Nesterov: True)")
    print(f"   - Weight decay: {BEST_PARAMS['weight_decay']}")
    print(
        f"   - Scheduler: StepLR (step={BEST_PARAMS['step_size']}, gamma={BEST_PARAMS['gamma']})"
    )
    print(f"   - Batch size: {FINAL_TRAINING_CONFIG['batch_size']}")
    print(f"   - √âpocas m√°ximas: {FINAL_TRAINING_CONFIG['n_epochs']}")
    print(
        f"   - Early stopping patience: {FINAL_TRAINING_CONFIG['early_stopping_patience']}"
    )

    # Mostrar diferencias con Ibarra si estamos usando Optuna
    if not USE_IBARRA:
        print(f"\nüìä DIFERENCIAS CON IBARRA:")
        print(f"   - Ibarra batch_size: 64 ‚Üí Optuna: {BEST_PARAMS['batch_size']}")
        print(f"   - Ibarra kernel_1: 6 ‚Üí Optuna: {BEST_PARAMS['kernel_size_1']}")
        print(f"   - Ibarra filters_2: 64 ‚Üí Optuna: {BEST_PARAMS['filters_2']}")
        print(f"   - Ibarra dense_units: 32 ‚Üí Optuna: {BEST_PARAMS['dense_units']}")

    print("=" * 70)
