#  Research - Investigaciones Doctorado

Esta carpeta contiene notebooks de investigaci贸n personal para el doctorado en detecci贸n de Parkinson mediante an谩lisis de voz.

## 锔 Nota Importante

**Los notebooks en esta carpeta NO deben ser revisados por el profesor.** Son investigaciones personales y experimentos adicionales que no forman parte del entregable principal.

##  Contenido

### Notebooks de Investigaci贸n

#### 1. `cnn_training.ipynb`
- **Prop贸sito**: CNN2D baseline con augmentation
- **Arquitectura**: Single-head CNN sin Domain Adaptation
- **Augmentation**: Pitch shifting, time stretching, noise injection, SpecAugment
- **Uso**: Modelo baseline para comparaci贸n

#### 2. `cnn_da_training.ipynb`
- **Prop贸sito**: CNN2D con Domain Adaptation
- **Arquitectura**: Dual-head CNN con Gradient Reversal Layer (GRL)
- **Referencia**: Implementaci贸n exacta seg煤n Ibarra et al. (2023)
- **Uso**: Modelo con adaptaci贸n de dominio

#### 3. `cnn1d_da_training.ipynb`
- **Prop贸sito**: CNN1D con Domain Adaptation y atenci贸n temporal
- **Arquitectura**: CNN1D + atenci贸n temporal + GRL
- **Uso**: Modelado temporal de caracter铆sticas

#### 4. `lstm_da_training.ipynb`
- **Prop贸sito**: Time-CNN-BiLSTM con Domain Adaptation
- **Arquitectura**: Time-distributed CNN + BiLSTM + GRL
- **Uso**: Modelado temporal de secuencias completas

##  Objetivos de Investigaci贸n

### Comparaci贸n de Arquitecturas
- **CNN2D**: Baseline sin Domain Adaptation
- **CNN2D_DA**: Con Domain Adaptation
- **CNN1D_DA**: Modelado temporal con atenci贸n
- **LSTM_DA**: Modelado temporal de secuencias

### T茅cnicas Implementadas
- **Domain Adaptation**: Gradient Reversal Layer (GRL)
- **Data Augmentation**: SpecAugment, pitch shifting, time stretching
- **Attention Mechanisms**: Temporal attention en CNN1D
- **Sequence Modeling**: BiLSTM para dependencias temporales

### M茅tricas de Evaluaci贸n
- **Accuracy**: Precisi贸n de clasificaci贸n
- **F1-Score**: Balance entre precisi贸n y recall
- **Confusion Matrix**: An谩lisis de errores
- **t-SNE**: Visualizaci贸n de embeddings

##  Experimentos Realizados

### 1. Comparaci贸n de Modelos
- CNN2D vs CNN2D_DA
- CNN1D vs CNN1D_DA
- CNN vs LSTM para modelado temporal

### 2. An谩lisis de Domain Adaptation
- Efectividad del GRL
- Lambda warm-up
- Multi-task learning

### 3. An谩lisis Temporal
- Secuencias de diferentes longitudes
- Attention weights
- Dependencias temporales

##  Resultados de Investigaci贸n

### Modelos Entrenados
- **CNN2D**: ~98.8% accuracy
- **CNN2D_DA**: TBD
- **CNN1D_DA**: TBD
- **LSTM_DA**: TBD

### An谩lisis de Incertidumbre
- **Epist茅mica**: MC Dropout
- **Aleatoria**: Heteroscedastic loss
- **GradCAM**: Visualizaci贸n de explicabilidad

##  Uso de los Notebooks

### Prerequisitos
1. Ejecutar `notebooks/data_preprocessing.ipynb` primero
2. Tener cache generado en `cache/original/`

### Orden de Ejecuci贸n
1. **CNN2D**: `cnn_training.ipynb`
2. **CNN2D_DA**: `cnn_da_training.ipynb`
3. **CNN1D_DA**: `cnn1d_da_training.ipynb`
4. **LSTM_DA**: `lstm_da_training.ipynb`

### Configuraci贸n
- **GPU**: Recomendado para entrenamiento
- **Memoria**: Al menos 8GB RAM
- **Tiempo**: 10-20 minutos por modelo

##  Referencias

### Papers Principales
- **Ibarra et al. (2023)**: "Towards a Corpus (and Language)-Independent Screening of Parkinson's Disease from Voice and Speech through Domain Adaptation"
- **Selvaraju et al. (2017)**: "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"
- **Kendall & Gal (2017)**: "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"

### T茅cnicas Implementadas
- **Domain Adaptation**: GRL para invarianza multi-corpus
- **Data Augmentation**: SpecAugment para robustez
- **Uncertainty Quantification**: MC Dropout + Heteroscedastic loss
- **Explainability**: GradCAM para interpretabilidad

##  Configuraci贸n T茅cnica

### Hiperpar谩metros
- **Learning Rate**: 0.001 (Adam) / 0.1 (SGD)
- **Batch Size**: 32
- **Epochs**: 100
- **Dropout**: 0.3 (conv) / 0.5 (fc)

### Arquitecturas
- **CNN2D**: 2 bloques Conv2D + MaxPool(33)
- **CNN1D**: 3 bloques Conv1D + atenci贸n temporal
- **LSTM**: Time-distributed CNN + BiLSTM

##  An谩lisis de Resultados

### M茅tricas por Modelo
- **Accuracy**: Clasificaci贸n correcta
- **F1-Score**: Balance precisi贸n/recall
- **Confusion Matrix**: An谩lisis de errores
- **ROC Curve**: Curva ROC

### Visualizaciones
- **Training Curves**: Loss y accuracy
- **Attention Maps**: Pesos de atenci贸n
- **GradCAM**: Mapas de activaci贸n
- **t-SNE**: Embeddings 2D

##  Contribuciones de Investigaci贸n

### Nuevas T茅cnicas
- **Temporal Attention**: En CNN1D para modelado temporal
- **Sequence Modeling**: BiLSTM para dependencias temporales
- **Uncertainty Quantification**: MC Dropout + Heteroscedastic loss

### Mejoras Implementadas
- **Modular Design**: Componentes reutilizables
- **Efficient Training**: K-fold cross-validation
- **Robust Evaluation**: Speaker-independent splits

##  Notas de Desarrollo

### Versiones
- **v1.0**: Implementaci贸n inicial CNN2D
- **v2.0**: Domain Adaptation (GRL)
- **v3.0**: CNN1D con atenci贸n temporal
- **v4.0**: LSTM con modelado temporal

### Pr贸ximos Pasos
- **Ensemble Methods**: Combinaci贸n de modelos
- **Transfer Learning**: Pre-trained models
- **Multi-modal**: Audio + texto + metadata

---

**Autor**: PHD Research Team  
**Fecha**: 2025-01-21  
**Versi贸n**: 4.0 (LSTM + Uncertainty)  
**Estado**: Investigaci贸n en progreso
