# Validaci√≥n de Configuraci√≥n de Entrenamiento

## Comparaci√≥n: Configuraci√≥n Actual vs. Recomendaciones

### ‚úÖ 1. M√©trica a vigilar: val_f1_macro o val_balanced_accuracy

**‚ùå PROBLEMA ACTUAL:**
- La funci√≥n `train_model()` usa **solo `val_loss`** para early stopping y guardar el mejor modelo
- L√≠neas 199-202 en `modules/models/cnn2d/training.py`:
  ```python
  early_stopping = EarlyStopping(
      patience=early_stopping_patience,
      mode="min",  # Minimizar val_loss
  )
  ```
- L√≠neas 252-254: Guarda el modelo bas√°ndose en `val_loss`:
  ```python
  if val_metrics["loss"] < best_val_loss:
      best_val_loss = val_metrics["loss"]
      best_model_state = model.state_dict().copy()
  ```

**‚úÖ RECOMENDACI√ìN:**
- Cambiar a `val_f1_macro` o `val_balanced_accuracy` como m√©trica principal
- En datasets desbalanceados (65.8% PD vs 34.2% HC), `val_loss` puede ser enga√±oso

**ACCI√ìN NECESARIA:** Modificar `train_model()` para usar `val_f1` como m√©trica de monitoreo.

---

### ‚ö†Ô∏è 2. EarlyStopping: patience=8-10, restore_best_weights=True

**ESTADO ACTUAL:**
- **Patience**: 15 √©pocas (en `TRAINING_CONFIG`)
  ```python
  TRAINING_CONFIG = {
      "n_epochs": 100,
      "early_stopping_patience": 15,  # ‚ö†Ô∏è Deber√≠a ser 8-10
      ...
  }
  ```

**‚úÖ IMPLEMENTACI√ìN:**
- Ya tiene `restore_best_weights=True` impl√≠citamente (l√≠neas 254-255 guardan best_model_state)
- L√≠nea 302 restaura el mejor modelo:
  ```python
  model.load_state_dict(best_model_state)
  ```

**ACCI√ìN NECESARIA:** Reducir patience de 15 a 8-10.

---

### ‚úÖ 3. √âpocas m√°ximas: 60-100

**ESTADO ACTUAL:**
- ‚úÖ **100 √©pocas** configuradas correctamente
  ```python
  TRAINING_CONFIG = {
      "n_epochs": 100,  # ‚úÖ OK
      ...
  }
  ```

**ACCI√ìN:** Ninguna, est√° correcto.

---

### ‚ùå 4. SGD: momentum=0.9, nesterov=True, weight_decay=1e-4

**PROBLEMA ACTUAL:**
- **Configuraci√≥n incompleta**:
  ```python
  OPTIMIZER_CONFIG = {
      "type": "SGD",
      "learning_rate": 0.1,
      "momentum": 0.9,        # ‚úÖ OK
      "weight_decay": 0.0     # ‚ùå Deber√≠a ser 1e-4
  }
  ```
- ‚ùå **Falta `nesterov=True`**
- ‚ùå **`weight_decay=0.0`** deber√≠a ser `1e-4`

**IMPLEMENTACI√ìN ACTUAL (celda 26 del notebook):**
```python
optimizer_final = optim.SGD(
    best_model.parameters(),
    0.1,  # learning_rate
    0.9,  # momentum
    0.0,  # weight_decay  ‚ùå Deber√≠a ser 1e-4
)
# ‚ùå Falta nesterov=True
```

**ACCI√ìN NECESARIA:** 
1. Agregar `nesterov=True`
2. Cambiar `weight_decay` de `0.0` a `1e-4`

---

### ‚úÖ 5. Class weights: activos

**ESTADO ACTUAL:**
- ‚úÖ **Habilitado** en la configuraci√≥n:
  ```python
  CLASS_WEIGHTS_CONFIG = {
      "enabled": True,
      "method": "inverse_frequency"
  }
  ```

- ‚úÖ **Implementado** en celda 26:
  ```python
  class_counts = torch.bincount(y_train)
  class_weights = 1.0 / class_counts.float()
  class_weights = class_weights / class_weights.sum()
  criterion_final = nn.CrossEntropyLoss(weight=class_weights.to(device))
  ```

**ACCI√ìN:** Ninguna, est√° correcto.

---

### ‚ùå 6. Checkpoint por fold: mejor epoch por fold + promedio de 10 folds

**PROBLEMA ACTUAL:**
- ‚ùå El notebook **NO implementa 10-fold CV completo**
- Solo usa **1 fold** del K-fold para entrenamiento:
  ```python
  # Para este notebook, usaremos el primer fold como ejemplo
  # En el paper real se promedian los resultados de los 10 folds
  train_indices = fold_splits[0]["train"]  # ‚ùå Solo usa fold 0
  val_indices = fold_splits[0]["val"]
  ```

**RECOMENDACI√ìN:**
- Implementar loop completo de 10 folds
- Guardar mejor modelo de cada fold
- Reportar m√©tricas promedio ¬± desviaci√≥n est√°ndar de los 10 folds

**C√ìDIGO NECESARIO:**
```python
# Iterar sobre los 10 folds
for fold_idx in range(10):
    print(f"\n{'='*70}")
    print(f"FOLD {fold_idx + 1}/10")
    print(f"{'='*70}")
    
    # Obtener √≠ndices del fold
    train_indices = fold_splits[fold_idx]["train"]
    val_indices = fold_splits[fold_idx]["val"]
    
    # Entrenar modelo para este fold
    # Guardar mejor modelo del fold
    # Evaluar en validation del fold
    
# Calcular m√©tricas promedio de todos los folds
mean_f1 = np.mean([fold['f1'] for fold in fold_results])
std_f1 = np.std([fold['f1'] for fold in fold_results])
print(f"F1-Score: {mean_f1:.4f} ¬± {std_f1:.4f}")
```

**NOTA:** Ya existe `train_model_da_kfold()` en `training.py` que implementa esto correctamente.

---

## üìã Resumen de Acciones Necesarias

### Cr√≠ticas (‚ùå):
1. **Cambiar m√©trica de early stopping** de `val_loss` a `val_f1_macro`
2. **Agregar `nesterov=True`** al optimizador SGD
3. **Cambiar `weight_decay`** de `0.0` a `1e-4`
4. **Implementar 10-fold CV completo** (actualmente solo usa 1 fold)

### Menores (‚ö†Ô∏è):
5. **Reducir patience** de 15 a 8-10 √©pocas

### Correctas (‚úÖ):
- ‚úÖ √âpocas m√°ximas: 100
- ‚úÖ SGD momentum: 0.9
- ‚úÖ Class weights: habilitado
- ‚úÖ Restore best weights: implementado

---

## üîß Archivos a Modificar

1. **`research/cnn_training.ipynb`**:
   - Celda 3: Actualizar `OPTIMIZER_CONFIG` y `TRAINING_CONFIG`
   - Celda 26: Agregar `nesterov=True` y `weight_decay=1e-4` al optimizador
   - Agregar loop de 10-fold CV completo

2. **`modules/models/cnn2d/training.py`**:
   - Modificar `train_model()` para usar `val_f1` en lugar de `val_loss` para early stopping
   - Agregar par√°metro `monitor_metric` (default: "f1")

3. **`modules/core/cnn2d_optuna_wrapper.py`**:
   - Verificar que Optuna tambi√©n use la m√©trica correcta

---

## üìä Impacto Esperado

- **M√©trica correcta**: Mejor selecci√≥n de modelo en datasets desbalanceados
- **Nesterov momentum**: Convergencia m√°s r√°pida y estable
- **Weight decay**: Mejor regularizaci√≥n, reduce overfitting
- **10-fold CV**: Estimaci√≥n m√°s robusta del rendimiento real del modelo
- **Patience reducida**: Ahorra tiempo de entrenamiento sin sacrificar rendimiento

