# Pruebas Unitarias del Proyecto

Este directorio contiene las pruebas unitarias para verificar la implementaciÃ³n correcta de los modelos.

## ğŸ“‹ Pruebas Disponibles

### `test_lstm_da_implementation.py` (NUEVO)

Pruebas completas para el modelo Time-CNN-BiLSTM-DA.

**Ejecutar:**
```bash
python test/test_lstm_da_implementation.py
```

**Cobertura (14 tests):**

#### 1. TestSequenceGeneration (3 tests)
- âœ… Verifica forma correcta de secuencias `(n_frames, 1, 65, 41)`
- âœ… Verifica que padding contiene ceros
- âœ… Verifica filtrado de secuencias con muy pocos frames

#### 2. TestLSTMModel (4 tests)
- âœ… Verifica dimensiones de outputs del modelo
- âœ… Verifica forward con masking (secuencias de longitud variable)
- âœ… Verifica retorno de embeddings para visualizaciÃ³n
- âœ… Verifica que forward es determinÃ­stico en modo eval

#### 3. TestGradientReversal (3 tests)
- âœ… Verifica que GRL no modifica valores en forward
- âœ… Verifica que GRL invierte gradientes en backward
- âœ… Verifica actualizaciÃ³n de lambda

#### 4. TestBiLSTMTemporal (2 tests)
- âœ… Verifica procesamiento correcto de secuencias temporales
- âœ… Verifica efecto del masking en resultados

#### 5. TestIntegration (1 test)
- âœ… Pipeline completo: dataset â†’ secuencias â†’ predicciones

#### 6. TestParameterCount (1 test)
- âœ… Verifica nÃºmero razonable de parÃ¡metros (~1.67M)

**Resultado:** âœ… 14/14 tests pasando

## ğŸ”§ Otras Pruebas del Proyecto

### `test_uncertainty_math.py`
Verifica la implementaciÃ³n matemÃ¡tica del modelo de incertidumbre.

### `test_grl_completo.py`
Verifica la implementaciÃ³n del Gradient Reversal Layer.

### `test_yarin_gal_implementation.py`
Verifica implementaciÃ³n de MC Dropout segÃºn Yarin Gal.

### `test_cnn1d_implementation.py`
Verifica implementaciÃ³n del modelo CNN1D-DA.

## ğŸ“Š CÃ³mo Ejecutar Todas las Pruebas

```bash
# Ejecutar todas las pruebas del proyecto
python -m unittest discover test

# Ejecutar una prueba especÃ­fica
python test/test_lstm_da_implementation.py

# Ejecutar con verbose
python test/test_lstm_da_implementation.py -v
```

## âœ… Validaciones Realizadas

Las pruebas verifican:

1. **Arquitectura Correcta**
   - Dimensiones de tensores en cada capa
   - Conectividad entre componentes
   - NÃºmero de parÃ¡metros

2. **MatemÃ¡tica Correcta**
   - Forward pass determinÃ­stico
   - Backward pass con gradientes invertidos (GRL)
   - Masking en LSTM funcional

3. **Procesamiento de Secuencias**
   - Zero-padding correcto
   - AgrupaciÃ³n por archivo de audio
   - Filtrado de secuencias muy cortas

4. **IntegraciÃ³n End-to-End**
   - Pipeline completo funcional
   - No hay NaNs en outputs
   - Probabilidades suman 1.0

## ğŸ¯ Buenas PrÃ¡cticas Aplicadas

- âœ… Tests aislados e independientes
- âœ… Fixtures con setUp() para datos de prueba
- âœ… Nombres descriptivos de tests
- âœ… Assertions claras y especÃ­ficas
- âœ… Cobertura de casos edge (secuencias cortas, padding, etc.)
- âœ… Tests de integraciÃ³n ademÃ¡s de unitarios

## ğŸ“ Agregar Nuevas Pruebas

Para agregar nuevas pruebas:

1. Crear clase que herede de `unittest.TestCase`
2. Implementar `setUp()` si necesitas fixtures
3. Nombrar tests como `test_descripcion_clara()`
4. Usar `self.assertEqual()`, `self.assertTrue()`, etc.
5. Agregar a `run_tests()` en el archivo principal

Ejemplo:
```python
class TestNuevaFuncionalidad(unittest.TestCase):
    def setUp(self):
        self.model = MiModelo()
    
    def test_nueva_funcionalidad(self):
        """Test: DescripciÃ³n clara de quÃ© verifica."""
        result = self.model.forward(datos_prueba)
        self.assertEqual(result.shape, (4, 2))
```

