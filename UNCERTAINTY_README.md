# ğŸ§  Sistema de EstimaciÃ³n de Incertidumbre para CNN

## ğŸ“‹ Resumen

Este sistema implementa estimaciÃ³n de incertidumbre **epistÃ©mica** y **aleatoria** en un modelo CNN para clasificaciÃ³n de Parkinson vs Healthy usando seÃ±ales de voz.

### ğŸ¯ Tipos de Incertidumbre

1. **EpistÃ©mica (modelo)**: 
   - Capturada con **MC Dropout**
   - Representa incertidumbre en los parÃ¡metros del modelo
   - **Reducible** con mÃ¡s datos de entrenamiento
   
2. **Aleatoria (datos)**:
   - Capturada con **cabeza de varianza** (heteroscedÃ¡stica)
   - Representa ruido inherente en los datos
   - **Irreducible** (ruido intrÃ­nseco)

---

## ğŸ—ï¸ Arquitectura del Modelo

### Backbone (Feature Extractor)
```
Input [B, 1, 65, 41]
    â†“
Bloque 1: Conv2D(32) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ MCDropout(0.25)
    â†“
Bloque 2: Conv2D(64) â†’ BN â†’ ReLU â†’ MaxPool(3Ã—3) â†’ MCDropout(0.25)
    â†“
AdaptiveAvgPool â†’ Flatten
    â†“
Features [B, feat_size]
```

### Cabeza A: PredicciÃ³n
```
Features â†’ Linear(64) â†’ ReLU â†’ MCDropout(0.25) â†’ Linear(C)
    â†“
logits [B, C]
```

### Cabeza B: Ruido de Datos
```
Features â†’ Linear(64) â†’ ReLU â†’ MCDropout(0.25) â†’ Linear(C)
    â†“
s_logit = clamp(x, -10, 3) [B, C]  # log-varianza
```

**Nota clave**: Se usa `MCDropout` que permanece activo incluso en `eval()` para hacer MC Dropout en inferencia.

---

## ğŸ”¬ Entrenamiento

### PÃ©rdida HeteroscedÃ¡stica

En lugar de Cross-Entropy estÃ¡ndar, usamos log-likelihood con ruido gaussiano:

```python
Ïƒ = exp(0.5 * s_logit)  # DesviaciÃ³n estÃ¡ndar por clase

# Para T_noise muestras:
for t in range(T_noise):
    Îµ ~ N(0, 1)
    xÌ‚_t = logits + Ïƒ âŠ™ Îµ  # Logits con ruido
    logp_t = log_softmax(xÌ‚_t)[y]  # Log-prob de clase correcta

# Log-mean-exp estable:
m = max_t logp_y_t
loss = -mean_batch( m + log(mean_t exp(logp_y_t - m)) )
```

**Ventajas**:
- El modelo aprende ÏƒÂ² Ãºtil para cada clase
- MÃ¡s robusto a ruido en etiquetas
- Mejor calibraciÃ³n

### HiperparÃ¡metros de Entrenamiento

```python
N_EPOCHS = 60
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 1e-4
OPTIMIZER = AdamW
T_NOISE = 5  # Muestras de ruido en training
DROPOUT_P = 0.25
S_CLAMP = [-10.0, 3.0]
BATCH_SIZE = 128
EARLY_STOPPING = 15 Ã©pocas
```

---

## ğŸ”® Inferencia con MC Dropout

### Procedimiento

```python
model.eval()  # Pero MCDropout sigue activo

# T_test pasadas (30-50)
for t in range(T_test):
    logits_t, s_logit_t = model(x)
    p_t = softmax(logits_t)
    ÏƒÂ²_t = exp(s_logit_t)
    
    guardar(p_t, ÏƒÂ²_t)

# AgregaciÃ³n
pÌ„ = mean_t(p_t)  # Probabilidades promedio
pred = argmax(pÌ„)   # PredicciÃ³n final
```

### CÃ¡lculo de Incertidumbres

#### 1. EntropÃ­a Total (Predictiva)
```python
H(pÌ„) = -Î£ pÌ„_c log(pÌ„_c)
```
Mide incertidumbre total en la predicciÃ³n.

#### 2. EpistÃ©mica (BALD)
```python
BALD = H(pÌ„) - mean_t(H(p_t))
```
- `H(pÌ„)`: EntropÃ­a del promedio
- `mean H(p_t)`: Promedio de entropÃ­as individuales
- **InterpretaciÃ³n**: Desacuerdo entre diferentes "versiones" del modelo (MC Dropout)

#### 3. Aleatoria
```python
ÏƒÂ²_aleatoric = mean_t( mean_c(ÏƒÂ²_t_c) )
```
Promedio de la varianza estimada por la cabeza B.

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### ClasificaciÃ³n
- **Accuracy**: % aciertos
- **Precision, Recall, F1**: Por clase y macro-promedio

### CalibraciÃ³n
- **NLL** (Negative Log-Likelihood): `- mean log(pÌ„[y])`
- **Brier Score**: `mean((pÌ„ - y_one_hot)Â²)`
- **ECE** (Expected Calibration Error): Diferencia entre confianza y accuracy por bins

### Incertidumbre
- **H_total**: EntropÃ­a promedio
- **EpistÃ©mica**: BALD promedio
- **Aleatoria**: ÏƒÂ² promedio
- **SeparaciÃ³n correcto/incorrecto**: Â¿Los errores tienen mayor incertidumbre?

---

## ğŸ“ˆ Visualizaciones

### 1. Histogramas de Incertidumbres
```
[EntropÃ­a]     [EpistÃ©mica]    [Aleatoria]
Correcto âœ…    Correcto âœ…     Correcto âœ…
Incorrecto âŒ  Incorrecto âŒ   Incorrecto âŒ
```
**Esperado**: Los incorrectos tienen **mayor** incertidumbre.

### 2. Reliability Diagram
```
Accuracy vs Confidence
Perfecto: diagonal 45Â°
```
Mide si el modelo estÃ¡ bien calibrado (confianza = accuracy real).

### 3. Scatter EpistÃ©mica vs Aleatoria
```
Y: Aleatoria
X: EpistÃ©mica
Color: Correcto (verde) / Incorrecto (rojo)
```
Permite ver si errores se deben a modelo o datos.

### 4. Matriz de ConfusiÃ³n
ConfusiÃ³n tradicional HC vs PD.

---

## ğŸš€ Uso del Sistema

### 1. Pre-requisito
```bash
# Ejecutar primero el notebook de preprocesamiento
jupyter notebook data_preprocessing.ipynb
```

### 2. Entrenar modelo con incertidumbre
```bash
jupyter notebook cnn_uncertainty_training.ipynb
```

### 3. Resultados guardados en `results/cnn_uncertainty/`:
```
best_model_uncertainty.pth
test_metrics_uncertainty.json
training_history.png
uncertainty_histograms.png
reliability_diagram.png
uncertainty_scatter.png
confusion_matrix_test.png
```

---

## ğŸ“ Estructura de Archivos

```
modules/
â”œâ”€â”€ uncertainty_model.py           # Modelo UncertaintyCNN + MCDropout
â”œâ”€â”€ uncertainty_loss.py            # PÃ©rdida heteroscedÃ¡stica
â”œâ”€â”€ uncertainty_training.py        # Train/eval con MC Dropout
â””â”€â”€ uncertainty_visualization.py   # Plots de incertidumbres

cnn_uncertainty_training.ipynb     # Notebook principal
UNCERTAINTY_README.md              # Este archivo
```

---

## ğŸ”¬ InterpretaciÃ³n de Resultados

### Caso Ideal

| MÃ©trica | Valor Esperado | Significado |
|---------|----------------|-------------|
| H(correctos) | **Bajo** | Predicciones seguras en aciertos |
| H(incorrectos) | **Alto** | Modelo "sabe que no sabe" |
| EpistÃ©mica alta | âœ **MÃ¡s datos** | Modelo incierto |
| Aleatoria alta | âœ **Datos ruidosos** | LÃ­mite intrÃ­nseco |
| ECE | **< 0.05** | Bien calibrado |

### Ejemplo de Salida

```
ğŸ“Š MÃ‰TRICAS DE CLASIFICACIÃ“N:
  Accuracy:  0.9850
  F1-Score:  0.9845

ğŸ“ˆ CALIBRACIÃ“N:
  NLL:  0.0523
  ECE:  0.0234  âœ… Bien calibrado

ğŸ² INCERTIDUMBRES:
  Total:      0.1234
  EpistÃ©mica: 0.0456  (reducible con mÃ¡s datos)
  Aleatoria:  0.0778  (ruido intrÃ­nseco)

âœ… âŒ SEPARACIÃ“N:
  H(correctos):   0.0892  âœ… Baja
  H(incorrectos): 0.4567  âœ… Alta (modelo detecta errores)
```

---

## ğŸ¯ Ventajas del Sistema

1. **Cuantifica confianza**: No solo predicciÃ³n, tambiÃ©n incertidumbre
2. **Detecta errores**: Modelo "sabe cuando no sabe"
3. **DiagnÃ³stico**: Separa problemas de modelo vs datos
4. **Mejor calibraciÃ³n**: PÃ©rdida heteroscedÃ¡stica mejora confianza
5. **OOD Detection**: Alto H(pÌ„) indica muestra fuera de distribuciÃ³n

---

## ğŸ’¡ Trucos y Gotchas

### âš ï¸ Estabilidad NumÃ©rica

```python
# âœ… BIEN
Ïƒ = exp(0.5 * s_logit)  # DesviaciÃ³n estÃ¡ndar

# âŒ MAL
Ïƒ = exp(s_logit)  # Explota numÃ©ricamente

# âœ… Clamp de s_logit
s = clamp(s, min=-10, max=3)
```

### âš ï¸ MC Dropout

```python
# âœ… BIEN: Usar MCDropout personalizado
class MCDropout(nn.Module):
    def forward(self, x):
        return F.dropout(x, p=self.p, training=True)  # Siempre activo

# âŒ MAL: Dropout normal no funciona en eval()
```

### âš ï¸ EntropÃ­a Estable

```python
# âœ… AÃ±adir epsilon
H = -(p * torch.log(p + 1e-12)).sum()

# âŒ Sin epsilon da NaN
H = -(p * torch.log(p)).sum()  # log(0) = -inf
```

---

## ğŸ“š Referencias

### TeÃ³ricas
- Gal & Ghahramani (2016): "Dropout as a Bayesian Approximation"
- Kendall & Gal (2017): "What Uncertainties Do We Need in Bayesian Deep Learning?"
- Niculescu-Mizil & Caruana (2005): "Predicting Good Probabilities"

### ImplementaciÃ³n
- CÃ³digo base: `cnn_training.ipynb` (sin incertidumbre)
- Arquitectura backbone: Ibarra et al. (2023)

---

## ğŸ”„ ComparaciÃ³n con Modelo Base

| Aspecto | CNN Base | CNN Incertidumbre |
|---------|----------|-------------------|
| Cabezas | 1 (logits) | 2 (logits + ÏƒÂ²) |
| PÃ©rdida | CrossEntropy | HeteroscedÃ¡stica |
| Inferencia | 1 pase | T_test pases (MC) |
| Output | `pred, probs` | `pred, H, epi, ale` |
| CalibraciÃ³n | EstÃ¡ndar | **Mejorada** |
| Tiempo train | ~5 min | ~7-8 min (+40%) |
| Tiempo test | <1 s | ~10-15 s (MCÃ—30) |

---

## âœ… Checklist de ValidaciÃ³n

- [ ] EntropÃ­a incorrectos > correctos
- [ ] ECE < 0.10 (idealmente < 0.05)
- [ ] Reliability diagram cerca de diagonal
- [ ] NLL razonable (< 0.15)
- [ ] Accuracy similar al modelo base
- [ ] Scatter muestra separaciÃ³n errores
- [ ] PÃ©rdida converge sin NaN

---

**Creado**: 2024  
**Autor**: Sistema de estimaciÃ³n de incertidumbre para diagnÃ³stico de Parkinson  
**Licencia**: [Tu licencia aquÃ­]

