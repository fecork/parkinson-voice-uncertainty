{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07799f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Clean dataset pipeline (PyTorch-ready)\n",
    "# ==============================\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SampleMeta:\n",
    "    \"\"\"Lightweight metadata holder for each audio segment.\"\"\"\n",
    "    subject_id: str\n",
    "    vowel_type: str\n",
    "    condition: str\n",
    "    filename: str\n",
    "    segment_id: int\n",
    "    sr: int\n",
    "\n",
    "PreprocessFn = Callable[[str, Optional[str]], Tuple[List[np.ndarray], List[np.ndarray]]]\n",
    "\n",
    "def parse_filename(file_stem: str) -> Tuple[str, str, str]:\n",
    "    parts = file_stem.split('-')\n",
    "    subject_id = parts[0] if len(parts) > 0 and parts[0] else \"unknown\"\n",
    "    vowel_type = parts[1] if len(parts) > 1 and parts[1] else \"a\"\n",
    "    condition  = parts[2] if len(parts) > 2 and parts[2] else \"unknown\"\n",
    "    return subject_id, vowel_type, condition\n",
    "\n",
    "def build_domain_index(vowels: Iterable[str]) -> Dict[str, int]:\n",
    "    uniq = sorted(set(vowels))\n",
    "    return {v: idx for idx, v in enumerate(uniq)}\n",
    "\n",
    "def map_condition_to_task(condition: str) -> int:\n",
    "    mapping = {\"h\": 1, \"l\": 0, \"n\": 0, \"lhl\": 1}\n",
    "    return mapping.get(condition, 0)\n",
    "\n",
    "def process_dataset(audio_files: Sequence, preprocess_fn: PreprocessFn, max_files: Optional[int] = None, default_sr: int = 44100):\n",
    "    if not audio_files:\n",
    "        print(\"‚ùå 'audio_files' est√° vac√≠o.\")\n",
    "        return []\n",
    "    files_to_process = list(audio_files[:max_files]) if max_files else list(audio_files)\n",
    "    print(f\"üîÑ Procesando {len(files_to_process)} archivos...\")\n",
    "    dataset = []\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        subject_id, vowel_type, condition = parse_filename(getattr(file_path, \"stem\", str(file_path)))\n",
    "        try:\n",
    "            spectrograms, segments = preprocess_fn(file_path, vowel_type=vowel_type)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en {file_path}: {e}\")\n",
    "            continue\n",
    "        if not spectrograms:\n",
    "            continue\n",
    "        for j, (spec, seg) in enumerate(zip(spectrograms, segments)):\n",
    "            dataset.append({\n",
    "                \"spectrogram\": spec,\n",
    "                \"segment\": seg,\n",
    "                \"metadata\": SampleMeta(subject_id, vowel_type, condition, getattr(file_path, \"name\", str(file_path)), j, default_sr)\n",
    "            })\n",
    "    print(f\"‚úÖ {len(dataset)} muestras generadas\")\n",
    "    return dataset\n",
    "\n",
    "def to_pytorch_tensors(dataset: List[Dict]):\n",
    "    if not dataset:\n",
    "        print(\"‚ùå Dataset vac√≠o.\")\n",
    "        return None, None, None, []\n",
    "    metas = [sample[\"metadata\"] for sample in dataset]\n",
    "    vowels = [m.vowel_type for m in metas]\n",
    "    domain_index = build_domain_index(vowels)\n",
    "    specs, y_task, y_domain = [], [], []\n",
    "    for sample in dataset:\n",
    "        spec = sample[\"spectrogram\"]\n",
    "        specs.append(np.expand_dims(spec, axis=0))\n",
    "        y_task.append(map_condition_to_task(sample[\"metadata\"].condition))\n",
    "        y_domain.append(domain_index[sample[\"metadata\"].vowel_type])\n",
    "    X = torch.from_numpy(np.stack(specs, axis=0)).float()\n",
    "    y_task_t = torch.tensor(y_task, dtype=torch.long)\n",
    "    y_domain_t = torch.tensor(y_domain, dtype=torch.long)\n",
    "    print(\"üìä Tensores listos:\", X.shape, y_task_t.shape, y_domain_t.shape)\n",
    "    return X, y_task_t, y_domain_t, metas\n",
    "\n",
    "class VowelSegmentsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y_task, y_domain, metas):\n",
    "        self.X, self.y_task, self.y_domain, self.metas = X, y_task, y_domain, metas\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"X\": self.X[idx], \"y_task\": self.y_task[idx], \"y_domain\": self.y_domain[idx], \"meta\": self.metas[idx]}\n",
    "\n",
    "def build_full_pipeline(audio_files: Optional[Sequence], preprocess_fn: PreprocessFn, max_files: Optional[int] = None):\n",
    "    if not audio_files:\n",
    "        print(\"‚ùå 'audio_files' no est√° definido.\")\n",
    "        return {\"dataset\": [], \"tensors\": (None, None, None), \"torch_ds\": None, \"metadata\": []}\n",
    "    dataset = process_dataset(audio_files, preprocess_fn, max_files)\n",
    "    if not dataset:\n",
    "        return {\"dataset\": [], \"tensors\": (None, None, None), \"torch_ds\": None, \"metadata\": []}\n",
    "    X, y_task, y_domain, metas = to_pytorch_tensors(dataset)\n",
    "    torch_ds = VowelSegmentsDataset(X, y_task, y_domain, metas) if X is not None else None\n",
    "    return {\"dataset\": dataset, \"tensors\": (X, y_task, y_domain), \"torch_ds\": torch_ds, \"metadata\": metas}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso (ajusta audio_files y preprocess_audio_paper a tu entorno)\n",
    "# results = build_full_pipeline(audio_files=audio_files, preprocess_fn=preprocess_audio_paper)\n",
    "# dataset = results[\"dataset\"]\n",
    "# X, y_task, y_domain = results[\"tensors\"]\n",
    "# torch_dataset = results[\"torch_ds\"]\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
